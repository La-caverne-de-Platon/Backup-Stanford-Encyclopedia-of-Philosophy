<!DOCTYPE html>
<!--[if lt IE 7]> <html class="ie6 ie"> <![endif]-->
<!--[if IE 7]>    <html class="ie7 ie"> <![endif]-->
<!--[if IE 8]>    <html class="ie8 ie"> <![endif]-->
<!--[if IE 9]>    <html class="ie9 ie"> <![endif]-->
<!--[if !IE]> --> <html> <!-- <![endif]-->

<!-- Mirrored from seop.illc.uva.nl/entries/epistemic-utility/ by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 22 Jun 2022 19:38:18 GMT -->
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>
Epistemic Utility Arguments for Probabilism (Stanford Encyclopedia of Philosophy)
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="robots" content="noarchive, noodp" />
<meta property="citation_title" content="Epistemic Utility Arguments for Probabilism" />
<meta property="citation_author" content="Pettigrew, Richard" />
<meta property="citation_publication_date" content="2011/09/23" />
<meta name="DC.title" content="Epistemic Utility Arguments for Probabilism" />
<meta name="DC.creator" content="Pettigrew, Richard" />
<meta name="DCTERMS.issued" content="2011-09-23" />
<meta name="DCTERMS.modified" content="2019-11-06" />

<!-- NOTE: Import webfonts using this link: -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300,600,200&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css" />

<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap-responsive.min.css" />
<link rel="stylesheet" type="text/css" href="../../css/font-awesome.min.css" />
<!--[if IE 7]> <link rel="stylesheet" type="text/css" href="../../css/font-awesome-ie7.min.css"> <![endif]-->
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/style.css" />
<link rel="stylesheet" type="text/css" media="print" href="../../css/print.css" />
<link rel="stylesheet" type="text/css" href="../../css/entry.css" />
<!--[if IE]> <link rel="stylesheet" type="text/css" href="../../css/ie.css" /> <![endif]-->
<script type="text/javascript" src="../../js/jquery-1.9.1.min.js"></script>
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>

<!-- NOTE: Javascript for sticky behavior needed on article and ToC pages -->
<script type="text/javascript" src="../../js/jquery-scrolltofixed-min.js"></script>
<script type="text/javascript" src="../../js/entry.js"></script>

<!-- SEP custom script -->
<script type="text/javascript" src="../../js/sep.js"></script>
</head>

<!-- NOTE: The nojs class is removed from the page if javascript is enabled. Otherwise, it drives the display when there is no javascript. -->
<body class="nojs article" id="pagetopright">
<div id="container">
<div id="header-wrapper">
  <div id="header">
    <div id="branding">
      <div id="site-logo"><a href="../../index.html"><img src="../../symbols/sep-man-red.png" alt="SEP logo" /></a></div>
      <div id="site-title"><a href="../../index.html">Stanford Encyclopedia of Philosophy</a></div>
    </div>
    <div id="navigation">
      <div class="navbar">
        <div class="navbar-inner">
          <div class="container">
            <button class="btn btn-navbar collapsed" data-target=".collapse-main-menu" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Menu </button>
            <div class="nav-collapse collapse-main-menu in collapse">
              <ul class="nav">
                <li class="dropdown open"><a id="drop1" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-book"></i> Browse</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop1">
                    <li><a href="../../contents.html">Table of Contents</a></li>
                    <li><a href="../../new.html">What's New</a></li>
                    <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
                    <li><a href="../../published.html">Chronological</a></li>
                    <li><a href="../../archives/index.html">Archives</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop2" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-info-sign"></i> About</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop2">
                    <li><a href="../../info.html">Editorial Information</a></li>
                    <li><a href="../../about.html">About the SEP</a></li>
                    <li><a href="../../board.html">Editorial Board</a></li>
                    <li><a href="../../cite.html">How to Cite the SEP</a></li>
                    <li><a href="../../special-characters.html">Special Characters</a></li>
                    <li><a href="../../tools/index.html">Advanced Tools</a></li>
                    <li><a href="../../contact.html">Contact</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop3" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-leaf"></i> Support SEP</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop3">
                    <li><a href="../../support/index.html">Support the SEP</a></li>
                    <li><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
                    <li><a href="../../support/donate.html">Make a Donation</a></li>
                    <li><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
                  </ul>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End navigation -->
    
    <div id="search">
      <form id="search-form" method="get" action="https://seop.illc.uva.nl/search/searcher.py">
        <input type="search" name="query" placeholder="Search SEP" />
        <div class="search-btn-wrapper"><button class="btn search-btn" type="submit"><i class="icon-search"></i></button></div>
      </form>
    </div>
    <!-- End search --> 
    
  </div>
  <!-- End header --> 
</div>
<!-- End header wrapper -->

<div id="content">

<!-- Begin article sidebar -->
<div id="article-sidebar" class="sticky">
  <div class="navbar">
    <div class="navbar-inner">
      <div class="container">
        <button class="btn btn-navbar" data-target=".collapse-sidebar" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Entry Navigation </button>
        <div id="article-nav" class="nav-collapse collapse-sidebar in collapse">
          <ul class="nav">
            <li><a href="#toc">Entry Contents</a></li>
            <li><a href="#Bib">Bibliography</a></li>
            <li><a href="#Aca">Academic Tools</a></li>
            <li><a href="https://leibniz.stanford.edu/friends/preview/epistemic-utility/">Friends PDF Preview <i class="icon-external-link"></i></a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=epistemic-utility">Author and Citation Info <i class="icon-external-link"></i></a> </li>
            <li><a href="#pagetopright" class="back-to-top">Back to Top <i class="icon-angle-up icon2x"></i></a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</div>
<!-- End article sidebar --> 

<!-- NOTE: Article content must have two wrapper divs: id="article" and id="article-content" -->
<div id="article">
<div id="article-content">

<!-- BEGIN ARTICLE HTML -->


<div id="aueditable"><!--DO NOT MODIFY THIS LINE AND ABOVE-->

<h1>Epistemic Utility Arguments for Probabilism</h1><div id="pubinfo"><em>First published Fri Sep 23, 2011; substantive revision Wed Nov 6, 2019</em></div>

<div id="preamble">

<p>
Our beliefs come in degrees; we believe some things more strongly than
others. For instance, I believe that the sun will rise tomorrow very
slightly more strongly than I believe that it will rise every morning
for the coming week; and I believe both of these propositions much
more strongly than I believe that there will be an earthquake tomorrow
in Bristol. We call the strength or the degree of our belief in a
proposition our <em>credence</em> in that proposition. Suppose I know
that a die is to be rolled, and I believe that it will land on six
more strongly than I believe that it will land on an even number. In
this case, we would say that there is something wrong with my
credences, for if it lands on six, it lands on an even number, and I
ought not to believe a proposition more strongly than I believe a
logical consequence of it. This follows from a popular doctrine in the
epistemology of credences called <strong>Probabilism</strong>, which
says that our credences at a given time ought to satisfy the axioms of
the probability calculus (given in detail below). Since this says
something about how our credences <em>ought to be</em> rather than how
they in fact <em>are</em>, we call this an epistemic norm.</p>

<p>
In this entry, we explore a particular strategy that we might deploy
when we wish to establish an epistemic norm such as
<strong>Probabilism</strong>. It is called <em>epistemic utility
theory</em> or <em>accuracy-first epistemology</em>, or sometimes
<em>cognitive</em> or <em>epistemic decision theory</em>. In this
entry, we will use the former. Epistemic utility theory is inspired by
traditional utility theory, so let&rsquo;s begin with a quick summary
of that.</p>

<p>
Traditional utility theory (also known as decision theory, see entry
on
 <a href="../rationality-normative-utility/index.html">normative theories of rational choice: expected utility</a>)
 explores a particular strategy for establishing the norms that govern
which actions it is rational for us to perform in a given situation.
Given a particular situation, the framework for the theory includes
<em>states of the world</em> that are relevant to the situation,
<em>actions</em> that are available to the agent in the situation, and
the agent&rsquo;s <em>utility function</em>, which takes a state of
the world and an action and returns a measure of the extent to which
she values the outcome of performing that action at that world. We
call this measure the <em>utility</em> of the outcome at the world.
For example, there might be just two relevant states of the world: one
in which it rains and one in which it does not. And there might be
just two relevant actions from which to choose: take an umbrella when
you leave the house or don&rsquo;t. Then your utility function will
measure how much you value the outcomes of each action at each state
of the world: that is, it will give the value of being in the rain
without an umbrella, being in the rain with an umbrella, being with an
umbrella when there is no rain, and being without an umbrella when
there is no rain. With this framework in hand, we can state certain
very general norms of action in terms of it. For instance, we might
say that an agent ought not to perform an action if there is some
other action that has greater utility than it at every possible state
of the world. This norm is called <strong>Naive Dominance</strong>. We
will have a lot to say about it in
 <a href="#JoyAccArgForPro">section 5.1</a>
 below.</p>

<p>
In epistemic utility theory, the states of the world remain the same,
but the possible actions an agent might perform are replaced by the
possible <em>epistemic states</em> she might adopt, and the utility
function is replaced, for each agent, by an <em>epistemic utility
function</em>, which takes a state of the world and a possible
epistemic state and returns a measure of the purely epistemic value
that the agent attaches to being in that epistemic state at that state
of the world. So, in epistemic utility theory, we appeal to epistemic
utility to ask which of a range of possible epistemic states it is
rational to adopt, just as in traditional utility theory we appeal to
non-epistemic, pragmatic utility to ask which of a range of possible
actions it is rational to perform. In fact, we will often talk of
epistemic <em>dis</em>utility rather than epistemic utility in this
entry. But it is easy to translate between them. If \(\mathfrak{EU}\)
is an epistemic utility function, then \(-\mathfrak{EU}\) is an
epistemic disutility function, and <em>vice versa</em>.</p>

<p>
Again, certain very general norms may be stated, such as the obvious
analogue of <strong>Naive Dominance</strong> from above. Thus, before
the die is rolled, we might ask whether I should adopt an epistemic
state in which I believe that the die will land on six more strongly
than I believe that it will land on an even number. And we might be
able to show that I shouldn&rsquo;t because there is some other
epistemic state I could adopt instead that will have greater epistemic
utility however the world turns out. In this case, we appeal to the
epistemic version of <strong>Naive Dominance</strong> to show that my
credences are irrational. This is an example of how epistemic utility
theory might come to justify <strong>Probabilism</strong>. As we will
see, arguments just like this have indeed been given. In this entry,
we explore these arguments.</p>
</div>

<div id="toc">
<!--Entry Contents-->

<ul>

 <li><a href="#ModEpiSta">1. Modelling Epistemic States</a></li>
 
 <li><a href="#ForArgEpiUtiThe">2. The Form of Arguments in Epistemic Utility Theory</a></li>
 
 <li><a href="#EpiNorPro">3. The Epistemic Norm of Probabilism</a></li>
 
 <li><a href="#CalArg">4. Calibration Arguments</a>
 
<ul>

 <li><a href="#CalMea">4.1 Calibration measures</a></li>
 
 <li><a href="#CalArgForPro">4.2 Calibration arguments for Probabilism</a></li>
 
 <li><a href="#ObjCalArgForPro">4.3 Objections to calibration arguments for Probabilism</a></li>
 </ul></li>

 <li><a href="#AccArg">5. Accuracy Arguments</a>
 
<ul>

 <li><a href="#JoyAccArgForPro">5.1 Joyce&rsquo;s accuracy argument for Probabilism</a></li>
 
 <li><a href="#SouEpiDis">5.2 The source(s) of epistemic disutility</a></li>
 
 <li><a href="#MeaIna">5.3 Measures of inaccuracy</a>
 
<ul>

 <li><a href="#JoyCon">5.3.1 Joyce on Convexity</a></li>
 
 <li><a href="#LocGloIna">5.3.2 Local and global inaccuracy</a></li>
 
 <li><a href="#CalAcc">5.3.3 Calibration and accuracy</a></li>
 
 <li><a href="#LevImp">5.3.4 Levinstein on importance</a></li>
 </ul></li>

 <li><a href="#DomPri">5.4 Dominance principles</a>
 
<ul>

 <li><a href="#BroObj">5.4.1 The Bronfman objection</a></li>
 
 <li><a href="#UndDom">5.4.2 Undominated dominance</a></li>
 
 <li><a href="#EviAcc">5.4.3 Evidence and Accuracy</a></li>
 
 <li><a href="#DomActStaDep">5.4.4 Dominance and Act-State Dependence</a></li>
 </ul></li>

 <li><a href="#EpExp">5.5 Epistemic Expansions</a></li>
 </ul></li>

 <li><a href="#EpiDisArg">6. Epistemic disutility arguments</a>
 
<ul>

 <li><a href="#MayWheNum">6.1 Mayo-Wilson and Wheeler on numerical representations of epistemic utility</a>
 </li>
</ul></li>

 <li><a href="#RelIss">7. Related issues</a>
 
<ul>

 <li><a href="#InfProSpa">7.1 Infinite probability spaces</a></li>
 
 <li><a href="#OthPriRatForCre">7.2 Other principles of rationality for credences</a></li>
 
 <li><a href="#OthDoxSta">7.3 Other doxastic states</a></li>
 
 <li><a href="#NonClaLog">7.4 Non-classical logic</a></li>
 </ul></li>

 <li><a href="#Bib">Bibliography</a></li>
 
 <li><a href="#Aca">Academic Tools</a></li>
 
 <li><a href="#Oth">Other Internet Resources</a></li>
 
 <li><a href="#Rel">Related Entries</a></li>
 </ul>

<!--Entry Contents-->

<hr />
</div>

<div id="main-text">

<h2><a id="ModEpiSta">1. Modelling Epistemic States</a></h2>

<p>
In formal epistemology, epistemic states are modelled in many
different ways (see entry on
 <a href="../formal-belief/index.html">formal representations of belief</a>).
 Given an epistemic agent and a time \(t\), we might model her
epistemic state at \(t\) using any of the following:</p>

<ul>

<li>the set of propositions she believes at \(t\) (we might call this
the <em>full belief model</em>; it is the object of study in much
traditional epistemology and in doxastic and epistemic logic);</li>

<li>the set of propositions she believes at \(t\) together with an
entrenchment ordering, which specifies the order in which she is
prepared to abandon her beliefs in the light of conflicting evidence
(this is the <em>ranking theory model</em>);</li>

<li>a single credence function at \(t\), which takes each proposition
about which she has an opinion and returns her credence in that
proposition at \(t\) (this is the <em>precise credence</em> or
<em>standard Bayesian model</em>);</li>

<li>a set of credence functions, each of which is a precisification of
her otherwise vague or imprecise or indeterminate credences at \(t\)
(this is the <em>imprecise credence model</em>);</li>

<li>her upper and lower probability functions at \(t\);</li>

<li>and so on.</li>
</ul>

<p>
Epistemic utility theory may be applied to any one of these ways of
modelling epistemic states. Whichever we choose, we define an
epistemic disutility function to be a function that takes an epistemic
state modelled in this way, together with a state of the world, to a
non-negative real number or the number \(\infty\), and we take this
number to measure the epistemic disutility of having that epistemic
state at that world.</p>

<p>
The vast majority of work carried out so far in epistemic utility
theory has taken an agent&rsquo;s epistemic state at time \(t\) to be
modelled by her credence function at \(t\). And, in any case, the
epistemic norm of <strong>Probabilism</strong> that interests us here
governs agents modelled in this way. Thus, we focus on this case. In
 <a href="#RelIss">section 7</a>,
 we will consider how the argument strategy employed here to justify
<strong>Probabilism</strong> for agents with precise credences might
be employed to establish other norms either for agents also
represented as having precise credences or for agents represented in
other ways.</p>

<p>
So, henceforth, we model an agent&rsquo;s epistemic state at \(t\) by
her credence function at \(t\). We now make more precise what this
means. We assume that the set of propositions about which an agent has
an opinion is finite and forms an algebra \(\mathcal{F}\). That
is:</p>

<ol>

<li>It contains a contradictory proposition (\(\bot\)). This is a
proposition that is false at all worlds.</li>

<li>It contains a tautologous proposition (\(\top\)). This is a
proposition that is true at all worlds.</li>

<li>It is closed under disjunction, conjunction, and negation. That
is, if \(A\) and \(B\) are in \(\mathcal{F}\), then \(A \vee B\), \(A\
\&amp;\ B\), and \(\neg A\) and \(\neg B\) are also in
\(\mathcal{F}\).</li>
</ol>

<p>
We then assume that our agent&rsquo;s credence in a proposition in
\(\mathcal{F}\) can be measured by a real number between 0 and 1
inclusive, where 0 represents minimal credence, and 1 represents
maximal credence. Then her credence function at \(t\) is a function
<em>c</em> from \(\mathcal{F}\) to the closed unit interval \([0,
1]\). If \(A\) is in \(\mathcal{F}\), then \(c(A)\) is our
agent&rsquo;s credence in \(A\) at \(t\). Throughout, we denote by
\(\mathcal{C_F}\) the set of possible credence functions defined on
\(\mathcal{F}\). There is no principled reason for restricting to the
case in which \(\mathcal{F}\) is finite. We do it here only because
the majority of work on this problem has been carried out under this
assumption. It is an interesting question how the results here might
be extended to the case in which \(\mathcal{F}\) is infinite, but we
will not explore it here (again, see
 <a href="#RelIss">section 7</a>).</p>
 
<p>
So, an epistemic utility function for credences takes a credence
function, together with a way the world might be, and returns a
measure of the epistemic utility of having that credence function if
the world were that way.</p>

<h2><a id="ForArgEpiUtiThe">2. The Form of Arguments in Epistemic Utility Theory</a></h2>

<p>
In epistemic utility theory, we attempt to justify an epistemic norm
<strong>N</strong> using the following two ingredients:</p>

<ul class="sentag tag2em">

<li><span class="tag bold">Q</span><span class="sen">A norm of
standard utility theory (or decision theory), which is to be applied,
using epistemic utility functions, to discover which credence
functions it is rational for an agent to adopt in a given
situation.</span></li>

<li><span class="tag bold">E</span><span class="sen">A set of
conditions that a legitimate measure of epistemic utility must
satisfy.</span></li>
</ul>

<p>
Typically, the inference from <strong>Q</strong> and
<strong>E</strong> to <strong>N</strong> appeals to a mathematical
theorem, which shows that, applied to any epistemic utility function
that satisfies the conditions <strong>E</strong>, the norm
<strong>Q</strong> entails the norm <strong>N</strong>.</p>

<p>
Given that the existing arguments of epistemic utility theory share
this common form, we might organize these arguments by the norms they
attempt to justify, or by the norms of standard utility theory they
employ, or by the set of constraints on epistemic utility functions
they impose. We will take the latter course in this survey.</p>

<p>
In sections
 <a href="#CalArg">4</a>
 and
 <a href="#AccArg">5</a>,
 we identify a specific epistemic goal and treat epistemic
<em>dis</em>utility functions as measures of the distance of an
epistemic state from that goal in a given situation; we lay down
conditions that it is claimed all such measures must satisfy. In
 <a href="#EpiDisArg">section 6</a>,
 we take an alternative route: we lay down putative general conditions
on any epistemic <em>dis</em>utility function, which it is claimed
such a function must satisfy regardless of whether or not it is a
measure of distance from a specified epistemic goal. In the next
section, we state <strong>Probabilism</strong> precisely, so that we
can refer back to it later.</p>

<h2><a id="EpiNorPro">3. The Epistemic Norm of Probabilism</a></h2>

<p>
<strong>Probabilism</strong> is often said to be a coherence
constraint on credence functions, which would mean that it governs how
an agent&rsquo;s credences in some propositions should relate to her
credences in other, related propositions. It is often likened to the
consistency constraint on sets of full beliefs. In fact, this
isn&rsquo;t quite right. Condition (ii) below is certainly a coherence
constraint, but condition (i) is not.</p>

<div class="indent" id="probabilism">

<p>
<strong>Probabilism</strong> A rational agent&rsquo;s credence
function \(c\) at a given time is a probability function. That is:</p>

<ol type="i">

<li>\(c(\bot) = 0\) and \(c(\top) = 1\).</li>

<li>\(c(A \vee B) = c(A) + c(B)\), for all mutually exclusive \(A\)
and \(B\) in \(\mathcal{F}\).</li>
</ol>
</div>

<p>
Note that any agent who satisfies <strong>Probabilism</strong> must be
<em>logically omniscient</em>: that is, she must be certain of every
tautology. Some other consequences of
<strong>Probabilism</strong>:</p>

<ul>

<li>\(c(A) \leq c(A \vee B)\) for any \(A\), \(B\) in
\(\mathcal{F}\).</li>

<li>\(c(A\ \&amp;\ B) \leq c(A)\) for any \(A\), \(B\) in
\(\mathcal{F}\).</li>

<li>\(c(A) = c(B)\) if \(A\) and \(B\) are logically equivalent.</li>
</ul>

<p>
<strong>Probabilism</strong> is one of a handful of norms that
characterise the Bayesian view in credal epistemology.</p>

<h2><a id="CalArg">4. Calibration Arguments</a></h2>

<p>
In this section, we consider the conditions imposed on an epistemic
disutility function when we treat it as a measure of the distance of
an epistemic state from the goal of being <em>actually</em> or
<em>hypothetically calibrated</em> (van Fraassen 1983; Lange 1999;
Shimony 1988). We say that a credence function is actually calibrated
at a particular possible world if the credence it assigns to a
proposition matches the relative frequency with which propositions of
that kind are true at that world. Thus, credence 0.2 in proposition
\(A\) is actually calibrated if one-fifth of propositions like \(A\)
are actually true. And we say that a credence function is
hypothetically calibrated if the credence it assigns to a proposition
matches the limiting relative frequency with which propositions of
that kind <em>would</em> be true <em>were</em> there more propositions
of that kind. Thus, credence 0.2 in proposition \(A\) is
hypothetically calibrated if, as we move to worlds with more and more
propositions like \(A\), the proportion of such propositions that are
true approaches one-fifth in the limit. According to the calibration
arguments, matching these relative frequencies or limiting relative
frequencies is an epistemic goal. And they attempt to justify
<strong>Probabilism</strong> by appealing to this goal and measures of
distance from it.</p>

<h3><a id="CalMea">4.1 Calibration measures</a></h3>

<p>
First, we must make precise what we mean by actual and hypothetical
calibration; then we can say which functions will count as measuring
distance from these putative goals. We treat actual calibration first.
Since we are talking of relative frequencies, we will need to assign
to each proposition in \(\mathcal{F}\) its <em>reference class</em>:
that is, the set of propositions that are relevantly similar to it.
Thus, we require an equivalence relation \(\sim\) on \(\mathcal{F}\),
where \(A \sim B\) iff \(A\) and \(B\) are relevantly similar. For
instance, if our algebra of propositions contains <em>Heads on first
toss of coin</em>, <em>Heads on second toss of coin</em>, and <em>Six
on first roll of die</em>, we might plausibly say that the first two
are relevantly similar, but neither first nor second is relevantly
similar to the third. Proponents of calibration arguments do not claim
to give an account of how the equivalence relation is determined. Nor
do they claim that there is a single, objectively correct equivalence
relation on a given algebra of propositions: this is the notorious
<em>problem of the reference class</em> that haunts frequentist
interpretations of objective probability. Rather they treat the
equivalence relation as a component of the agent&rsquo;s epistemic
state, along with her credence function. Indeed, for van Fraassen, it
is determined entirely by the credence function together with the form
of the propositions in \(\mathcal{F}\) (van Fraassen 1983: 299).
However, they do impose some rational constraints on \(\sim\) in order
to establish their conclusion. We will not discuss these conditions in
any detail. Rather we denote them \(C(\sim)\), and keep in mind that
this is a placeholder for a full account of conditions on \(\sim\).
Detailed accounts of these conditions have been given by van Fraassen
(1983) and Shimony (1988). We say that a credence function \(c\),
together with an equivalence relation \(\sim\), is perfectly
calibrated or not relative to a way the world might be. We are now
ready to give our first definitions; but we preface these with an
example.</p>

<p>
Suppose a coin is to be flipped 1000 times. And suppose that \(A\) is
the proposition <em>Heads on toss 1</em>. And suppose that the
propositions that are relevantly similar to \(A\) in algebra
\(\mathcal{F}\) are: <em>Heads on toss 1</em>, &hellip;<em>Heads on
toss 1000</em>. Finally, suppose that \(w\) is a possible world; a way
that the world might be. In fact, throughout this article, we need not
quantify over genuine possible worlds, which are maximally specific
ways the world might be; we need only quantify over ways the world
might be that are specific enough to assign truth values to each of
the propositions in the algebra \(\mathcal{F}\). Let&rsquo;s call
these <em>possible worlds relative to \(\mathcal{F}\)</em> and let
\(\mathcal{W_F}\) be the set of them for a given algebra
\(\mathcal{F}\). Then the <em>relative frequency of \(A\) at
\(w\)</em> (written \(\mathrm{Freq}(\mathcal{F}, A, \sim, w)\)) is the
proportion of the propositions relevantly similar to \(A\) that are
true at \(w\): that is, the frequency of heads amongst the 1000 coin
tosses at that world. For instance, if every second toss lands heads
at \(w\), or if the first five hundred land heads and the rest land
tails at \(w\), then \(\mathrm{Freq}(\mathcal{F}, A, \sim, w) =
\frac{1}{2}\). If every third toss lands heads at \(w\), then
\(\mathrm{Freq}(\mathcal{F}, A, \sim, w) = \frac{1}{3}\). And so
on.</p>

<p>
Now we give the definition in full generality. Suppose \(\sim\) is an
equivalence relation on \(\mathcal{F}\), and \(w\) is a possible world
relative to \(\mathcal{F}\). Then:</p>

<ul>

<li>For each \(A\) in \(\mathcal{F}\), the <em>relative frequency of
truths among propositions like \(A\)</em> is defined as follows:

\[\mathrm{Freq}(\mathcal{F}, A, \sim, w) := \frac{|\{ X \in
\mathcal{F} : X \sim A\ \&amp;\ v_w(X) = 1\}|}{|\{X \in \mathcal{F} :
X \sim A\}|}\]

 where \(|X|\) is the cardinality of the set \(X\) and
\(v_w\) is the standard numerical truth value assignment at that
world, so that \(v_w(X) = 1\) if \(X\) is true at \(w\) and \(v_w(X) =
0\) if \(X\) is false at \(w\) (we call \(v_w\) the <em>omniscient
credence function at \(w\)</em>). Thus, \(\mathrm{Freq}(\mathcal{F},
A, \sim, w)\) is the proportion of true propositions amongst all
propositions in \(\mathcal{F}\) that are relevantly similar to the
proposition \(A\).</li>

<li>Relative to \(\sim\), the credence <em>r</em> in proposition \(A\)
is <em>actually calibrated</em> at \(w\) if \(r =
\mathrm{Freq}(\mathcal{F}, A, \sim, w)\).</li>
</ul>

<p>
The idea is that, if \(\sim\) satisfies constraints \(C(\sim)\), then
the function \(\mathrm{Freq}(\mathcal{F}, \cdot, \sim, w)\) is always
a probability function on \(\mathcal{F}\).</p>

<p>
Next, we treat hypothetical calibration. For this, we need the notion
of the limiting relative frequency of truths amongst propositions of a
certain sort. The idea is that, for each proposition \(A\) in
\(\mathcal{F}\), there is not just a fact of the matter about what the
frequency of truths amongst propositions like \(A\) actually is; there
is also a fact of the matter about what the frequency of truths
amongst propositions like \(A\) would be if there were more
propositions like \(A\). For instance, there is not just a fact of the
matter about how many actual tosses of a given coin will land heads;
there is also a fact of the matter about the frequency of heads
amongst hypothetical further tosses of the same coin. In general,
suppose we have a possible world \(w\), an extension \(\mathcal{F}'\)
of \(\mathcal{F}\) (containing new propositions like \(A\)), and an
extension \(\sim'\) of \(\sim\) to cover the new propositions in
\(\mathcal{F}'\). Then there is a single unique number
\(\mathrm{Freq}(\mathcal{F}', A, \sim', w)\) that gives what the
relative frequency of truths amongst propositions like \(A\) would be
were there all the propositions in \(\mathcal{F}'\) and where the
relation of similarity amongst them is given by \(\sim'\), where this
counterfactual is evaluated at the world \(w\). Again, let us
illustrate this using our example of the coin toss from above.</p>

<p>
Suppose again that \(A\) is the proposition <em>Heads on toss 1</em>
and that the propositions in \(\mathcal{F}\) that are relevantly
similar to \(A\) according to \(\sim\) are <em>Heads on toss 1</em>,
&hellip;, <em>Heads on toss 1000</em>. Now suppose that
\(\mathcal{F}_1\) extends \(\mathcal{F}\) by introducing a new
proposition about a further hypothetical toss of the coin (as well as
perhaps other propositions). That is, it introduces <em>Heads on toss
1001</em> (and closes out under negation, disjunction, and
conjunction). And suppose that \(\sim_1\) extends \(\sim\), so that
the new proposition <em>Heads on toss 1001</em> is considered
relevantly similar to each <em>Heads on toss 1</em>, &hellip;,
<em>Heads on toss 1000</em>. Then those who appeal to hypothetical
limiting frequencies must claim that there is a unique number that
gives what the frequency of heads would be, were the coin tossed 1001
times. They denote this number \(\mathrm{Freq}(\mathcal{F}_1, A,
\sim_1, w)\). Now suppose that \(\mathcal{F}_2\) extends
\(\mathcal{F}_1\) by adding the new proposition <em>Heads on toss
1002</em> and \(\sim_2\) extends \(\sim_1\), so that the new
proposition <em>Heads on toss 1002</em> is considered relevantly
similar to each <em>Heads on toss 1</em>, &hellip;, <em>Heads on toss
1001</em>. And so on. Then the limiting relative frequency of \(A\) at
\(w\) (written \(\mathrm{LimFreq}(\mathcal{F}, A, \sim, w)\)) is the
number towards which the following sequence tends: 

\[\mathrm{Freq}(\mathcal{F}, A, \sim, w), \mathrm{Freq}(\mathcal{F}_1,
A, \sim_1, w), \mathrm{Freq}(\mathcal{F}_2, A, \sim_2, w),
\ldots\]</p>

<p>
In general, for each algebra \(\mathcal{F}\) and equivalence relation
\(\sim\), there is an infinite sequence 

\[(\mathcal{F}, \sim) = (\mathcal{F}_0, \sim_0), (\mathcal{F}_1,
\sim_1), (\mathcal{F}_2, \sim_2), \ldots\]

 of pairs of
algebras \(\mathcal{F}_i\) and equivalence relations \(\sim_i\) such
that each \(\mathcal{F}_{i+1}\) is an extension of \(\mathcal{F}_i\)
and each \(\sim_{i+1}\) is an extension of \(\sim_i\) and, for each
\(i\), \(C(\sim_i)\). Using this, we can define the notion of limiting
relative frequency and the associated notion of hypothetical
calibration in full generality. Suppose \(\sim\) is an equivalence
relation on \(\mathcal{F}\) and \(w\) is a possible world. And suppose

\[(\mathcal{F}_0, \sim_0), (\mathcal{F}_1, \sim_1), (\mathcal{F}_2,
\sim_2), \ldots\]

 is the sequence just mentioned. Then:</p>

<ul>

<li>For each \(A\) in \(\mathcal{F}\), the <em>limiting relative
frequency of truths among propositions like \(A\)</em> is 

\[\mathrm{LimFreq}(\mathcal{F}, A, \sim, w) := \lim_{n \rightarrow
\infty} \mathrm{Freq}(\mathcal{F}_n, A, \sim_n, w)\]

That is, the limiting relative frequency of \(A\) is the number
approached arbitrarily closely by the hypothetical relative
frequencies of truths as we extend the algebra \(\mathcal{F}\) to
include more and more propositions like \(A\).</li>

<li>Relative to \(\sim\), the credence <em>r</em> in proposition \(A\)
is <em>hypothetically calibrated</em> at \(w\) if 

\[r = \mathrm{LimFreq}(\mathcal{F}, A, \sim, w)\]</li>

</ul>

<p>
According to some calibration arguments, actual calibration is an
epistemic goal; according to others, hypothetical calibration is the
goal. Whichever it is, the epistemic disutility of a credence ought to
be given by its distance from this epistemic goal. We say that an
epistemic disutility function is <em>local</em> if it measures only
the epistemic disutility of an individual credence at a world; we say
that it is <em>global</em> if it measures the epistemic disutility of
an entire credence function at a world. In this section, we will be
concerned only with local epistemic disutility functions. In sections
 <a href="#AccArg">5</a>
 and
 <a href="#EpiDisArg">6</a>,
 we will be concerned instead with global epistemic disutility
functions.</p>

<p>
The goals of actual calibration and hypothetical calibration give rise
to the following definitions of two sorts of local epistemic
disutility function:</p>

<ul>

<li id="actual-calibration">An <em>actual calibration measure</em> is
a function of the form 

\[\mathfrak{c}(r, A, \mathcal{F}, \sim, w) =
f(|\mathrm{Freq}(\mathcal{F}, A, \sim, w) - r|)\]

 where \(f : [0, 1] \rightarrow
\mathbb{R}\) is a strictly increasing continuous function with \(f(0)
= 0\). Let <strong>Actual Calibration</strong> be the claim that
\(\mathfrak{c}\) is the measure of epistemic disutility.</li>

<li id="hypothetical-calibration">A <em>hypothetical calibration
measure</em> is a function of the form 

\[\mathfrak{hc}(r, A, \mathcal{F}, \sim, w) =
f(|\mathrm{LimFreq}(\mathcal{F}, A, \sim, w) - r|)\] where again \(f :

 [0, 1] \rightarrow
\mathbb{R}\) is a strictly increasing continuous function with \(f(0)
= 0\). Let <strong>Hypothetical Calibration</strong> be the claim that
\(\mathfrak{hc}\) is the measure of epistemic disutility.</li>
</ul>

<p>
Our next task is to identify the norms of standard decision
theory/utility theory that are deployed in conjunction with this
characterization to derive <strong>Probabilism</strong>.</p>

<h3><a id="CalArgForPro">4.2 Calibration arguments for Probabilism</a></h3>

<p>
In this section, we consider the two accounts of epistemic disutility
for credences given in the previous section and we combine them with
decision-theoretic norms to derive epistemic norms. When we state the
decision-theoretic norms in question, we state them in full
generality. In practical decision theory, we evaluate acts: it is acts
that have practical disutilities at worlds. In epistemic decision
theory, on the other hand, we evaluate credence functions: it is
credence functions that have epistemic disutilities at worlds. And in
another context still, we might wish to use decision theory to
evaluate some other sort of thing, such as a scientific theory (Maher
1993). So we want to state the decision-theoretic norms in a way that
is neutral between these. We will talk of <em>options</em> as the
things that are being evaluated and that have utilities at worlds.
Options can thus be acts or credence functions or scientific theories
or some other sort of thing.</p>

<p>
Here&rsquo;s our first putative norm of standard decision theory (van
Fraassen 1983: 297):</p>

<div class="indent" id="possibility-vindication">

<p>
<strong>Possibility of Vindication</strong> A rational agent will not
adopt an option that has no possibility of attaining minimal
disutility, when such a minimum exists.</p>

<p>
Here it is a little more formally: Suppose \(\mathcal{O}\) is a set of
options, \(\mathcal{W}\) is the set of possible worlds, and
\(\mathfrak{U}\) is a disutility function. Then, if \(o^*\) is an
option, and there is no \(w^*\) in \(\mathcal{W}\) such 

\[\mathfrak{U}(o^*, w^*) = \min \{\mathfrak{U}(o, w) : o \in
\mathcal{O}\ \&amp;\ w \in \mathcal{W}\}\] (when this minimum exists),

then \(o^*\) is irrational.</p>
</div>

<p>
It can be shown that, together with <strong>Actual
Calibration</strong> from the previous section and suitable
constraints \(C(\sim)\) on the equivalence relation \(\sim\), this
norm entails something stronger than <strong>Probabilism</strong>. It
entails:</p>

<div class="indent" id="rational-valued">

<p>
<strong>Rational-valued Probabilism</strong> At any time \(t\), a
rational agent&rsquo;s credence function \(c\) is a probability
function <em>that takes only values in</em> \(\mathbb{Q}\) (where
\(\mathbb{Q}\) is the set of rational numbers).</p>
</div>

<p>
This is a consequence of the following theorem:</p>

<div class="indent" id="theorem-1">

<p>
<strong>Theorem 1</strong> Suppose \(\mathfrak{c}\) is a calibration
measure and suppose \(C(\sim)\). Then the following are
equivalent:</p>

<ol type="i">

<li>\(c\) is a probability function on \(\mathcal{F}\) that takes only
values in \(\mathbb{Q}\);</li>

<li>There is a world at which \(c\) is actually calibrated. That is,
there is a world \(w\) in \(\mathcal{W}\) such that, for all \(A\) in
\(\mathcal{F}\), \(\mathfrak{c}(c(A), A, \mathcal{F}, \sim, w) =
0\).</li>
</ol>
</div>

<p>
Different versions of this theorem result from different constraints
\(C(\sim)\) on the equivalence relation \(\sim\) (van Fraassen 1983;
Shimony 1988), but the result is not surprising. An agent will satisfy
<strong>Possibility of Vindication</strong> just in case her credences
match the relative frequencies at some world. And those relative
frequencies will satisfy the probability axioms if \(C(\sim)\) and if
we have specified that condition correctly. That they will be rational
numbers follows from the definition of the relative frequency of a
proposition at a world.</p>

<p>
Thus, we have the following argument:</p>

<div class="indent" id="actual-rational-valued">

<p>
<strong>Actual Calibration argument for Rational-valued
Probabilism</strong></p>

<ul class="sentag tag2em">

<li><span class="tag">\((1)\)</span><span  class="sen"><a class="internal" href="#actual-calibration">Actual Calibration</a></span></li>
 
<li><span class="tag">\((2)\)</span><span  class="sen"><a class="internal" href="#possibility-vindication">Possibility of Vindication</a></span>e</li>
 
<li><span class="tag">\((3)\)</span><span  class="sen"><a class="internal" href="#theorem-1">Theorem 1</a></span></li>
 
<li>Therefore,</li>

<li><span class="tag">\((4)\)</span><span  class="sen"><a class="internal" href="#rational-valued">Rational-valued Probabilism</a></span></li>
 </ul>

</div>

<p>
Most proponents of the calibration argument are reluctant to accept a
norm that rules out every credence given by an irrational number. To
establish the weaker norm of <strong>Probabilism</strong>, there are
two strategies they might adopt. The first is to appeal to the
epistemic goal of hypothetical calibration instead of actual
calibration. This, together with
 <a class="internal" href="#possibility-vindication"><strong>Possibility of Vindication</strong></a>
 gives us <strong>Probabilism</strong> via the following theorem:</p>

<div class="indent" id="theorem-2">

<p>
<strong>Theorem 2</strong> Suppose \(C(\sim)\). Then the following are
equivalent:</p>

<ol type="i">

<li>\(c\) is a probability function on \(\mathcal{F}\).</li>

<li>There is a world at which \(c\) is hypothetically calibrated. That
is, there is a world \(w\) in \(\mathcal{W}\) such that, for all \(A\)
in \(\mathcal{F}\), \(\mathfrak{hc}(c(A), A, \mathcal{F}, \sim, w) =
0\).</li>
</ol>
</div>

<p>
The reason is that, while relative frequencies are always rational
numbers, the limit of an infinite sequence of rational numbers may be
an irrational number. And, in fact, for any irrational number, there
is a sequence of rational numbers that approaches it in the limit
(indeed, there are infinitely many such sequences).</p>

<p>
Thus, we have the following argument:</p>

<div class="indent" id="hypothetical-calibration-probabilism">

<p>
<strong>Hypothetical Calibration argument for Probabilism</strong></p>

<ul class="sentag tag2em">

<li><span class="tag">\((1)\)</span><span  class="sen"><a class="internal" href="#hypothetical-calibration">Hypothetical Calibration</a></span></li>
 
<li><span class="tag">\((2)\)</span><span  class="sen"><a class="internal" href="#possibility-vindication">Possibility of Vindication</a></span></li>
 
<li><span class="tag">\((3)\)</span><span  class="sen"><a class="internal" href="#theorem-2">Theorem 2</a></span></li>
 
<li>Therefore,</li>

<li><span class="tag">\((4)\)</span><span  class="sen"><a class="internal" href="#probabilism">Probabilism</a></span></li>
 </ul>
</div>

<p>
An alternative route to <strong>Probabilism</strong> changes the
decision-theoretic norm to which we appeal, rather than the sort of
calibration from which we wish our epistemic disutility function to
measure distance. The alternative norm is:</p>

<div class="indent" id="poss-arb-closeness">

<p>
<strong>Possibility of Arbitrary Closeness to Vindication.</strong> An
agent ought not to adopt an option unless there are worlds at which it
is arbitrarily close to achieving minimal disutility.</p>

<p>
That is: Suppose \(\mathcal{O}\) is a set of options, \(\mathcal{W}\)
is the set of possible worlds, and \(\mathfrak{U}\) is a disutility
function. Then, if \(o^*\) is an option, and if it is not the case
that, for any \(\varepsilon &gt; 0\), there is a possible 

world \(w^*_\varepsilon\) in \(\mathcal{W}\) such \[|
\mathfrak{U}(o^*, w^*_\varepsilon) - \min \{\mathfrak{U}(o, w) : o \in
\mathcal{O}\ \&amp;\ w \in \mathcal{W}\}| &lt; \varepsilon\] (when

these minima exist), then \(o^*\) is irrational.</p>
</div>

<p>
Together with the characterization of calibration measures given
above, suitable constraints \(C(\sim)\) on the equivalence relation
\(\sim\), and two extra assumptions, this norm does establish
<strong>Probabilism</strong>. The extra assumptions are these: First,
if our agent has a credence function \(c\) in \(\mathcal{C_F}\), the
possible worlds that we are considering include not only all
(consistent) truth assignments to \(\mathcal{F}\), but also any
(consistent) truth assignments to any (finite) algebra
\(\mathcal{F}'\) that extends \(\mathcal{F}\). And, second, given any
such \(\mathcal{F}'\), the equivalence relation \(\sim\) can be
extended in any possible way, providing the extension \(\sim'\) of
\(\sim\) satisfies \(C(\sim')\).</p>

<div class="indent" id="theorem-3">

<p>
<strong>Theorem 3</strong> Suppose \(C(\sim)\). Then the following are
equivalent:</p>

<ol type="i">

<li>\(c\) is a probability function on \(\mathcal{F}\).</li>

<li>For all \(\varepsilon &gt; 0\), there is a finite extension
\(\mathcal{F}'\) of \(\mathcal{F}\) and an extension \(\sim'\) of
\(\sim\) that satisfies \(C(\sim')\), and a possible world \(w'\) in
\(\mathcal{W}\) such that, for all \(A\) in \(\mathcal{F}\),
\(\mathfrak{c}(c(A), A, \mathcal{F}', \sim', w') &lt;
\varepsilon\)</li>
</ol>
</div>

<p>
Thus, if our agent satisfies <strong>Probabilism</strong>, then
however close she would like to be to actual calibration, there is
some possible world at which she is that close. And conversely.</p>

<p>
Thus, we have the following argument:</p>

<div class="indent" id="actual-calibration-probabilism">

<p>
<strong>Actual Calibration argument for Probabilism</strong></p>

<ul class="sentag tag2em">

<li><span class="tag">\((1)\)</span><span  class="sen"><a class="internal" href="#actual-calibration">Actual Calibration</a></span></li>
 
<li><span class="tag">\((2)\)</span><span  class="sen"><a class="internal" href="#poss-arb-closeness">Possibility of Arbitrary Closeness to Vindication</a></span></li>
 
<li><span class="tag">\((3)\)</span><span  class="sen"><a class="internal" href="#theorem-3">Theorem 3</a></span></li>
 
<li>Therefore,</li>

<li><span class="tag">\((4)\)</span><span  class="sen"><a class="internal" href="#probabilism">Probabilism</a></span></li>
 </ul>
</div>

<p>
These are the calibration arguments for <strong>Probabilism</strong>.
In the next section, we consider objections that may be raised against
them.</p>

<h3><a id="ObjCalArgForPro">4.3 Objections to calibration arguments for Probabilism</a></h3>

<p>
Objection 1: <em>Calibration is not an epistemic goal.</em> It may be
objected that neither actual nor hypothetical calibration measures are
<em>truth-directed</em> epistemic disutility functions, where this is
taken to be a necessary condition on such a function (Joyce 1998: 595;
Seidenfeld 1985). We say that a local epistemic disutility
function&mdash;that is, recall, an epistemic disutility function
defined for individual credences&mdash;is truth-directed if the
disutility that it assigns to a credence in a true proposition
increases as the credence decreases, and the disutility it assigns to
a credence in a false proposition increases as the credence increases.
Calibration measures do not have this property. To see this, let us
return to our toy example: the propositions <em>Heads on toss 1</em>,
&hellip;, <em>Heads on toss 1000</em> are in \(\mathcal{F}\) and they
are all relevantly similar according to \(\sim\). Now suppose that the
first coin toss lands heads, but all the others land tails. Then
credence 0.001 in <em>Heads on toss 1</em> is actually calibrated,
since exactly one out of one-thousand relevantly similar propositions
are true; so it has epistemic disutility 0. Credence 0.993, on the
other hand, is not, and thus receives a positive epistemic disutility.
However, it is a higher credence in a true proposition, and thus
should be assigned a lower epistemic disutility, according to the
requirement of truth-directedness. One natural response to this
objection is that it is question-begging. Proponents of the
calibration argument will simply reject the claim that an epistemic
disutility function must be truth-directed. Credences, unlike beliefs,
they might say, are not in the business of getting close to the truth;
they are in the business of getting close to being calibrated.</p>

<p>
Objection 2: <em>Limiting relative frequencies are not
well-defined.</em> To define the limiting relative frequency of \(A\)
at a world \(w\), we require that there is a unique sequence of
extensions of the algebra each of which contains more propositions
that are relevantly similar to \(A\) than the previous extension, and
a corresponding sequence of relative frequencies of truths amongst the
propositions like \(A\) in the corresponding algebra. But the
assumption of such a unique sequence is extremely controversial and
the problems to which it gives rise have haunted hypothetical
frequentism about objective probability (H&aacute;jek 2009).</p>

<p>
Objection 3: <em>Neither
 <a class="internal" href="#possibility-vindication"><strong>Possibility of Vindication</strong></a>
 nor
 <a class="internal" href="#poss-arb-closeness"><strong>Possibility of Arbitrary Closeness to Vindication</strong></a>
 is a norm.</em> It might be that the only actions that give rise to
the possibility of vindication or of arbitrary closeness to
vindication also give rise to the possibility of maximal distance from
vindication. And it might be that there are actions that do not give
rise to the possibility of vindication or of arbitrary closeness to
vindication, but do limit the distance from vindication that is risked
by choosing that action. In such cases, it is not at all clear that it
is rationally required of an agent that she ought to risk maximal
distance from vindication in order to leave open the possibility of
vindication or of arbitrary closeness to vindication. Compare: I have
two options&mdash;if I choose option 1, I will receive &pound;0 or
&pound;100, but I don&rsquo;t know which; if I choose option 2, I will
receive &pound;99 for sure. Even before they know the objective
chances of the two possibilities that the first option creates, many
people will opt for the second. However, by doing so, they rule out
the possibility that they will receive the maximum possible utility,
which is obtained by option 1 if I receive &pound;100. It seems that
ruling out such a possibility is not irrational. To put it another
way:
 <a class="internal" href="#possibility-vindication"><strong>Possibility of Vindication</strong></a>
 and
 <a class="internal" href="#poss-arb-closeness"><strong>Possibility of Arbitrary Closeness to Vindication</strong></a>
 are extreme risk-seeking norms. That is, they suggest that we make
our decisions by trying to maximise the utility we obtain in our
best-case scenario. But while it might be rationally permissible to be
so risk-seeking, it is certainly not mandatory (Easwaran &amp;
Fitelson 2015: Section 8).</p>

<p>
Objection 4: <em>The constraints on \(\sim\) are ill-motivated.</em>
This objection will vary with the constraints \(C(\sim)\) that are
imposed on \(\sim\). One uncontroversial constraint is this: If \(A
\sim B\), then \(c(A) = c(B)\). The further constraints imposed by
Shimony (1988) and van Fraassen (1983) are more controversial (Joyce
1998: 594&ndash;6). Moreover, they limit the application of the
result, since they involve assumptions about the form of the
propositions in \(\mathcal{F}\). Thus, the calibration arguments do
not show in general, of any finite algebra \(\mathcal{F}\), that a
credence function on \(\mathcal{F}\) ought to be a probability
function, since not every such algebra will contain propositions with
the form required by the constraints \(C(\sim)\).</p>

<h2><a id="AccArg">5. Accuracy Arguments</a></h2>

<p>
In this section, we move from calibration arguments to accuracy
arguments for <strong>Probabilism</strong>. These arguments have the
same structure as the calibration arguments. They consist of a
mathematically-precise account of epistemic disutility and a
decision-theoretic norm. And they derive, from that norm together with
that account of disutility, an epistemic norm. In particular, they
derive <strong>Probabilism</strong>. And that derivation goes via a
mathematical theorem. However, they will use different accounts of
epistemic disutility and different decision-theoretic norms.</p>

<p>
In this section, we will begin with the original accuracy-based
argument for <strong>Probabilism</strong> due to James M. Joyce (1998;
see also Rosenkrantz 1981; both versions, as well as subsequent
versions, build on mathematical results due to de Finetti (1974)).
Then we&rsquo;ll consider its various components in turn, and explore
the objections they have elicited and the adjustments that have been
made to them.</p>

<h3><a id="JoyAccArgForPro">5.1 Joyce&rsquo;s accuracy argument for Probabilism</a></h3>

<p>
Joyce&rsquo;s argument consists of an account of the epistemic
disutility of credences and a decision-theoretic norm. Let&rsquo;s
consider each in turn.</p>

<p>
Joyce&rsquo;s account of the epistemic disutility of credences itself
consists of two components. The first identifies epistemic disutility
with gradational inaccuracy; the second gives a mathematically-precise
account of gradational inaccuracy.</p>

<p id="credal-veritism">
In more detail: The first component of Joyce&rsquo;s account of
epistemic disutility for credences is the claim&mdash;which we will
call <strong>Credal Veritism</strong>, partly following Goldman (2002:
58)&mdash;that the only source of value for credences that is relevant
to their epistemic status is their <em>gradational accuracy</em>,
where the gradational accuracy of a credence in a true proposition is
higher when the credence is closer to 1, which we might think of as
the ideal or vindicated credence in a true proposition, while the
gradational accuracy of a false proposition is higher when the
credence is closer to 0, which we might think of as the ideal or
vindicated credence in a false proposition. Thus, the only source of
<em>dis</em>value for credences is their gradational
<em>in</em>accuracy.</p>

<p id="joycean-inaccuracy">
The second component of Joyce&rsquo;s account of epistemic disutility
for credences is a set of mathematically-precise conditions that a
measure of the gradational inaccuracy of a credence function at a
given possible world must satisfy. A putative inaccuracy measure for
credence functions over an algebra \(\mathcal{F}\) is a mathematical
function \(\mathfrak{I}\) that takes a credence function \(c\) in
\(\mathcal{C_F}\) and a possible world \(w\) in \(\mathcal{W_F}\) and
returns a number \(\mathfrak{I}(c, w)\) in \([0, \infty]\) that
measures the inaccuracy of \(c\) at \(w\). (The set \([0, \infty]\)
contains all non-negative real numbers together with \(\infty\).) Here
is an example, called the <em>Brier score</em>: 

\[\mathfrak{B}(c, w) := \sum_{X \in \mathcal{F}} |v_w(X) - c(w)|^2\]

 Thus, the
Brier score measures the inaccuracy of a credence function at a world
as follows: it takes each proposition to which the credence function
assigns credences; it takes the difference between the credence that
the credence function assigns to that proposition and the ideal or
vindicated credence in that proposition at that world; it squares this
difference; and it sums up the results. We shall not give all of
Joyce&rsquo;s conditions here, but just note that the Brier score just
defined satisfies them all. Let us say that any putative inaccuracy
measure \(\mathfrak{I}\) that satisfies these conditions is a
<em>Joycean inaccuracy measure</em>. And let <strong>Joycean
Inaccuracy</strong> be the claim that all legitimate inaccuracy
measures are Joycean inaccuracy measures.</p>

<p>
Combining <strong>Credal Veritism</strong> and <strong>Joycean
Inaccuracy</strong>, we have the claim that the epistemic disutility
of a credence function at a world is given by its inaccuracy at that
world as measured by a Joycean inaccuracy measure.</p>

<p>
Let us turn now to the decision-theoretic norm to which Joyce appeals.
We have met it already above in the introduction to this article: it
is the norm of <strong>Naive Dominance</strong>. We will state it here
precisely:</p>

<div class="indent" id="naive-dominance">

<p>
<strong>Naive Dominance</strong> A rational agent will not adopt an
option when there is another option that has lower disutility at all
worlds.</p>

<p>
That is: Suppose \(\mathcal{O}\) is a set of options, \(\mathcal{W}\)
is the set of possible worlds, and \(\mathfrak{U}\) is a disutility
function. Then, if \(o^*\) is an option, and if there is another
option \(o'\) such that \(\mathfrak{U}(o', w) &lt; \mathfrak{U}(o^*,
w)\) for all worlds \(w\) in \(\mathcal{W}\), then \(o^*\) is
irrational. (In this situation, we say that \(o^*\)
\(\mathfrak{U}\)-dominates \(o'\).)</p>
</div>

<p>
The idea behind <strong>Naive Dominance</strong> is this: If there is
one option that is guaranteed to have lower disutility than another
option, then the latter is guaranteed to be worse than the former; so
the agent can know <em>a priori</em> that the latter is worse than the
former. And surely it is irrational to adopt an option if there is
another that you know <em>a priori</em> to be better.</p>

<p>
Thus, we have the substantial components of Joyce&rsquo;s argument:
<strong>Credal Veritism</strong>, <strong>Joycean Inaccuracy</strong>,
and <strong>Naive Dominance</strong>. From these, we can derive
<strong>Probabilism</strong> via the following mathematical theorem
Joyce (1998: 597&ndash;600):</p>

<div class="indent" id="theorem-4">

<p>
<strong>Theorem 4 (Joyce&rsquo;s Main Theorem)</strong> Suppose
\(\mathcal{F}\) is an algebra and \(\mathfrak{I} : \mathcal{C_F}
\times \mathcal{W_F} \rightarrow [0, \infty]\) is a Joycean inaccuracy
measure for the credence functions on \(\mathcal{F}\). Now suppose
that \(c^*\) is a credence function in \(\mathcal{C_F}\) that violates
<strong>Probabilism</strong>. Then there is a credence function \(c'\)
in \(\mathcal{C_F}\) such that \(\mathfrak{I}(c', w) &lt;
\mathfrak{I}(c^*, w)\) for all \(w\) in \(\mathcal{W_F}\). (In this
situation, we say that <em>\(c'\) accuracy dominates \(c^*\) relative
to \(\mathfrak{I}\)</em>.)</p>
</div>

<p>

 <a href="#fig1">Figure 1</a>
 illustrates this result in the particular very simple case in which
\(\mathcal{F}\) contains just a proposition, <em>Heads</em>, and its
negation, <em>Tails</em>, and inaccuracy is measured using the Brier
score.</p>

<p>
Thus, we have the following argument:</p>

<div class="indent">

<p>
<strong>Joyce&rsquo;s accuracy argument for Probabilism</strong></p>

<ul class="sentag tag2em">

<li><span class="tag">\((1)\)</span><span  class="sen"><a class="internal" href="#credal-veritism">Credal Veritism</a>
 +
 <a class="internal" href="#joycean-inaccuracy">Joycean Inaccuracy</a></span></li>
 
<li><span class="tag">\((2)\)</span><span  class="sen"><a class="internal" href="#naive-dominance">Naive Dominance</a></span></li>
 
<li><span class="tag">\((3)\)</span><span  class="sen"><a class="internal" href="#theorem-4">Theorem 4</a></span></li>
 
<li>Therefore,</li>

<li><span class="tag">\((4)\)</span><span  class="sen"><a class="internal" href="#probabilism">Probabilism</a></span></li>
 </ul>

</div>

<div class="figure" id="fig1">
<img src="fig1.png" width="314" height="303" alt="[a graph of two vertical lines and two horizontal lines forming a square but lines extend beyond the intersections. The left vertical line is labelled 'Tails' and the lower horizontal line labelled 'Heads'. The upper left corner is labelled \(v_{w_1}\) , the lower right corner is labelled \(v_{w_2}\)] and a diagonal linke connects the two. Two arcs, one stretches from the lower left vertical line to the upper right horizontal line and the other from the upper right vertical line to the lower left horizontal line. The two arcs intersect twice. The upper right intersection is labelled \(c*\) and a point on the diagonal line in the middle of the intersection space is labelled \(c'\)." />

<p style="line-height: 120%">
<span class="figlabel">Figure 1:</span> In this figure, we plot the
various possible credence functions defined on a proposition
<em>Heads</em> and its negation <em>Tails</em> in the unit square.
Thus, we plot the credence in <em>Heads</em> along the horizontal axis
and the credence in <em>Tails</em> up the vertical axis. We also plot
the vindicated credence functions \(v_{w_1}\) and \(v_{w_2}\) for the
two worlds \(w_1\) (at which <em>Tails</em> is true and <em>Heads</em>
is false) and \(w_2\) (at which <em>Heads</em> is true and
<em>Tails</em> is false). The diagonal line between them contains all
and only the credence functions on these two propositions that are
probability functions and thus satisfy <strong>Probabilism</strong>.
\(c^*\) (which assigns 0.7 to <em>Heads</em> and 0.6 to
<em>Tails</em>) violates <strong>Probabilism</strong>. The lower
right-hand arc contains all the credence functions that are exactly as
inaccurate as \(c\) at world \(w_2\), where that inaccuracy is
measured using the Brier score. To see this, note that the Brier score
of \(c^*\) at \(w_2\) is the square of the Euclidean distance of the
point \(c^*\) from the point \(v_{w_2}\). Thus, the credence functions
that have exactly the same Brier score as \(c^*\) at \(w_2\) are those
that lie equally far from \(v_{w_2}\). For the same reason, the upper
left-hand arc contains all the credence functions that are exactly as
inaccuracy as \(c\) at world \(w_1\). Every credence function that
lies between the two arcs is more accurate than \(c^*\) at both
worlds. These are the ones whose squared Euclidean distance from
\(v_{w_2}\) is less than the squared Euclidean distance of \(c^*\)
from \(v_{w_2}\), and similarly for \(v_{w_1}\). It assigns 0.55 to
<em>Heads</em> and 0.45 to <em>Tails</em>. \(c'\) is such a credence
function. \(c'\) also satisfies <strong>Probabilism</strong>. </p>
</div>

<h3><a id="SouEpiDis">5.2 The source(s) of epistemic disutility</a></h3>

<p>
Let us start by considering the first of the two components that
comprise Joyce&rsquo;s account of epistemic disutility for credences,
namely,
 <strong><a class="internal" href="#credal-veritism">Credal Veritism</a></strong>.
 This says that the sole fundamental source of epistemic disutility
for a credence is its gradational inaccuracy. That is, any other vice
that the credence has must derive from this vice (Goldman 2002:
52).</p>

<p>
First, let&rsquo;s note why it is important to make this assumption.
Would it not be sufficient to say merely that one of the sources of
disutility for a credence is its inaccuracy, and then to point out
that any credence function that isn&rsquo;t a probability function is
accuracy dominated? If it could always be guaranteed that, for any
non-probabilistic credence function, none of its accuracy dominators
has any epistemic vice to a greater degree than does the credence
function it dominates, then this would be sufficient. But, if it were
possible that every accuracy dominator is guaranteed to be better
along the dimension of inaccuracy but worse along some other dimension
of epistemic disutility, then being accuracy dominated would not rule
out a credence function as irrational. Thus, if the accuracy dominance
argument for <strong>Probabilism</strong> is to work, we must claim,
with
 <strong><a class="internal" href="#credal-veritism">Credal Veritism</a></strong>,
 that inaccuracy is the only source of epistemic disutility for
credences.</p>

<p>
How are we to establish this? How can we be sure there aren&rsquo;t
other sources of disutility. For instance, perhaps it is a virtue of a
credence function if the credences it assigns cohere with one another
in a particular way, and a vice if they do not. This is a
<em>coherentist</em> claim of the sort endorsed for full beliefs,
rather than credences, by the likes of BonJour (1985) and Harman
(1973). Or perhaps it is a virtue of a credence in a particular
proposition if it matches the degree of support given to that
proposition by the agent&rsquo;s current total evidence. This claim is
dubbed <em>evidential proportionalism</em> by Goldman (2002:
55&ndash;7). Recent proponents might include Williamson (2000) and
White (2009). Another possibility: perhaps the <em>verisimilitude</em>
or <em>truthlikeness</em> of your credences is a source of their
epistemic utility over and above their gradational accuracy (see entry
on
 <a href="../truthlikeness/index.html">truthlikeness</a>).
 Graham Oddie (2019) argues in favour of this. Each of these seem
plausible. How is the credal veritist to answer the objection that
there are sources of epistemic disutility, such as these three, that
go beyond gradational inaccuracy? Of course, it is notoriously
difficult to prove a negative existential claim, such as the credal
veritist claim that there are no other epistemic vices beyond
inaccuracy. But here is a natural strategy: for each proposed
candidate epistemic vice besides accuracy, the credal veritist should
provide an account of how its badness derives from the badness of
inaccuracy.</p>

<p>
In the case of the coherentist described above, who proposes that it
is a vice to have credences that fail to cohere in a particular way,
there is a very natural instance of this strategy. The coherence that
we demand of credences is precisely that they relate to one another in
the way that <strong>Probabilism</strong> demands, so that, for
instance, no disjunction is assigned lower credence than is assigned
to either of the disjuncts, no proposition is assigned very high
credence at the same time that its negation is also assigned very high
credence, and so on. If that is correct, then of course Joyce&rsquo;s
accuracy argument for <strong>Probabilism</strong> detailed above
provides an argument that this vice derives its badness from the
badness of inaccuracy: after all, if a credence function lacks the
coherence that the coherentist considers virtuous, they will be
accuracy dominated.</p>

<p>
What of the evidential proportionalist? Here it is a little more
difficult. There are principles that the evidential proportionalist
will take to govern evidential support that go beyond merely
<strong>Probabilism</strong>, which is a relatively weak and
undemanding principle. So it is not sufficient to point to the
accuracy argument for that principle in the way we did in response to
the coherentist. However, here is an attempt at an answer. It comes
from collecting together a series of accuracy arguments for other
principles of rationality that we take to govern our credences. For
instance, Greaves &amp; Wallace (2006) and Briggs &amp; Pettigrew
(2018) give an accuracy argument for the principle of
conditionalization, which says that, if an agent is rational, her
credence function at a later time will be obtained from her credence
function at an earlier time by conditionalizing on the total evidence
she obtains between those two times; Easwaran (2013) and Huttegger
(2013) extend the argument, and Schoenfield (2016) and Carr (2019)
clarify the norm that it establishes. Moreover, Pettigrew (2013a)
gives an accuracy argument for the Principal Principle, which says
that, if an agent is rational, her credences in propositions
concerning the objective chances will relate to propositions to which
those chances attach in a particular way. Pettigrew (2014b) and Konek
(2016) give rather different accuracy-based arguments for the
Principle of Indifference, which says how a rational agent with no
evidence will distribute their credences. Moss (2011), Lam (2013), and
Levinstein (2015) describe principles that rational agents will obey
in the presence of peer disagreement and provide accuracy-based
arguments in their favour. And finally Horowitz (2014) uses
accuracy-based arguments to evaluate various species of permissivism.
The point is that, piece by piece, the principles that are taken to
govern the degree of support provided to a proposition by a body of
evidence are being shown to follow from accuracy considerations alone.
This, it seems, constitutes a response to the concerns of the
evidential proportionalist.</p>

<p>
Christopher Meacham (2018) objects to this response in two ways:
first, he argues that the different decision-theoretic norms that are
used in the justifications of the various credal norms listed above
might be incompatible with one another; and, second, he worries that
some of the decision-theoretic norms that are used in those
justifications are not themselves purely alethic and therefore fail to
provide purely veritistic justifications of the norms in question.</p>

<p>
Both the response to the coherentist and the response to the
evidential proportionalist leave the accuracy argument for
<strong>Probabilism</strong> in a strange position. The argument for,
or defence of, one component of its first premise, namely,
 <strong><a class="internal" href="#credal-veritism">Credal Veritism</a></strong>
 appeals to an argument of which it is a premise! In fact, this
isn&rsquo;t problematic. The credal veritist and her opponent might
agree that the argument at least establishes a conditional:
<em>if</em> credal veritism is true, <em>then</em> probabilism is
true. You need not accept credal veritism to accept that conditional.
And it is that conditional to which the credal veritist appeals in
defending her position against the coherentist and the evidential
proportionalist. Having successfully defended credal veritism in this
way, she can then appeal to its truth to derive
<strong>Probabilism</strong>.</p>

<p>
Before we consider how to measure inaccuracy in the next section,
let&rsquo;s consider the claim that verisimilitude or truthlikeness is
a further source of epistemic utility. It is most easily introduced by
an example. Suppose I am interested in how many stars there are on the
flag of Suriname. I have credences in three propositions: <em>1</em>
(which says there&rsquo;s one star), <em>2</em> (which says there are
two), and <em>3</em> (which says three). In fact, there is one star on
the flag. That is, <em>1</em> is true at the actual world, while
<em>2</em> and <em>3</em> are false. Now consider two different
credence functions on these three propositions:</p>

<ul>

<li> \(c(1) = 0\), \(c(2) = 0.5\), \(c(3) = 0.5\)</li>

<li> \(c'(1) = 0\), \(c'(2) = 1\), \(c'(3) = 0\)</li>
</ul>

<p>
That is, \(c\) and \(c'\) both assign credence 0 to the true
proposition, <em>1</em>; they are certain that there are either 2 or 3
stars on the flag, but while \(c\) spreads its credence equally over
these two false options, \(c'\) is certain of the first. According to
Oddie (2019), \(c'\) has greater truthlikeness than \(c\) at the
actual world because it assigns a higher credence to a proposition
that, while false, is more truthlike, namely, <em>2</em>, and it
assigns a lower credence to a proposition that is, while also false,
less truthlike, namely, <em>3</em>. On this basis, he argues that any
measure of epistemic disutility must judge \(c\) to be worse than
\(c'\). However, he notes, nearly all measures of gradational accuracy
that are used in accuracy dominance arguments for
<strong>Probabilism</strong> will not judge in that way: they will
judge \(c'\) worse than \(c\). And indeed those that do so judge will
fail to respect truthlikeness in other ways. Jeffrey Dunn (2018) and
Miriam Schoenfield (2019) respond to Oddie&rsquo;s arguments.</p>

<h3><a id="MeaIna">5.3 Measures of inaccuracy</a></h3>

<h4><a id="JoyCon">5.3.1 Joyce on Convexity</a></h4>

<p>
So much, then, for the first component of the first premise of the
accuracy argument for <strong>Probabilism</strong>, namely,
 <strong><a class="internal" href="#credal-veritism">Credal Veritism</a></strong>.
 In this section, we turn to the second component, namely,
 <strong><a class="internal" href="#joycean-inaccuracy">Joycean Inaccuracy</a></strong>.
 Let&rsquo;s focus on a particular condition that Joyce places on
measures of inaccuracy, namely, <strong>Strong Convexity</strong>
(Joyce calls it Weak Convexity, but we change the name in this
presentation because, as Patrick Maher (2002) points out, it is
considerably stronger than Joyce imagines.)</p>

<div class="indent" id="strongconvex">

<p>
<strong>Strong Convexity</strong> Suppose \(\mathfrak{I}\) is a
legitimate inaccuracy measure. Then if \(c \neq c'\) and
\(\mathfrak{I}(c, w) = \mathfrak{I}(c', w)\), then 

\[\mathfrak{I}\left(\frac{1}{2}c + \frac{1}{2}c', w\right) &lt;
\mathfrak{I}(c, w) = \mathfrak{I}(c', w)\] (Given two credence

functions, \(c\) and \(c'\), we define a third credence function
\(\frac{1}{2}c + \frac{1}{2} c'\) as follows: the credence that
\(\frac{1}{2}c + \frac{1}{2}c'\) assigns to a proposition is the
straight average of the credences that \(c\) and \(c'\) assign to it.
Thus, \((\frac{1}{2}c + \frac{1}{2}c')(X) = \frac{1}{2}c(X) +
\frac{1}{2}c'(X).\) We call this the <em>equal mixture of \(c\) and
\(c'\)</em>.)</p>
</div>

<p>
This says that, for any two distinct credence functions that are
equally inaccurate at a given world, the third credence function
obtained by &ldquo;splitting the difference&rdquo; between them and
taking an equal mixture of the two is less inaccurate than either of
them. Here is Joyce&rsquo;s justification of this condition:</p>

<blockquote>

<p>
[Strong] Convexity is motivated by the intuition that extremism in the
pursuit of accuracy is no virtue. It says that if a certain change in
a person&rsquo;s degrees of belief does not improve accuracy then a
more radical change in the same direction and of the same magnitude
should not improve accuracy either. Indeed, this is just what the
principle says. (Joyce 1998: 596)</p>
</blockquote>

<p>
Joyce&rsquo;s point is this: Suppose we have three credence functions,
\(c\), \(m\), and \(c'\). And suppose that, to move from \(m\) to
\(c'\) is just to move in the same direction and by the same amount as
to move from \(c\) to \(m\), which is exactly what will be true if
\(m\) is the equal mixture of \(c\) and \(c'\). Now suppose that \(m\)
is at least as inaccurate as \(c\)&mdash;that is, the change from
\(c\) to \(m\) does not &ldquo;improve accuracy&rdquo;. Then, Joyce
claims, \(c'\) must be at least as inaccurate as \(m\)&mdash;that is,
the change from \(m\) to \(c'\) also does not &ldquo;improve
accuracy&rdquo;.</p>

<p>
Objection: <em>The justification given doesn&rsquo;t justify
<strong>Strong Convexity</strong>.</em> The problem with this
justification is that it establishes a weaker principle than
<strong>Strong Convexity</strong>. This was first pointed out by
Patrick Maher (2002), who noted that Joyce&rsquo;s justification in
fact motivates the following weaker principle:</p>

<div class="indent">

<p>
<strong>Weak Convexity</strong> Suppose \(\mathfrak{I}\) is a
legitimate inaccuracy measure. Then if \(c \neq c'\) and
\(\mathfrak{I}(c, w) = \mathfrak{I}(c', w)\), then 

\[\mathfrak{I}\left(\frac{1}{2}c + \frac{1}{2}c', w\right) \leq
\mathfrak{I}(c, w) = \mathfrak{I}(c', w)\]</p>

</div>

<p>
That is, Joyce&rsquo;s motivation rules out situations in which
inaccuracy <em>increases</em> from \(c\) to \(m\) and then
<em>decreases</em> from \(m\) to \(c'\). And this is what <strong>Weak
Convexity</strong> also rules out. But <strong>Strong
Convexity</strong> furthermore rules out situations in which
inaccuracy <em>remains the same</em> from \(c\) to \(m\) and then from
\(m\) to \(c'\). And Joyce has given no reason to think that such
changes are problematic. What&rsquo;s more, as Maher proves, the
stronger convexity condition is crucial for Joyce&rsquo;s proof. With
only the weaker condition, the theorem is false.</p>

<h4><a id="LocGloIna">5.3.2 Local and global inaccuracy</a></h4>

<p id="brier-inaccuracy">
In this section, we consider alternative sets of conditions on
inaccuracy measures that are presented by Leitgeb &amp; Pettigrew
(2010a). These propose that we replace the claim
 <strong><a class="internal" href="#joycean-inaccuracy">Joycean Inaccuracy</a></strong>
 in Joyce&rsquo;s accuracy argument for <strong>Probabilism</strong>
with an alternative claim that says that the legitimate inaccuracy
measures are (amongst) those that satisfy Leitgeb and
Pettigrew&rsquo;s alternative conditions. Unlike Joyce&rsquo;s
conditions, these are sufficient to narrow the field of legitimate
inaccuracy measures to just a single one, namely, the Brier score
\(\mathfrak{B}\) that we met in
 <a class="internal" href="#JoyAccArgForPro">section 5.1</a>
 above. Let us say that <strong>Brier Inaccuracy</strong> is the claim
that the Brier score is the only legitimate measure of inaccuracy. And
note that, if we replace
 <strong><a class="internal" href="#joycean-inaccuracy">Joycean Inaccuracy</a></strong>
 with <strong>Brier Inaccuracy</strong> in Joyce&rsquo;s argument for
<strong>Probabilism</strong>, we retain our argument for that
epistemic norm:</p>

<div class="indent">

<p>
<strong>Brier accuracy-based argument for Probabilism: I</strong></p>

<ul class="sentag tag2em">

<li><span class="tag">\((1)\)</span><span  class="sen"><a class="internal" href="#credal-veritism">Credal Veritism</a>
 +
 <a class="internal" href="#brier-inaccuracy">Brier Inaccuracy</a></span></li>
 
<li><span class="tag">\((2)\)</span><span  class="sen"><a class="internal" href="#naive-dominance">Naive Dominance</a></span></li>
 
<li><span class="tag">\((3)\)</span><span  class="sen"><a class="internal" href="#theorem-4">Theorem 4</a></span></li>
 
<li>Therefore,</li>

<li><span class="tag">\((4)\)</span><span  class="sen"><a class="internal" href="#probabilism">Probabilism</a></span></li>
 </ul>
</div>

<p>
So far, in this section, we have been concerned only with what we
might call <em>global</em> measures of inaccuracy&mdash;that is,
measures of the inaccuracy of entire credence functions. Leitgeb and
Pettigrew are certainly interested in those. But they are also
interested in what we might call <em>local</em> measures of
inaccuracy&mdash;that is, measures of the inaccuracy of individual
credences. Indeed, they are interested in how these two sorts of
inaccuracy measure interact. They lay down constraints on each of the
inaccuracy measures individually, and then they lay down constraints
on how they combine. The guiding idea in each case is that any feature
of the inaccuracy of credences that is determined from the point of
view of local inaccuracy measures&mdash;such as their total
inaccuracy, or the urgency with which an agent with inaccurate
credences should change them&mdash;should match that same feature when
it is determined from the point of view of global inaccuracy measures.
If this doesn&rsquo;t happen, then the agent will face a rational
dilemma when choosing which of the two ways she should use to
determine that feature. Here, we shall focus only on one of the most
powerful of Leitgeb and Pettigrew&rsquo;s conditions, which also turns
out to be the most problematic. Here it is:</p>

<div class="indent" id="global-normdom">

<p>
<strong>Global Normality and Dominance</strong> If \(\mathfrak{I}\) is
a legitimate global inaccuracy measure, there is a strictly increasing
\(f:[0, \infty) \rightarrow [0, \infty)\) such 

\[\mathfrak{I}(c, w) = f(||v_w - c||_2).\] where, for any two credence

 functions
\(c\), \(c'\) defined on \(\mathcal{F}\), 

\[||c - c'||_2 := \sqrt{\sum_{X \in \mathcal{F}} |c(X) - c'(X)|^2}\]

 and we call
\(||c - c'||_2\) the <em>Euclidean distance between \(c\) and
\(c'\)</em>; and, recall, \(v_w\) is the omniscient credence function
at \(w\), so that \(v_w(X) = 1\) if \(X\) is true at \(w\) and
\(v_w(X) = 0\) if \(X\) is false at \(w\).</p>
</div>

<p>
Thus, <strong>Global Normality and Dominance</strong> says that the
inaccuracy of a credence function at a world should supervene in a
certain way upon the Euclidean distance between that credence function
and the omniscient credence function at that world. Indeed, it should
be a strictly increasing function of that distance between them.</p>

<p>
Objection 1: <em>There is no motivation for the appeal to Euclidean
distance.</em> Leitgeb and Pettigrew show that the only inaccuracy
measure that satisfies
 <strong><a class="internal" href="#global-normdom">Global Normality and Dominance</a></strong>,
 together with their other conditions on inaccuracy measures, is the
Brier score, which we defined above. That is, imposing these
conditions entails
 <strong><a class="internal" href="#brier-inaccuracy">Brier Inaccuracy</a></strong>.
 The problem with this characterization, however, is that it depends
crucially on the appeal to the Euclidean distance made in
 <strong><a class="internal" href="#global-normdom">Global Normality and Dominance</a></strong>,
 and no reason is given for appealing to the Euclidean distance
measure in particular, rather than some other measure of distance
between credence functions. Suppose we replace that condition with one
that says that a legitimate global inaccuracy measure must be a
strictly increasing function of the so-called <em>Manhattan</em> or
<em>city block</em> distance measure, where the distance between two
credence functions measured in this way is defined as follows:

\[||c - c'||_1 := \sum_{X \in \mathcal{F}} |c(X) - c'(X)|\] That is,

 the Manhattan distance between two credence functions is
obtained by summing the differences between the credences they each
assign to the various propositions on which they are defined. Together
with the other constraints that Leitgeb and Pettigrew place on
inaccuracy measures, this alternative constraint entails that the only
legitimate inaccuracy measure is the so-called <em>absolute value
score</em>, which is defined as follows: 

\[\mathfrak{A}(c, w) := \sum_{X \in \mathcal{F}} |v_w(X) - c(X)|\]</p>

<p>
Now, it turns out that the absolute value score cannot ground an
accuracy argument for <strong>Probabilism</strong>. In fact, there are
situations in which non-probabilistic credence functions accuracy
dominate probabilistic credence functions when inaccuracy is measured
using the absolute value score. Let \(\mathcal{F} = \{X_1, X_2,
X_3\}\), where \(X_1\), \(X_2\), and \(X_3\) are mutually exclusive
and exhaustive propositions. And consider the following two credence
functions: \(c(X_i) = \frac{1}{3}\) for each \(i = 1, 2, 3\);
\(c'(X_i) = 0\) for each \(i = 1, 2, 3\). The former, \(c\), is
probabilistic; the latter, \(c'\), is not. But, if we measure
inaccuracy using the absolute score, the inaccuracy of \(c\) at each
of the three possible worlds is \(\frac{4}{3}\), whereas the
inaccuracy of \(c'\) at each of the three possible worlds is \(1\).
The upshot of this observation is that it is crucial, if our accuracy
argument for <strong>Probabilism</strong> is to succeed, to rule out
the absolute value score. The problem with the Leitgeb and Pettigrew
characterization is that it rules out this measure essentially by
fiat. It rules it out by demanding that the inaccuracy of a credence
function at a world supervenes on the Euclidean distance between the
credence function and the omniscient credence function at that world.
But it gives no reason for favouring this measure of distance over
another, such as Manhattan distance.</p>

<p>
Objection 2: <em>Using the Brier score to measure inaccuracy has
unintuitive consequences.</em> A further objection to Leitgeb and
Pettigrew&rsquo;s characterization of inaccuracy measures is given by
Levinstein (2012). In the sequel to the paper in which they give this
characterization, Leitgeb and Pettigrew use it to argue in favour of
an updating rule for credences that applies in the same situations as
so-called Jeffrey Conditionalization (or Probability Kinematics) but
offers different advice (Jeffrey 1965; Leitgeb &amp; Pettigrew 2010b).
Levinstein objects to the use of the Brier score to measure inaccuracy
on the grounds that this alternative updating rule gives deeply
unintuitive results.</p>

<h4><a id="CalAcc">5.3.3 Calibration and accuracy</a></h4>

<p>
The final characterization of inaccuracy measures that we consider
here is due to Pettigrew (2016). Again, we won&rsquo;t enumerate all
of the conditions here. Instead, we&rsquo;ll describe the most
contentious and mathematically powerful of the conditions&mdash;the
one that in some sense does the main mathematical
&ldquo;heavylifting&rdquo; when it comes to showing what putative
inaccuracy measures these conditions permit.</p>

<p>
So far in this entry, we have presented calibration accounts of
epistemic utility and accuracy accounts as separate and incompatible.
The condition on inaccuracy measures that Pettigrew proposes and that
we consider in this section denies that. Rather, it claims that
closeness to calibration in fact plays a role in determining the
accuracy of a credence function; the difference between this approach
and the calibration arguments of section
 <a href="#CalArg">4</a>
 is that Pettigrew does not think that closeness to calibration is the
whole story. Let \(\mathfrak{D}\) be a putative measure of the
distance between two credence functions. That is, \(\mathfrak{D} :
\mathcal{C_F} \times \mathcal{C_F} \rightarrow [0, \infty]\), and
we&rsquo;ll assume that \(\mathfrak{D}(c, c') = 0\) iff \(c = c'\).
Now first we use this measure of distance to define a measure of the
distance that a credence function lies from being perfectly calibrated
at a world. Then, following a point already made above in our
treatment of calibration arguments for <strong>Probabilism</strong>,
we note that this, on its own, cannot define a measure of inaccuracy
because it lacks a crucial feature that we demand of any such measure:
it is not truth-directed. However, we then note how to supplement the
measure of distance from calibration in order to give an inaccuracy
measure that does have the crucial feature. And we claim that all
inaccuracy measures are produced by supplementing a measure of
distance from calibration in this way.</p>

<p>
As in
 <a href="#CalMea">section 4.1</a>,
 we let \(\sim\) be an equivalence relation on the set \(\mathcal{F}\)
of propositions to which our agent assigns opinions. It is the
relation of relevant similarity between two propositions. In
 <a href="#CalMea">section 4.1</a>,
 we said that we would impose conditions \(C(\sim)\) on this
equivalence relation, but we said no more to identify those
conditions. In this section, we in fact define this equivalence
relation. We take it to be relative to a credence function \(c\), so
we write it \(\sim_c\), and we define it as follows: \(A \sim_c B\)
iff \(c(A) = c(B)\). That is, two propositions are relevantly similar
for our agent with credence function \(c\) if \(c\) assigns them the
same credence. Thus, given a possible world \(w\), we say that a
credence function \(c\) is <em>perfectly calibrated at \(w\)</em> if,
for each \(A\) in \(\mathcal{F}\), 

\[c(A) = \mathrm{Freq}(\mathcal{F}, A, \sim_c, w)\]</p>

<p>
Next, given a credence function \(c\) and a world \(w\), the
<em>perfectly calibrated counterpart of \(c\) at \(w\)</em> is a
credence function also defined on \(\mathcal{F}\) that is defined as
follows: for each \(A\) in \(\mathcal{F}\) 

\[c^w(A) = \mathrm{Freq}(\mathcal{F}, A, \sim_c, w)\]

 That is, the
perfectly calibrated counterpart of \(c\) at \(w\) assigns to each
proposition \(A\) the frequency of truths at \(w\) amongst all
propositions to which \(c\) assigns the same credence that it assigns
to \(A\). Note that \(c^w\) is perfectly calibrated at \(w\). And if
\(c\) is perfectly calibrated at \(w\), then \(c^w = c\). Now, we
define the distance that a credence function \(c\) lies from
calibration at a world \(w\) to be the distance, \(\mathfrak{D}(c^w,
c)\), from \(c^w\) to \(c\). Now, as we saw in Objection 1 from
 <a href="#ObjCalArgForPro">section 4.3</a>
 above, this measure does not itself give a measure of epistemic
disutility. The problem is that an agent can move closer to
calibration at a world \(w\) while moving uniformly further from the
omniscient credence function at that world: that is, the measure of
epistemic disutility provided by the distance of the credence function
from its perfectly calibrated counterpart is not truth-directed. Thus,
if an agent&rsquo;s distance from her perfectly calibrated counterpart
is to contribute to a measure of her inaccuracy, it must be
supplemented by something that ensures that the resulting measure
avoids this consequence. The idea that Pettigrew proposes is this: the
inaccuracy of \(c\) at \(w\) is given by the distance of \(c\) from
the omniscient credence function \(v_w\) at \(w\); and that is given
by adding the distance of \(c\) from its perfectly calibrated
counterpart \(c^w\) to the distance of \(c^w\) from \(v_w\). Thus,
while moving to a credence function that is closer to its perfectly
calibrated counterpart may move you further from the omniscient
credence function, this can only be because the perfectly calibrated
counterpart of your new credence function is further from the
omniscient credence function than the perfectly calibrated counterpart
of your current credence function. If their perfectly calibrated
counterparts are the same, or if they are different but equally close
to the omniscient credence function, then moving closer to them will
move you closer to the omniscient credence function. Thus, Pettigrew
imposes the following constraint:</p>

<div class="indent" id="decomposition">

<p>
<strong>Decomposition</strong> Suppose \(\mathfrak{I}\) is a
legitimate inaccuracy measure and \(\mathfrak{D}\) is a distance
measure such that \(\mathfrak{I}(c, w) = \mathfrak{D}(v_w, c)\). Then

\[\mathfrak{I}(c, w) = \mathfrak{D}(v_w, c) = \mathfrak{D}(c^w, c) + \mathfrak{D}(v_w, c^w)\]</p>

</div>

<p>
Together with the other conditions that Pettigrew imposes,
<strong>Decomposition</strong> narrows down the class of legitimate
inaccuracy measures to a single one, namely, the Brier score. That is,
imposing these conditions entails
 <strong><a class="internal" href="#brier-inaccuracy">Brier Inaccuracy</a></strong>.</p>
 

<p>
Objection 1: <em>Appeal to summation is arbitrary.</em> One concern
about <strong>Decomposition</strong> is this: it is crucial for the
proof that the Brier score and only the Brier score satisfies all of
Pettigrew&rsquo;s conditions that in <strong>Decomposition</strong> we
combine the distance between \(c\) and \(c^w\) with the distance
between \(c^w\) and \(v_w\) by summing them together. But we could
have combined those quantities in other ways: we might have multiplied
them together, for instance; or, we might have summed them and then
taken a strictly increasing function of that sum. It might be
mathematically natural simply to add them together: but that
doesn&rsquo;t privilege that means of combining them for philosophical
purposes. However, if we combine them in any of these alternative
ways, Pettigrew&rsquo;s conditions will no longer hold of the Brier
score.</p>

<p>
Objection 2: <em>Proximity to calibration is not a good.</em> Another
concern is that, while proximity to being perfectly calibrated seems
epistemically good in the standard cases that are used to motivate
calibrationist accounts, it seems less compelling in other cases. For
instance, suppose you have opinions only about three propositions:
<em>First coin toss lands heads</em>, <em>Second coin toss lands
heads</em>, <em>Third coin toss lands heads</em>. And suppose you
assign to each of them the same credence, \(\frac{1}{3}\). Then, in
that situation, it seems plausible that you are doing better if one
out of the three tosses comes up heads. Now suppose that I have
opinions only about three propositions: <em>Djibouti is the capital of
Ghana</em>, <em>Serena Williams is a badminton player</em>, <em>Doris
Lessing wrote <em>The Golden Notebook</em></em>. And suppose I assign
to each of them the same credence, \(\frac{1}{3}\). Then, in that
situation, do we really retain the intuition that I do best if one out
of the three turns out true?</p>

<h4><a id="LevImp">5.3.4 Levinstein on importance</a> </h4>

<p>
In many of the accounts of inaccuracy measures that we&rsquo;ve
considered, such as those that propose the Brier score, the inaccuracy
of a whole credence function is given by the sum of the inaccuracies
of the individual credences it comprises; and the inaccuracy of every
individual credence is measured using the same local inaccuracy
measure. You might object, however, that not all credences should be
contribute equally to the overall inaccuracy of a credence function of
which they are a part. The accuracy of my credence that general
relativity is correct is surely more important and contributes more to
the overall epistemic utility of my credence function than the
accuracy of my credence that there are 1,239,382 blades of grass on my
front lawn. Now, we might accommodate this intuition by taking the
inaccuracy of the whole credence function to be not the straight sum
of the local inaccuracies of the credences it assigns, but rather the
weighted sum, where a credence gets greater weight the more important
the proposition to which it is assigned is. Now, as Predd, et al.
(2009) show, if we define global inaccuracy like this, and if the
importance of a proposition is the same at every world, then an
analogue of Theorem 4 still holds and we can still establish
<strong>Probabilism</strong>. However, Ben Levinstein (2018) shows
that, if the importance of a proposition changes from world to world,
the analogue of Theorem 4 will fail and with it the accuracy dominance
argument for <strong>Probabilism</strong>. And, he argues, the
importance of a proposition does often change from world to world. In
worlds where I have a brother, propositions that concern his
well-being have great importance to me, and the accuracy of my
credences in them should contribute greatly to the epistemic utility
of my whole credence function; in worlds where I am an only child, on
the other hand, the importance of these propositions is much
diminished, as is the contribution of the accuracy of my credences in
them to my total epistemic utility.</p>

<h3><a id="DomPri">5.4 Dominance principles</a></h3>

<p>
So far, we have considered the two components of the first premise of
Joyce&rsquo;s accuracy argument for <strong>Probabilism</strong>:
 <strong><a class="internal" href="#credal-veritism">Credal Veritism</a></strong>
 and
 <strong><a class="internal" href="#joycean-inaccuracy">Joycean Inaccuracy</a></strong>.
 We have left the former intact, but we have seen concerns with the
latter, and we have considered arguments for a stronger claim,
 <strong><a class="internal" href="#brier-inaccuracy">Brier Inaccuracy</a></strong>,
 though these also face difficulties. In this section, we move from
the account of epistemic disutility on which the argument is based to
the decision-theoretic principle to which we appeal in order to derive
<strong>Probabilism</strong> from this account. Let&rsquo;s recall the
version of the principle to which Joyce appeals in his original
paper:</p>

<div class="indent">

<p>
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>
 A rational agent will not adopt an option when there is another
option that has lower disutility at all worlds.</p>

<p>
That is: Suppose \(\mathcal{O}\) is a set of options, \(\mathcal{W}\)
is the set of possible worlds, and \(\mathfrak{U}\) is a disutility
function. Then, if \(o^*\) is an option, and if there is another
option \(o'\) such that \(\mathfrak{U}(o', w) &lt; \mathfrak{U}(o^*,
w)\) for all worlds \(w\) in \(\mathcal{W}\), then \(o^*\) is
irrational.</p>
</div>

<p>
Thus, according to Joyce, a credence function is irrational if it is
accuracy dominated.</p>

<p>
In this section, we&rsquo;ll consider four objections that have been
raised against
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>
 in the context of the accuracy argument for
<strong>Probabilism</strong>.</p>

<h4><a id="BroObj">5.4.1 The Bronfman objection</a></h4>

<p>
The first objection to the application of
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>
 in the context of the accuracy argument for
<strong>Probabilism</strong> was first stated in an unpublished
manuscript by Aaron Bronfman entitled &ldquo;A Gap in Joyce&rsquo;s
Proof of Probabilism&rdquo;; it has been discussed by H&aacute;jek
(2008) and Pettigrew (2010, 2013b). The starting point for the
objection is the observation that
 <strong><a class="internal" href="#credal-veritism">Credal Veritism</a></strong>
 and
 <strong><a class="internal" href="#joycean-inaccuracy">Joycean Inaccuracy</a></strong>
 do not together narrow down the class of legitimate measures of
epistemic disutility to a single function; they characterize a family
of such measures. But, for all that
 <a class="internal" href="#theorem-4">Theorem 4</a>
 (Joyce&rsquo;s Main Theorem) tells us, it may well be that, for a
given non-probabilistic credence function \(c^*\), different measures
in this family of legitimate inaccuracy measures give different sets
of credence functions that accuracy dominate \(c^*\). Thus, an agent
with a non-probabilistic credence function \(c^*\) might be faced with
a range of credence functions, each of which accuracy dominates
\(c^*\) relative to a different legitimate inaccuracy measure.
Moreover, it may be that any credence function that accuracy dominates
\(c^*\) relative to Joycean inaccuracy measure \(\mathfrak{I}\) does
not accuracy dominate \(c^*\) relative to the alternative Joycean
measure \(\mathfrak{I}'\); indeed, it may be that any credence
function that dominates \(c^*\) relative to \(\mathfrak{I}\) risks
very high inaccuracy at some world relative to \(\mathfrak{I}'\), and
<em>vice versa</em>. In this situation, it is plausible that the agent
is rationally permitted to stick with her non-probabilistic credence
function \(c^*\).</p>

<p>
There are two replies to this objection. According to the first, the
objection relies on a false meta-normative claim; according to the
second, it misunderstands the purpose of Joyce&rsquo;s conditions.</p>

<p>
Reply 1: <em>No requirement to give advice.</em> The meta-normative
claim on which the objection seems to rely is the following: For a
norm to hold, there must be specific advice available to those who
violate that norm concerning how to improve their behaviour.
Bronfman&rsquo;s objection begins with the observation that, for any
specific advice that one might give to a non-probabilistic agent
concerning which credence function she should adopt in favour of her
own, there will be inaccuracy measures that satisfy Joyce&rsquo;s
conditions, but don&rsquo;t sanction this advice; indeed, there will
be inaccuracy measures relative to which that advice is very bad.
Thus, Joyce&rsquo;s accuracy argument violates the meta-normative
constraint. But, the reply submits, the meta-normative claim is false:
for a norm to hold, it is sufficient that there is a serious defect
suffered by those who violate the norm that is not shared by those who
satisfy the norm; it is not also required that there should be advice
on which specific action an agent should perform to improve her
behaviour. And Joyce&rsquo;s argument satisfies this sufficient
condition. An agent ought to satisfy <strong>Probabilism</strong>
because non-probabilistic credence functions suffer from a serious
epistemic defect (namely, being accuracy dominated) that does not
beset probabilistic ones. And this fact is &ldquo;supertrue&rdquo;, so
to speak: that is, it is true on any precisification of the notion of
accuracy that obeys Joyce&rsquo;s conditions on an inaccuracy
measure.</p>

<p>
Reply 2: <em>Each agent uses a single inaccuracy measure.</em> The
second reply to this objection does not take issue with the
meta-normative claim mentioned above; indeed, on the understanding of
the accuracy argument for <strong>Probabilism</strong> that it
proposes, the argument satisfies the necessary condition imposed by
that claim. That is, according to this reply, the accuracy argument,
properly understood, does in fact provide specific advice to
non-probabilistic agents. The idea is this: There are (at least) three
ways to understand the purpose of Joyce&rsquo;s conditions on
inaccuracy measures. First, we might think that the notion of
inaccuracy is vague; and we might say that any inaccuracy measure that
satisfies the conditions is a legitimate precisification of it. This
is a <em>supervaluationist</em> approach. On this approach, there is
no specific advice available to non-probabilistic agents that is
sanctioned by all precisifications. Second, we might think that the
notion of inaccuracy is precise, but that we have only limited
knowledge about it, and that the sum total of our knowledge is
embodied in the conditions. This is an <em>epistemicist</em> approach.
On this approach, there is specific advice, but it is not available to
us. Third, we might think that there is no objectively correct
inaccuracy measure; rather, any inaccuracy measure that satisfies the
conditions is rationally permissible. But nonetheless, any particular
agent has exactly one such measure. This is a <em>subjectivist</em>
approach. On this understanding, there is specific advice for any
non-probabilistic agent. Any such agent uses an inaccuracy measure
that satisfies Joyce&rsquo;s conditions. And this gives, for any
non-probabilistic credence function, a probabilistic credence function
that strongly dominates it. So the specific advice is this: adopt one
of the probabilistic credence functions that strongly dominates your
non-probabilistic credence function relative to your favoured measure
of inaccuracy. This gives us <strong>Probabilism</strong> and does so
without violating the meta-normative claim on which Bronfman&rsquo;s
objection relies.</p>

<p>
However, this response isn&rsquo;t without its own problems. For
instance, it assumes that each agent values inaccuracy in a
sufficiently specific way that they narrow down the class of
inaccuracy measures to a single measure that they can then use to
obtain this advice. But, at least for those who think that
 <strong><a class="internal" href="#joycean-inaccuracy">Joycean Inaccuracy</a></strong>
 is the strongest condition we can place on the inaccuracy measures,
this seems too strong. How can we assume that each rational agent will
have a unique inaccuracy measure in mind when we don&rsquo;t think
that there are conditions that demand that we narrow down the class of
legitimate inaccuracy measures this far?</p>

<h4><a id="UndDom">5.4.2 Undominated dominance</a></h4>

<p>
The second objection to
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>
 comes from Pettigrew (2014a). Here, Pettigrew observes that there are
decisions in which
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>
 does not seem to hold because the irrationality of being dominated
depends on the status of the dominating options in some way.
Here&rsquo;s Pettigrew&rsquo;s central example:</p>

<div class="indent" id="namefortune">

<p>
<strong>Name Your Fortune\(^*\)</strong> You have a choice: play a
game with God or don&rsquo;t. If you don&rsquo;t, you receive 2 utiles
for sure. If you do, you then pick an integer. If you pick \(k\), God
will then do one of two things: (i) give you \(2^{k-1}\) utiles; or
(ii) give you \(2 - \frac{1}{2^{k-1}}\) utiles. (Pettigrew 2014a:
587)</p>
</div>

<p>
In this example, the only option that isn&rsquo;t dominated is the
option in which you do not play the game with God. If you choose that
option, you get 2 utiles for sure. If, on the other hand, you choose
to play the game and pick integer \(k\), then choosing integer \(k+1\)
will be guaranteed to get you more utility: either \(2^{k+1}\) utiles
compared with \(2^k\) or \(2 - \frac{1}{2^k}\) utiles compared with
\(2 - \frac{1}{2^{k-1}}\). However, the option in which you get 2
utiles for sure seems a lousy option given the other possibilities
available. One way to see this is as follows: Take a probability
distribution over the two possibilities (i) and (ii) between which God
will choose if you choose to play; then, providing it doesn&rsquo;t
assign all probability to God choosing (i), there will be some option
you can take if you play the game that has greater expected utility
than the option of not playing the game. If
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>
 is correct, however, not playing the game is the only rational
option. This seems to tell against
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>.</p>
 
<p>
The moral that Pettigrew draws from this example is the following. Not
all dominated options are irrational. Whether or not a dominated
option is irrational depends on the status of the options that
dominate it. If all of the options that dominate a given option are
themselves dominated, then being dominated does not rule out the given
option as irrational. Thus, in
 <strong><a class="internal" href="#namefortune">Name Your Fortune\(^*\)</a></strong>,
 none of the options are ruled irrational because they are dominated;
after all, all of the dominated options are only dominated by other
options that are also themselves dominated. Thus, Pettigrew instead
suggests a decision-theoretic principle to replace
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>.
 To state it, we must distinguish between two notions of dominance: a
strong notion and a weak notion. Suppose \(o^*\) and \(o'\) are
options. We say that <em>\(o^*\) strongly dominates \(o'\)</em> if
\(o^*\) has greater utility than \(o'\) at all worlds. We say that
<em>\(o^*\) weakly dominates \(o'\)</em> if \(o^*\) has at least as
great utility as \(o'\) at all worlds and greater utility at some
world.</p>

<div class="indent" id="undominated-dominance">

<p>
<strong>Undominated Dominance</strong> A rational agent will not adopt
an option that is strongly dominated by an option that is not itself
even weakly dominated.</p>
</div>

<p>
Now, it turns out that, if we accept
 <strong><a class="internal" href="#brier-inaccuracy">Brier Inaccuracy</a></strong>,
 we can still derive <strong>Probabilism</strong> using only
<strong>Undominated Dominance</strong>. This is a consequence of the
following theorem:</p>

<div class="indent" id="theorem-5-finetti">

<p>
<strong>Theorem 5 (de Finetti)</strong> Suppose that \(c^*\) is a
credence function in \(\mathcal{C_F}\) that violates
<strong>Probabilism</strong>. Then </p>

<ol type="i">

<li> there is a credence function \(c'\) in \(\mathcal{C_F}\) such
that \(\mathfrak{B}(c', w) &lt; \mathfrak{B}(c^*, w)\) for all \(w\)
in \(\mathcal{W_F}\), and </li>

<li> there is no credence function \(c\) such that \(\mathfrak{B}(c,
w) \leq \mathfrak{B}(c', w)\) for all \(w\) in \(\mathcal{W_F}\) and
\(\mathfrak{B}(c, w) &lt; \mathfrak{B}(c', w)\) for some \(w\) in
\(\mathcal{W_F}\).</li>
</ol>
</div>

<p>
Thus, we have the following argument:</p>

<div class="indent">

<p>
<strong>Brier-based accuracy argument for Probabilism: II</strong></p>

<ul class="sentag tag2em">

<li><span class="tag">(1)</span><span  class="sen"><a class="internal" href="#credal-veritism">Credal Veritism</a>
 +
 <a class="internal" href="#brier-inaccuracy">Brier Inaccuracy</a></span></li>
 
<li><span class="tag">(2)</span><span  class="sen"><a class="internal" href="#undominated-dominance">Undominated Dominance</a></span></li>
 
<li><span class="tag">(3)</span><span  class="sen"><a class="internal" href="#theorem-5-finetti">Theorem 5</a></span></li>
 
<li>Therefore,</li>

<li><span class="tag">(4)</span><span  class="sen"><a class="internal" href="#probabilism">Probabilism</a></span></li>
 </ul>
</div>

<h4><a id="EviAcc">5.4.3 Evidence and Accuracy</a></h4>

<p>
The next objection to
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>
 is similar to the objection raised in the previous section. In the
previous section, the moral we drew from
 <strong><a class="internal" href="#namefortune">Name Your Fortune\(^*\)</a></strong>
 is that a dominated option is only ruled irrational in virtue of
being dominated if at least one of the options that dominate it is not
itself dominated. But there may be other features that a credence
function might have besides itself being dominated such that being
dominated by that credence function does not entail irrationality.
Easwaran &amp; Fitelson (2012) suggest the following feature. Suppose
that your credence function is non-probabilistic, but it matches the
evidence that you have: that is, the credence it assigns to a
proposition matches the extent to which your evidence supports that
proposition. And suppose that none of the credence functions that
accuracy dominate your credence function have that feature. Then, we
might say, the fact that your credence function is accuracy dominated
does not rule it irrational. After all, it is dominated only by
credence functions that violate the constraints that your evidence
imposes on your credences. Thus, Easwaran and Fitelson suggest the
following decision-theoretic principle, which applies only when the
options in question are credence functions:</p>

<div class="indent">

<p>
<strong>Evidential Dominance</strong> A rational agent will not adopt
a credence function that is strongly dominated by an alternative
credence function that is not itself even weakly dominated and which
matches the agent&rsquo;s evidence if the dominated credence function
does.</p>
</div>

<p>
Easwaran and Fitelson then object that there are situations in which
<strong>Evidential Dominance</strong> does not entail
<strong>Probabilism</strong>. For instance, suppose that a trick coin
is about to be tossed. Your evidence tells you that the chance of it
landing heads is 0.7. Your credence that it will lands heads is 0.7
and your credence that it will land tails is 0.6. Then you might think
that your credences match your evidence, because you have evidence
only about it landing heads and your credence that it will land heads
equals the known chance that it will land heads. However, it turns out
that all of the credence functions that accuracy dominate your
credence function (when accuracy is measured by the Brier score) fail
to match this evidence: that is, they assign credence other than 0.7
to <em>Heads</em>. Thus, <strong>Evidential Dominance</strong> does
not entail that your credence function is irrational.
 <a href="#fig2">Figure 2</a>
 illustrates this result. Pettigrew (2014a) responds to this objection
on behalf of the accuracy argument for
<strong>Probabilism</strong>.</p>

<div class="figure" id="fig2">
<img src="fig2.png" width="316" height="300" alt="[The same as figure 1 except there is a vertical dashed line going through the point labelled \(c*\) . (figure 1 description repeated: a graph of two vertical lines and two horizontal lines forming a square but lines extend beyond the intersections. The left vertical line is labelled 'Tails' and the lower horizontal line labelled 'Heads'. The upper left corner is labelled \(v_{w_1}\), the lower right corner is labelled \(v_{w_2}\)] and a diagonal line connects the two. Two arcs, one stretches from the lower left vertical line to the upper right horizontal line and the other from the upper right vertical line to the lower left horizontal line. The two arcs intersect twice. The upper right intersection is labelled \(c*\) and a point on the diagonal line in the middle of the intersection space is labelled \(c'\).]" />

<p style="line-height: 120%">
<span class="figlabel">Figure 2:</span> In this figure, as in
 <a href="#fig1">Figure 1</a>,
 we plot the various possible credence functions defined on a
proposition <em>Heads</em> and its negation <em>Tails</em> in the unit
square. The diagonal line contains all and only the probability
functions. Let \(c^*\) be your credence function: that is, it assigns
0.7 to <em>Heads</em> and 0.6 to <em>Tails</em>. So it violates
<strong>Probabilism</strong>. The credence functions that lie between
the two arcs are all and only the credence functions that accuracy
dominate \(c^*\). The credence functions on the dashed line are all
and only the credence functions that match your evidence that the
chance of <em>Heads</em> is 0.7. Notice that the dashed line does not
overlap with the set of credence functions that accuracy dominate
yours at any point. This is the crucial fact on which Easwaran and
Fitelson&rsquo;s objection rests. </p>
</div>

<h4><a id="DomActStaDep">5.4.4 Dominance and Act-State Dependence</a></h4>

<p>
The final objection to
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>
 comes from Hilary Greaves (2013) and Michael Caie (2013), who point
out that, in practical decision theory, only a restricted version of
that principle is accepted (see also Jenkins 2007; Berker 2013a,b;
Carr 2017). To see why such a restriction is needed, consider the
following case:</p>

<div class="indent">

<p>
<strong>Driving Test</strong> My driving test is in a week&rsquo;s
time. I can choose now whether or not I will practise for it. Other
things being equal, I prefer not to practise. But I also want to pass
the test, and I know that I won&rsquo;t pass if I don&rsquo;t
practise, and I will pass if I do. Here is my decision table:</p>

<div class="figure">

<table class="hrules cellpad-med-dense nocaption">
<tr>
<td></td>
<td><em>Pass</em></td>
<td><em>Fail</em></td> </tr>
<tr>
  <td><em>Practise</em></td>
  <td>10</td>
  <td>2</td> </tr>
<tr>
  <td><em>Don&rsquo;t Practise</em></td>
  <td>15</td>
  <td>7</td> </tr>
</table>
</div>
</div>

<p>
According to
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>,
 it is irrational to practise. After all, whether or not I pass or
fail, I obtain higher utility if I don&rsquo;t practise, so not
practising strongly dominates practising. But this is clearly the
wrong result. The reason is that I should not compare practising at
the world at which I pass with not practising at that world, and
practising at the world at which I fail with not practising at that
world. For if I practise, I will pass; and if I don&rsquo;t, I will
fail. Moreover, I know all this. So I should compare practising at the
world at which I pass with not practising at the world at which I
fail. And then my utility is higher if I practise.</p>

<p>
The moral of this example is that
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>
 should be restricted so that it applies only in situations in which
the options between which the agent is choosing will not influence the
way the world is if they are adopted. Such situations are sometimes
called situations of <em>act-state independence</em>. In situations in
which the acts (options) influence the states (of the world),
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>
 does not apply. To see how this affects the accuracy argument for
<strong>Probabilism</strong>, consider the following example, which
borrows from Caie&rsquo;s and Greaves&rsquo; examples:</p>

<div class="indent">

<p>
<strong>Thwarted Accuracy</strong> Suppose I can read your mind. You
have opinions only about two propositions, \(A\) and \(\neg A\). And
suppose that I have control over the truth of \(A\) and \(\neg A\). I
decide to do the following. First, define the non-probabilistic
credence function \(c^\dag(A) = 0.99\) and \(c^\dag(\neg A) = 0.005\).
Then:</p>

<ol type="i">

<li>If your credence function is \(c^\dag\), I will make \(A\) true
(and thereby make your credence function very accurate);</li>

<li>If your credence function is not \(c^\dag\) and your credence in
\(A\) is greater than 0.5, I will make \(A\) false (and thereby make
your credence function rather inaccurate);</li>

<li>If your credence function is not \(c^\dag\) and your credence in
\(A\) is at most 0.5, I will make \(A\) true (and thereby make your
credence function rather inaccurate).</li>
</ol>
</div>

<p>
In this case, since the credence function \(c^\dag\) is not a
probability function, it is accuracy dominated by Joyce&rsquo;s
theorem and thus it is ruled out as irrational by
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>,
 just as the option of practising is ruled out as irrational in
Driving Test. However, this is a situation in which adopting an option
influences the way the world is in such a way that it affects the
utility of the option, just as choosing whether or not to practise
does in Driving Test. If I were to have credence function \(c^\dag\),
I would be more accurate than I would be were I to have any other
credence function. Thus, it seems that, just as we said that
practising is in fact the only option that shouldn&rsquo;t be ruled
irrational in Driving Test, so now we must say that credence function
\(c^\dag\) is the only option that shouldn&rsquo;t be ruled irrational
in Thwarted Accuracy. But of course, it then follows that
<strong>Probabilism</strong> is false, for there are situations such
as this one in which it is irrational to do anything other than have a
non-probabilistic credence function.</p>

<p>
There are three responses available here: the first is to bite the
bullet, accept the restriction to
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>,
 and therefore accept a restriction on the cases in which
<strong>Probabilism</strong> holds; the second is to argue that the
practical case and the epistemic case are different, with different
decision-theoretic principles applying to each; the third, of course,
is to abandon the accuracy argument for <strong>Probabilism</strong>.
Joyce (2018) and Pettigrew (2018a) argue for the first response. They
advocate different decision-theoretic principles to replace
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>
 in the epistemic case: Joyce advocates standard causal decision
theory together with a Ratifiability condition (Jeffrey 1983);
Pettigrew omits the ratifiability condition. But they both agree that
these principles will agree with
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>
 in cases of act-state independence; and they agree with the verdict
that \(c^\dag\) is the only credence function that isn&rsquo;t ruled
out as irrational in Thwarted Accuracy. Konek &amp; Levinstein (2019)
argue for the second response, claiming that, since doxastic states
and actions have different directions of fit, different
decision-theoretic principles will govern them. They hold that
 <strong><a class="internal" href="#naive-dominance">Naive Dominance</a></strong>
 (or, perhaps, <strong>Undominated Dominance</strong>) is the correct
principle when the options are credence functions, even though it is
not the correct principle when the options are actions. Caie (2013)
and Berker (2013b), on the other hand, argue for the third option.</p>

<h3><a id="EpExp">5.5 Epistemic expansions</a></h3>

<p>
In
 <a href="#UndDom">section 5.4.2</a>,
 we introduced Undominated Dominance and we stated Theorem 5, which
says that every non-probabilistic credence function \(c^*\) defined on
\(\mathcal{F}\) is accuracy dominated by a credence function \(c'\)
defined on \(\mathcal{F}\) that is itself not accuracy dominated by a
credence function \(c\) defined on \(\mathcal{F}\). But you might
think that this is still not sufficient to establish
<strong>Probabilism</strong>. After all, while \(c'\) is not itself
dominated by a credence function defined on \(\mathcal{F}\), it might
be accuracy dominated by a credence function \(c^\dag\) defined on
some other set of propositions \(\mathcal{F}^\dag\). For instance,
take the non-probabilistic credence function \(c^*\) defined on
\(\mathcal{F} = \{X, \neg X\}\), where \(c^*(X) = 0.6 = c^*(\neg X)\).
It is Brier dominated by \(c'(X) = 0.5 = c'(\neg X)\), since \(c^*\)
has Brier score \(|1-0.6|^2 + |0-0.6|^2 = 0.52\) at both worlds in
\(\mathcal{W_F}\) while \(c'\) has \(|1-0.5|^2 + |0-0.5|^2 = 0.5\) at
both worlds. But \(c'\) is Brier dominated by \(c^\dag\) defined on
\(\mathcal{F}^\dag = \{X\}\), where \(c^\dag(X) = 0.5\), since this
has Brier score \(|1-0.5|^2 = |0-0.5|^2 = 0.25\) at both worlds. A
natural reaction to this is to define the epistemic disutility
function to be the <em>average Brier score</em> rather than the
<em>total Brier score</em>:</p> 

\[\mathfrak{B}'(c, w) := \frac{1}{|\mathcal{F}|}\sum_{X \in \mathcal{F}} |v_w(X) - c(w)|^2\]

<p>
Now, relative to \(\mathfrak{B}'\), \(c^*\) is indeed accuracy
dominated by \(c'\) and \(c'\) is not accuracy dominated by
\(c^\dag\). But \(c'\) is accuracy dominated by \(c^+\) defined on
\(\mathcal{F}^+ = \{\top\}\), where \(c^+(\top) = 1\), since the
average Brier score of \(c^+\) is \(\frac{1}{1}|1-1|^2 = 0\) at all
worlds, while the average Brier score of \(c'\) is
\(\frac{1}{2}(|1-0.5|^2 + |0-0.5|^2) = 0.25\) at all worlds. Jennifer
Carr (2015) initiated the investigation into how epistemic utility
arguments for <strong>Probabilism</strong> might work when we start to
compare credence functions defined on different sets of propositions.
She notes the analogy with population axiology in ethics (see entry on
 <a href="../repugnant-conclusion/index.html">the repugnant conclusion</a>).
 Pettigrew (2018b) takes this analogy further, proving an
impossibility result analogous to those prevalent in that part of
ethics.</p>

<h2><a id="EpiDisArg">6. Epistemic disutility arguments</a></h2>

<p>
So far, we have considered calibration arguments and accuracy
arguments for <strong>Probabilism</strong>. In each of these cases, we
identify a particular feature of a credence function&mdash;the
proximity of its credences to being calibrated, or their proximity to
the omniscience credences&mdash;we claim that it is the source of all
epistemic utility, and we attempt to characterize the mathematical
functions that legitimately measure the extent to which the credence
function has that feature. In this section, we consider an argument
due to Joyce (2009) that attempts to characterize epistemic disutility
functions directly.</p>

<p>
Joyce&rsquo;s central condition is <strong>Coherent
Admissibility</strong>, which says that a measure of epistemic
disutility should never render a probabilistic credence function
weakly dominated. More precisely:</p>

<div class="indent">

<p>
<strong>Coherent Admissibility</strong> Suppose \(\mathcal{F}\) is a
set of propositions and \(\mathfrak{D}: \mathcal{C_F} \times
\mathcal{W_F} \rightarrow [0, \infty]\) is a measure of epistemic
disutility. Then, if \(c^*\) is a probabilistic credence function,
then \(c^*\) is not weakly dominated relative to \(\mathfrak{D}\).
That is, for any probabilistic credence function \(c^*\), there is no
credence function \(c'\) such that </p>

<ol type="i">

<li>\(\mathfrak{D}(c', w) \leq \mathfrak{D}(c^*, w)\) for all \(w\);
and</li>

<li> \(\mathfrak{D}(c', w) &lt; \mathfrak{D}(c^*, w)\) for some
\(w\).</li>
</ol>
</div>

<p id="joycean-disutility">
We then have two results concerning coherent admissible epistemic
disutility functions. On the first, we restrict attention to the case
in which \(\mathcal{F}\) is a partition, and we prove that, together
with <strong>Undominated Dominance</strong>, we can establish
<strong>Probabilism</strong>; on the second, we focus on the case in
which \(\mathcal{F}\) is an algebra, and we prove that, together with
<strong>Naive Dominance</strong>, we can establish
<strong>Probabilism</strong>.</p>

<div class="indent" id="theorem-5-joyce">

<p>
<strong>Theorem 5 (Joyce 2009)</strong> Suppose \(\mathcal{F}\) is a
set of propositions and \(\mathfrak{D}\) is a Joycean epistemic
disutility function for the credence functions in \(\mathcal{C_F}\).
</p>

<ol type="1">

<li> Suppose \(\mathcal{F}\) is an algebra. Then, if \(c^*\) is a
credence function in \(\mathcal{C_F}\) that violates
<strong>Probabilism</strong>, then

<ol type="i">

<li> there is a credence function \(c'\) in \(\mathcal{C_F}\) such
that \(\mathfrak{D}(c', w) &lt; \mathfrak{D}(c^*, w)\) for all \(w\)
in \(\mathcal{W_F}\).</li>
</ol> </li>

<li> Suppose \(\mathcal{F}\) is a partition. Then, if \(c^*\) is a
credence function in \(\mathcal{C_F}\) that violates
<strong>Probabilism</strong>, then

<ol type="i">

<li> there is a credence function \(c'\) in \(\mathcal{C_F}\) such
that \(\mathfrak{D}(c', w) &lt; \mathfrak{D}(c^*, w)\) for all \(w\)
in \(\mathcal{W_F}\), and</li>

<li> there is no credence function \(c\) such that \(\mathfrak{D}(c,
w) \leq \mathfrak{D}(c', w)\) for all \(w\) in \(\mathcal{W_F}\) and
\(\mathfrak{D}(c, w) &lt; \mathfrak{D}(c', w)\) for some \(w\) in
\(\mathcal{W_F}\).</li>
</ol> </li>
</ol>
</div>

<p>
Let&rsquo;s say that <strong>Joycean Disutility</strong> is the claim
that all legitimate measures of epistemic disutility satisfy
<strong>Coherent Admissibility</strong> along with the other new
conditions that Joyce imposes. Then we have the following
argument:</p>

<div class="indent">

<p>
<strong>Joycean epistemic disutility argument for
Probabilism</strong></p>

<ul class="sentag tag2em">

<li><span class="tag">\((1)\)</span><span  class="sen"><a class="internal" href="#joycean-disutility">Joycean Disutility</a></span></li>
 
<li><span class="tag">\((2)\)</span><span  class="sen"><a class="internal" href="#undominated-dominance">Undominated or Naive Dominance</a></span></li>
 
<li><span class="tag">\((3)\)</span><span  class="sen"><a class="internal" href="#theorem-5-joyce">Theorem 5</a></span></li>
 
<li>Therefore,</li>

<li><span class="tag">\((4)\)</span><span  class="sen"><a class="internal" href="#probabilism">Probabilism</a></span></li>
 </ul>

</div>

<p>
Joyce argues for <strong>Coherent Admissibility</strong> as
follows.</p>

<ul class="sentag tag2em">

<li><span class="tag">\((1)\)</span><span class="sen">For each
probabilistic credence function \(c\), there is a possible world at
which \(c\) is the objective chance function.</span></li>

<li><span class="tag">\((2)\)</span><span class="sen">If an agent
learns with certainty that \(c\) is the objective chance function, and
nothing more, then the unique rational response to her evidence is to
set her credence function to \(c\). (This is close to David
Lewis&rsquo; Principal Principle (Lewis 1980).)</span></li>

<li><span class="tag">\((3)\)</span><span class="sen">Thus, by (1) and
(2): for each probabilistic credence function \(c\), there is an
evidential situation in which an agent might find herself such that
\(c\) is the unique rational response to that evidential
situation.</span></li>

<li><span class="tag">\((4)\)</span><span class="sen">Thus, by (3):
Let \(c^*\) be a probabilistic credence function. Then there is an
evidential situation in which \(c^*\) is the unique rational
response.</span></li>

<li><span class="tag">\((5)\)</span><span class="sen">If \(c'\) weakly
dominates \(c^*\) relative to a legitimate measure of epistemic
disutility, and \(c^*\) is rationally permitted, then \(c'\) is also
rationally permitted.</span></li>

<li><span class="tag">\((6)\)</span><span class="sen">Thus, by (4) and
(5): if \(c^*\) is weakly dominated, there is no evidential situation
in which \(c^*\) the unique rational response.</span></li>

<li>Therefore,</li>

<li><span class="tag">\((7)\)</span><span class="sen">\(c^*\) is not
weakly dominated relative to any legitimate measure of epistemic
disutility.</span></li>
</ul>

<p>
Let&rsquo;s consider two objections to this argument.</p>

<p>
Objection 1: <em>Not all probabilistic credence functions could be
chance functions.</em> The first objection denies (1). It&rsquo;s due
to Alan H&aacute;jek (2008). As H&aacute;jek notes, if a credence
function \(c\) is defined on propositions about the chances
themselves, it&rsquo;s not obvious that any chance function will be
defined on that proposition. If that&rsquo;s right \(c\) is not a
possible chance function. And his argument might be extended. We can
assign a credence function on propositions concerning ethical matters,
or mathematical matters, or aesthetic matters, or facts about the
current time or the agent&rsquo;s current location. But it is not
clear that such a credence function could possibly be the chance
function of any world, since it seems natural to think that chances
cannot attach to these sorts of proposition. Pettigrew (2014b: 5.2.1)
replies on Joyce&rsquo;s behalf.</p>

<p>
Objection 2: <em>The argument over-generates.</em> The second
objection claims that, in the absence of <strong>Probabilism</strong>,
which is supposed to be the conclusion of the argument for which
<strong>Coherent Admissibility</strong> is a crucial part, this
argument overgenerates. Consider, for instance, the following
claim:</p>

<ul class="sentag tag2em">

<li><span class="tag">(\(2'\))</span><span class="sen">If an agent
learns with certainty that \(c\) is the credence function that
constitutes the unique rational response to her evidence at that time,
and nothing more, then the unique rational response is to set her
credence function to \(c\).</span></li>
</ul>

<p>
Now, suppose \(c^\dag\) is a non-probabilistic credence function and
apply the version of Joyce&rsquo;s argument that results from
replacing (2) with (2&rsquo;). That is, we assume that it is possible
that the agent learn with certainty that \(c^\dag\) is the unique
rational response to her evidence, even if in fact it is not. We might
assume, for instance, that a mischievous God whispers in the
agent&rsquo;s ear that this is the case. Then we must conclude that
\(c^\dag\) is not weakly dominated relative to any legitimate measure
of epistemic disutility. But now we have that no credence function is
weakly dominated, whether it is probabilistic or not. And, combined
with Joyce&rsquo;s other considerations, this is impossible. If no
probabilistic credence functions are weakly dominated relative to an
epistemic disutility function, then all of the non-probabilistic
credence functions are: that&rsquo;s the lesson of
 <a class="internal" href="#theorem-5-joyce">Theorem 5</a>
 above. Of course, the natural response to this objection is to note
that (2&rsquo;) only holds when \(c\) is a probabilistic credence
function. But such a restriction is unmotivated until we have
established <strong>Probabilism</strong>.</p>

<h4><a id="MayWheNum">6.1 Mayo-Wilson and Wheeler on numerical representations of epistemic utility</a> </h4>

<p>
We have considered a number of different characterizations of the
legitimate ways of measuring (in)accuracy and epistemic (dis)utility.
Each has assumed that the measures of these quantities are numerically
representable; that is, each assumes it makes sense to use real
numbers to measure these quantities. Conor Mayo-Wilson and Greg
Wheeler call this assumption into question (Mayo-Wilson &amp; Wheeler,
ms.). They argue that, in order to represent a quantity numerically,
you need to prove a representation theorem for it in measurement
theory. And, if you wish to use that quantity as a measure of utility,
or as a component of a measure of utility, you need to prove a
representation theorem not only for the quantity itself, but for its
use in expected utility calculations. They note that this was the
purpose of the representation theorems of von Neumann &amp;
Morgenstern as well as Savage and Jeffrey (see entry on
 <a href="../rationality-normative-utility/index.html">normative theories of rational choice: expected utility</a>).
 And they argue that the methods that these authors use are not
available to the proponent of epistemic utility arguments. </p>

<h2><a id="RelIss">7. Related issues</a></h2>

<p>
That completes our survey of the existing literature on the epistemic
utility arguments for Probabilism. We have considered three families
of argument: calibration arguments, accuracy arguments, and epistemic
disutility arguments. In this final section, we briefly consider ways
in which the argument strategy employed here (and described in
 <a href="#ForArgEpiUtiThe">section 2</a>)
 might be generalised.</p>

<h3><a id="InfProSpa">7.1 Infinite probability spaces</a></h3>

<p>
We have assumed throughout that the set of propositions on which an
agent&rsquo;s credence function is defined is finite. What happens
when we lift this restriction? The first problem is that we need to
say how to measure the inaccuracy of a credence function defined over
an infinite set of propositions. Then, having done that, we need to
say which such credence functions are accuracy dominated relative to
these measures, and which aren&rsquo;t.</p>

<p>
Sean Walsh has described an extension of the Brier score to the
case in which the set of propositions to which we assign credences is
countably infinite; and he has shown that non-probabilistic credence
functions on such sets are accuracy dominated relative to that
inaccuracy measure, while probabilistic ones aren&rsquo;t. (For a description of Walsh's unpublished work, see Kelley 2019). Mikayla
Kelley (2019) has then gone considerably further and generalized
Walsh&rsquo;s results significantly by describing a wide range of
possible inaccuracy measures and characterizing the undominated
credence functions defined on sets of propositions of different
varieties. One interesting consequence is that an accuracy dominance
argument for the norm of Countable Additivity does not seem to be
forthcoming.</p>

<h3><a id="OthPriRatForCre">7.2 Other principles of rationality for credences</a></h3>

<p>
We have focussed here on the synchronic coherence principle of
<strong>Probabilism</strong>. But there are many other principles that
are thought to govern rational credence. It is natural to ask whether
we can give similar arguments for those. As we saw above in
 <a href="#SouEpiDis">section 5.2</a>,
 a number of epistemic norms have been explored in this framework, but
of course there are many more still to consider.</p>

<h3><a id="OthDoxSta">7.3 Other doxastic states</a></h3>

<p>
In this entry, we have considered agents represented as having precise
credence functions. But there are, of course, many other models of
doxastic states that are considered in current epistemology. As
mentioned at the outset, we might represent an agent by the set of
propositions that they believe (their full beliefs); or we might
represent them using a set of precise credence functions (their
imprecise credences); or a comparative confidence ordering; or a
precise primitive conditional probability function. And, when modelled
in this way, there are principles of rationality that apply to these
agents. Are there accuracy arguments in their favour?</p>

<p>
In the case of full beliefs, it is reasonably straightforward to
define the measures of accuracy: a belief in a proposition receives
some positive epistemic utility \(R\) when the proposition is true and
some negative epistemic utility \(-W\) when it is false; and if you
suspend judgment on a proposition, you receive epistemic utility 0
whether it is true or false. Kenny Easwaran (2015) and Kevin Dorst
(2017) explore the consequences of this account and show that it
naturally gives rise to what is known as the Lockean account of the
relationship between credences and full beliefs. Relative to a
probability function \(c\), you maximise your expected epistemic
utility by believing a proposition \(A\) iff \(c(A) &gt;
\frac{W}{R+W}\).</p>

<p>
In the case of imprecise credences, it is much less straightforward to
define the measures of accuracy. Indeed, there are a number of
powerful results that seem to show that there can be no measures that
satisfy some basic plausible conditions (Seidenfeld, Schervish, &amp;
Kadane 2012, Schoenfield 2015, Mayo-Wilson &amp; Wheeler 2016). To see
how these work, note that one of the central motivations for
representing an agent&rsquo;s credences using imprecise credences is
that there are often situations in which our evidence seems to demand
that our credences are not captured by a single precise credence
function. When our evidence is complex, mixed, and ambiguous, we might
well think only an imprecise credal state is an appropriate response.
However, as these various results show, if your accuracy measure
satisfies certain plausible conditions, then for any imprecise credal
state, there is a precise one that is at least as accurate as the
imprecise one at all worlds. But if that&rsquo;s the case, Veritism
says that it would be just as good to have the precise state as the
imprecise one. But that goes against the requirements of our evidence.
Jason Konek (2019, section 3) offers a response to these impossibility
results on behalf of the epistemic utility theorist who wishes to use
imprecise credences to represent agents. </p>

<h3><a id="NonClaLog">7.4 Non-classical logic</a></h3>

<p>
In all of the arguments we&rsquo;ve surveyed above, we have assumed
that classical logic governs the propositions to which our agent
assigns credences. This secures Probabilism, which demands, among
other things, that an agent assign maximal credence to every classical
tautology. But what happens if we drop this assumption? What if,
instead, the propositions are governed by a three-valued logic, such
as strong Kleene logic or the Logic of Paradox (see entry on
 <a href="../logic-manyvalued/index.html">many-valued logic</a>)?
 In a series of papers, Robbie Williams has built on mathematical
results by Jeff Paris and Jean-Yves Jaffray to understand what norms
of credence the accuracy arguments establish in this case (Williams
2012a,b, 2018, Paris 2001, Jaffray 1989). I&rsquo;ll give a single
example here to illustrate. Strong Kleene logic has three truth
values: <em>true</em>, <em>false</em>, and <em>neither</em>. Our first
question is this: what is the omniscient credence in a proposition
that is neither true nor false? Williams argues that it should be
zero. And then he shows that, if we measure the inaccuracy of a
credence function at a world as its distance from the omniscient
credence function at that world in the usual way, then the credence
functions that are not accuracy dominated are precisely those that
satisfy the norm of <strong>Generalized Probabilism</strong>:</p>

<div class="indent">

<p>
<strong>Generalized Probabilism</strong> Suppose \(\models\) is the
logical consequence relation of the correct logic. A rational
agent&rsquo;s credence function \(c\) at a given time is a generalized
probability function for that logic. That is:</p>

<ol type="i">

<li> If \(\bot \models\), then \(c(\bot) = 0\).</li>

<li> If \(\models \top\), then \(c(\top) = 1\).</li>

<li> If \(A \models B\), then \(c(A)\leq c(B)\).</li>

<li>\(c(A \vee B) = c(A) + c(B) - c(A \wedge B)\).</li>
</ol>
</div>

<p>
Note that, if \(\models\) is classical, then Generalized Probabilism
is equivalent to Probabilism. </p>
</div>

<div id="bibliography">

<h2><a id="Bib">Bibliography</a></h2>

<ul class="hanging">

<li>Ahlstrom-Vij, K. &amp; J. Dunn (eds.), 2018, <em>Epistemic
Consequentialism</em>, Oxford: Oxford University Press.</li>

<li>Berker, S., 2013a, &ldquo;Epistemic Teleology and the Separateness
of Propositions&rdquo;, <em>Philosophical Review</em>, 122(3):
337&ndash;393.</li>

<li>&ndash;&ndash;&ndash;, 2013b, &ldquo;The Rejection of Epistemic
Consequentialism&rdquo;, <em>Philosophical Issues (Supp.
No&ucirc;s)</em>, 23(1): 363&ndash;387.</li>

<li>Briggs, R. A. &amp; R. Pettigrew, forthcoming, &ldquo;An
Accuracy-Dominance Argument for Conditionalization&rdquo;,
<em>No&ucirc;s</em>. doi:10.1111/nous.12258</li>

<li>BonJour, L., 1985, <em>The Structure of Empirical Knowledge</em>,
Cambridge, MA: Harvard University Press.</li>

<li>Caie, M., 2013, &ldquo;Rational Probabilistic Incoherence&rdquo;,
<em>Philosophical Review</em>, 122(4): 527&ndash;575.</li>

<li>Carr, J., 2015, &ldquo;Epistemic Expansions&rdquo;, <em>Res
Philosophica</em>, 92(2): 217&ndash;236.</li>

<li>&ndash;&ndash;&ndash;, 2017, &ldquo;Epistemic Utility Theory and
the Aim of Belief&rdquo;, <em>Philosophy and Phenomenological
Research</em>, 95(3): 511&ndash;534.</li>

<li>&ndash;&ndash;&ndash;, 2019, &ldquo;A Modesty Proposal&rdquo;,
<em>Synthese</em>. doi:10.1007/s11229-019-02301-x</li>

<li>de Finetti, B., 1974, <em>Theory of Probability</em> Vol. 1, New
York: Wiley.</li>

<li>Dorst, K., 2017, &ldquo;Lockeans Maximize Expected
Accuracy&rdquo;, <em>Mind</em>, 128(509): 175&ndash;211.</li>

<li>Dunn, J., 2018, &ldquo;Accuracy, Verisimilitude, and Scoring
Rules&rdquo;, <em>Australasian Journal of Philosophy</em>, 97(1):
151&ndash;166.</li>

<li>Easwaran, K., 2013, &ldquo;Expected Accuracy Supports
Conditionalization&mdash;and Conglomerability and Reflection&rdquo;,
<em>Philosophy of Science</em>, 80(1): 119&ndash;142.</li>

<li>&ndash;&ndash;&ndash;, 2016, &ldquo;Dr Truthlove, Or: How I
Learned to Stop Worrying and Love Bayesian Probabilities&rdquo;,
<em>No&ucirc;s</em>, 50(4): 816&ndash;853</li>

<li>Easwaran, K. &amp; B. Fitelson, 2012, &ldquo;An
&lsquo;evidentialist&rsquo; worry about Joyce&rsquo;s argument for
Probabilism&rdquo;, <em>Dialectica</em>, 66(3): 425&ndash;433.</li>

<li>&ndash;&ndash;&ndash;, 2015, &ldquo;Accuracy, Coherence, and
Evidence&rdquo;, <em>Oxford Studies in Epistemology</em>, 5,
61&ndash;96.</li>

<li>Fraassen, B.C. van, 1983, &ldquo;Calibration: Frequency
Justification for Personal Probability&rdquo;, in R.S. Cohen &amp; L.
Laudan (eds.), <em>Physics, Philosophy, and Psychoanalysis</em>,
Dordrecht: Springer.</li>

<li>Goldman, A.I., 2002, <em>Pathways to Knowledge: Private and
Public</em>, New York: Oxford University Press.</li>

<li>Greaves, H., 2013, &ldquo;Epistemic Decision Theory&rdquo;,
<em>Mind</em>, 122(488): 915&ndash;952.</li>

<li>Greaves, H. &amp; D. Wallace, 2006, &ldquo;Justifying
Conditionalization: Conditionalization Maximizes Expected Epistemic
Utility&rdquo;, <em>Mind</em>, 115(459): 607&ndash;632.</li>

<li>Harman, G., 1973, <em>Thought</em>, Princeton, NJ: Princeton
University Press.</li>

<li>H&aacute;jek, A., 2008, &ldquo;Arguments For&mdash;Or
Against&mdash;Probabilism?&rdquo;, <em>The British Journal for the
Philosophy of Science</em>, 59(4): 793&ndash;819.</li>

<li>&ndash;&ndash;&ndash;, 2009, &ldquo;Fifteen Arguments against
Hypothetical Frequentism&rdquo;, <em>Erkenntnis</em>, 70:
211&ndash;235.</li>

<li>Horowitz, S., 2014, &ldquo;Immoderately rational&rdquo;,
<em>Philosophical Studies</em>, 167: 41&ndash;56.</li>

<li>Huttegger, S.M., 2013, &ldquo;In Defense of Reflection&rdquo;,
<em>Philosophy of Science</em>, 80(3): 413&ndash;433.</li>

<li>Jaffray, J-Y., 1989, &ldquo;Coherent bets under partially
resolving uncertainty and belief functions&rdquo;, <em>Theory and
Decision</em>, 26: 90&ndash;105.</li>

<li>Jeffrey, R., 1965, <em>The Logic of Decision</em>, New York:
McGraw-Hill.</li>

<li>Jeffrey, R., 1983, <em>The Logic of Decision</em>
(2<sup>nd</sup>). Chicago; London: University of Chicago Press.</li>

<li>Jenkins, C.S., 2007, &ldquo;Entitlement and Rationality&rdquo;,
<em>Synthese</em>, 157: 25&ndash;45.</li>

<li>Joyce, J.M., 1998, &ldquo;A Nonpragmatic Vindication of
Probabilism&rdquo;, <em>Philosophy of Science</em>, 65(4):
575&ndash;603.</li>

<li>&ndash;&ndash;&ndash;, 2009, &ldquo;Accuracy and Coherence:
Prospects for an Alethic Epistemology of Partial Belief&rdquo;, in F.
Huber &amp; C. Schmidt-Petri (eds.), <em>Degrees of Belief</em>,
Springer.</li>

<li>&ndash;&ndash;&ndash;, 2018, &ldquo;The True Consequences of
Epistemic Consequentialism&rdquo;, in Ahlstrom-Vij &amp; Dunn
2018.</li>

<li>Kelley, M., 2019, <em>Accuracy Dominance on Infinite Opinion
Sets</em>, MA Thesis, UC Berkeley.
 <a href="https://mikaylalynnkelley.weebly.com/uploads/1/1/7/0/117045875/ma_thesis_final_draft.pdf" target="other">[Online version available here]</a>
 </li>

<li>Konek, J., 2016, &ldquo;Probabilistic Knowledge and Cognitive
Ability&rdquo;, <em>Philosophical Review</em>, 125(4):
509&ndash;587.</li>

<li>&ndash;&ndash;&ndash;, 2019, &ldquo;IP Scoring Rules: Foundations
and Applications&rdquo;, <em>Proceedings of Machine Learning
Research</em>, 103: 256&ndash;264.</li>

<li>Konek, J. &amp; B.A. Levinstein, 2019, &ldquo;The Foundations of
Epistemic Decision Theory&rdquo;, <em>Mind</em>, 128(509):
69&ndash;107.</li>

<li>Lam, B., 2013, &ldquo;Calibrated Probabilities and the
Epistemology of Disagreement&rdquo;, <em>Synthese</em>, 190(6):
1079&ndash;1098.</li>

<li>Lange, M., 1999, &ldquo;Calibration and the Epistemological Role
of Bayesian Conditionalization&rdquo;, <em>The Journal of
Philosophy</em>, 96(6): 294&ndash;324.</li>

<li>Leitgeb, H. &amp; R. Pettigrew, 2010a, &ldquo;An Objective
Justification of Bayesianism I: Measuring Inaccuracy&rdquo;,
<em>Philosophy of Science</em>, 77: 201&ndash;235.</li>

<li>&ndash;&ndash;&ndash;, 2010b, &ldquo;An Objective Justification of
Bayesianism II: The Consequences of Minimizing Inaccuracy&rdquo;,
<em>Philosophy of Science</em>, 77: 236&ndash;272.</li>

<li>Levinstein, B.A., 2012, &ldquo;Leitgeb and Pettigrew on Accuracy
and Updating&rdquo;, <em>Philosophy of Science</em>, 79(3):
413&ndash;424.</li>

<li>&ndash;&ndash;&ndash;, 2015, &ldquo;With All Due Respect: The
Macro-Epistemology of Disagreement&rdquo;, <em>Philosophers&rsquo;
Imprint</em>, 15(3): 1&ndash;20.</li>

<li>&ndash;&ndash;&ndash;, 2018, &ldquo;An Objection of Varying
Importance to Epistemic Utility Theory&rdquo;, <em>Philosophical
Studies</em>. doi:10.1007/s11098-018-1157-9</li>

<li>Lewis, D., 1980, &ldquo;A Subjectivist&rsquo;s Guide to Objective
Chance&rdquo;, in R.C. Jeffrey (ed.), <em>Studies in Inductive Logic
and Probability</em> (Vol. II). Berkeley: University of California
Press.</li>

<li>Maher, P., 1993, <em>Betting on Theories</em>, Cambridge:
Cambridge University Press.</li>

<li>&ndash;&ndash;&ndash;, 2002, &ldquo;Joyce&rsquo;s Argument for
Probabilism&rdquo;, <em>Philosophy of Science</em>, 69(1):
73&ndash;81.</li>

<li>Mayo-Wilson, C. &amp; G. Wheeler, 2016, &ldquo;Scoring Imprecise
Credences: A Mildly Immodest Proposal&rdquo;, <em>Philosophy and
Phenomenological Research</em>, 92(1): 55&ndash;78.</li>

<li>&ndash;&ndash;&ndash;, ms., &ldquo;Epistemic Decision
Theory&rsquo;s Reckoning&rdquo;. Unpublished manuscript.
 <a href="http://philsci-archive.pitt.edu/16374/" target="other">[Online version available here]</a>
 </li>

<li>Meacham, C. J. G., 2018, &ldquo;Can All-Accuracy Accounts Justify
Evidential Norms&rdquo;, in Ahlstrom-Vij &amp; Dunn 2018.</li>

<li>Moss, S., 2011, &ldquo;Scoring Rules and Epistemic
Compromise&rdquo;, <em>Mind</em>, 120(480): 1053&ndash;1069.</li>

<li>Oddie, G., 2019, &ldquo;What Accuracy Could Not Be&rdquo;, <em>The
British Journal for the Philosophy of Science</em>, 70(2):
551&ndash;580.</li>

<li>Paris, J. B., 2001, &ldquo;A Note on the Dutch Book Method&rdquo;,
<em>Proceedings of the Second International Symposium on Imprecise
Probabilities and their Applications</em> Ithaca, NY: Shaker.</li>

<li>Pettigrew, R., 2010, &ldquo;Modelling uncertainty&rdquo;,
<em>Grazer Philosophische Studien</em>, 80.</li>

<li>&ndash;&ndash;&ndash;, 2013a, &ldquo;A New Epistemic Utility
Argument for the Principal Principle&rdquo;, <em>Episteme</em>, 10(1):
19&ndash;35.</li>

<li>&ndash;&ndash;&ndash;, 2013b, &ldquo;Epistemic Utility and Norms
for Credence&rdquo;, <em>Philosophy Compass</em>, 8(10):
897&ndash;908.</li>

<li>&ndash;&ndash;&ndash;, 2014a, &ldquo;Accuracy and Evidence&rdquo;,
<em>Dialectica</em>.</li>

<li>&ndash;&ndash;&ndash;, 2014b, &ldquo;Accuracy, Risk, and the
Principle of Indifference&rdquo;, <em>Philosophy and Phenomenological
Research</em>.</li>

<li>&ndash;&ndash;&ndash;, 2016, <em>Accuracy and the Laws of
Credence</em>, Oxford: Oxford University Press.</li>

<li>&ndash;&ndash;&ndash;, 2018a, &ldquo;Making Things Right: the true
consequences of decision theory in epistemology&rdquo;, in
Ahlstrom-Vij &amp; Dunn 2018.</li>

<li>&ndash;&ndash;&ndash;, 2018b, &ldquo;The Population Ethics of
Belief: In Search of an Epistemic Theory X&rdquo;,
<em>No&ucirc;s</em>, 52(2): 336&ndash;372.</li>

<li>Predd, J., et al., 2009, &ldquo;Probabilistic Coherence and Proper
Scoring Rules&rdquo;, <em>IEEE Transactions on Information Theory</em>
55(10): 4786&ndash;4792.</li>

<li>Rosenkrantz, R.D., 1981, <em>Foundations and Applications of
Inductive Probability</em>, Atascadero, CA: Ridgeview Press.</li>

<li>Schoenfield, M., 2016, &ldquo;Conditionalization does not (in
general) Maximize Expected Accuracy&rdquo;, <em>Mind</em>, 126(504):
1155&ndash;1187</li>

<li>&ndash;&ndash;&ndash;, 2017, &ldquo;The Accuracy and Rationality
of Imprecise Credences&rdquo;, <em>No&ucirc;s</em>, 51(4):
667&ndash;685.</li>

<li>&ndash;&ndash;&ndash;, 2019, &ldquo;Accuracy and Verisimilitude:
The Good, The Bad, and The Ugly&rdquo;, <em>The British Journal for
the Philosophy of Science</em>. doi:10.1093/bjps/axz032</li>

<li>Seidenfeld, T., 1985, &ldquo;Calibration, Coherence, and Scoring
Rules&rdquo;, <em>Philosophy of Science</em>, 52(2):
274&ndash;294.</li>

<li>Seidenfeld, T., M.J. Schervish, &amp; J.B. Kadane, 2012,
&ldquo;Forecasting with imprecise probabilities&rdquo;,
<em>International Journal of Approximate Reasoning</em>, 53:
1248&ndash;1261.</li>

<li>Shimony, A., 1988, &ldquo;An Adamite Derivation of the Calculus of
Probability&rdquo;, in J. Fetzer (ed.), 1988, <em>Probability and
Causality: Essays in Honor of Wesley C. Salmon</em>, Dordrecht:
Reidel.</li>

<li>Walsh, S., ms., &ldquo;Probabilism in Infinite Dimensions&rdquo;.
Unpublished manuscript. </li>

<li>White, R., 2009, &ldquo;Evidential Symmetry and Mushy
Credence&rdquo;, <em>Oxford Studies in Epistemology</em>, 3:
161&ndash;186.</li>

<li>Williams, J. R. G., 2012a, &ldquo;Gradational accuracy and
nonclassical semantics&rdquo;, <em>Review of Symbolic Logic</em>,
5(4):513&ndash;537.</li>

<li>&ndash;&ndash;&ndash;, 2012b, &ldquo;Generalized Probabilism:
Dutch Books and accuracy domination&rdquo;, <em>Journal of
Philosophical Logic</em>, 41(5):811&ndash;840.</li>

<li>&ndash;&ndash;&ndash;, 2018, &ldquo;Rational Illogicality&rdquo;,
<em>Australasian Journal of Philosophy</em>, 96(1):
127&ndash;141.</li>

<li>Williamson, T., 2000, <em>Knowledge and its Limits</em>, Oxford:
Oxford University Press.</li>
</ul>

</div> 
<div id="academic-tools">

<h2 id="Aca">Academic Tools</h2>

<blockquote>
<table class="vert-top">
<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=epistemic-utility" target="other">How to cite this entry</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://leibniz.stanford.edu/friends/preview/epistemic-utility/" target="other">Preview the PDF version of this entry</a> at the
 <a href="https://leibniz.stanford.edu/friends/" target="other">Friends of the SEP Society</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/inpho.png" alt="inpho icon" /></td>
<td><a href="https://www.inphoproject.org/entity?sep=epistemic-utility&amp;redirect=True" target="other">Look up topics and thinkers related to this entry</a>
 at the Internet Philosophy Ontology Project (InPhO).</td>
</tr>

<tr>
<td><img src="../../symbols/pp.gif" alt="phil papers icon" /></td>
<td><a href="https://philpapers.org/sep/epistemic-utility/" target="other">Enhanced bibliography for this entry</a>
at <a href="https://philpapers.org/" target="other">PhilPapers</a>, with links to its database.</td>
</tr>

</table>
</blockquote>

</div>

<div id="other-internet-resources">

<h2><a id="Oth">Other Internet Resources</a></h2>

<ul>

<li>Pettigrew, Richard,
 <a href="https://richardpettigrew.com/books/accuracy/" target="other">The webpage for Pettigrew&rsquo;s <em>Accuracy and the Laws of Credence</em></a>.
 This includes video tutorials that work through some of the central
results in accuracy-first epistemology.</li>

<li>Weisberg, Jonathan,
 <a href="https://jonathanweisberg.org/tags/accuracy-for-dummies/" target="other">A series of blogposts</a>
 that walk slowly through the technical side of accuracy-first
epistemology.</li>
</ul>
</div>

<div id="related-entries">

<h2><a id="Rel">Related Entries</a></h2>

<p>

 <a href="../formal-belief/index.html">belief, formal representations of</a> |
 <a href="../epistemology/index.html">epistemology</a> |
 <a href="../epistemology-bayesian/index.html">epistemology: Bayesian</a> |
 <a href="../probability-interpret/index.html">probability, interpretations of</a>

</p>

</div>

<script type="text/javascript" src="local.js"></script>
<script type="text/javascript" src="../../MathJax/MathJaxb198.js?config=TeX-MML-AM_CHTML"></script>

</div><!-- #aueditable --><!--DO NOT MODIFY THIS LINE AND BELOW-->

<!-- END ARTICLE HTML -->

</div> <!-- End article-content -->

  <div id="article-copyright">
    <p>
 <a href="../../info.html#c">Copyright &copy; 2019</a> by

<br />
<a href="http://eis.bris.ac.uk/~rp3959/" target="other">Richard Pettigrew</a>
&lt;<a href="m&#97;ilto:richard&#37;2epettigrew&#37;40bris&#37;2eac&#37;2euk"><em>richard<abbr title=" dot ">&#46;</abbr>pettigrew<abbr title=" at ">&#64;</abbr>bris<abbr title=" dot ">&#46;</abbr>ac<abbr title=" dot ">&#46;</abbr>uk</em></a>&gt;
    </p>
  </div>

</div> <!-- End article -->

<!-- NOTE: article banner is outside of the id="article" div. -->
<div id="article-banner" class="scroll-block">
  <div id="article-banner-content">
    <a href="../../fundraising/index.html">
    Open access to the SEP is made possible by a world-wide funding initiative.<br />
    The Encyclopedia Now Needs Your Support<br />
    Please Read How You Can Help Keep the Encyclopedia Free</a>
  </div>
</div> <!-- End article-banner -->

    </div> <!-- End content -->

    <div id="footer">

      <div id="footer-menu">
        <div class="menu-block">
          <h4><i class="icon-book"></i> Browse</h4>
          <ul role="menu">
            <li><a href="../../contents.html">Table of Contents</a></li>
            <li><a href="../../new.html">What's New</a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
            <li><a href="../../published.html">Chronological</a></li>
            <li><a href="../../archives/index.html">Archives</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-info-sign"></i> About</h4>
          <ul role="menu">
            <li><a href="../../info.html">Editorial Information</a></li>
            <li><a href="../../about.html">About the SEP</a></li>
            <li><a href="../../board.html">Editorial Board</a></li>
            <li><a href="../../cite.html">How to Cite the SEP</a></li>
            <li><a href="../../special-characters.html">Special Characters</a></li>
            <li><a href="../../tools/index.html">Advanced Tools</a></li>
            <li><a href="../../contact.html">Contact</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-leaf"></i> Support SEP</h4>
          <ul role="menu">
            <li><a href="../../support/index.html">Support the SEP</a></li>
            <li><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
            <li><a href="../../support/donate.html">Make a Donation</a></li>
            <li><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
          </ul>
        </div>
      </div> <!-- End footer menu -->

      <div id="mirrors">
        <div id="mirror-info">
          <h4><i class="icon-globe"></i> Mirror Sites</h4>
          <p>View this site from another server:</p>
        </div>
        <div class="btn-group open">
          <a class="btn dropdown-toggle" data-toggle="dropdown" href="https://plato.stanford.edu/">
            <span class="flag flag-usa"></span> USA (Main Site) <span class="caret"></span>
            <span class="mirror-source">Philosophy, Stanford University</span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="../../mirrors.html">Info about mirror sites</a></li>
          </ul>
        </div>
      </div> <!-- End mirrors -->
      
      <div id="site-credits">
        <p>The Stanford Encyclopedia of Philosophy is <a href="../../info.html#c">copyright &copy; 2021</a> by <a href="http://mally.stanford.edu/">The Metaphysics Research Lab</a>, Department of Philosophy, Stanford University</p>
        <p>Library of Congress Catalog Data: ISSN 1095-5054</p>
      </div> <!-- End site credits -->

    </div> <!-- End footer -->

  </div> <!-- End container -->

   <!-- NOTE: Script required for drop-down button to work (mirrors). -->
  <script>
    $('.dropdown-toggle').dropdown();
  </script>

</body>

<!-- Mirrored from seop.illc.uva.nl/entries/epistemic-utility/ by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 22 Jun 2022 19:38:19 GMT -->
</html>
