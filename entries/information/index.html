<!DOCTYPE html>
<!--[if lt IE 7]> <html class="ie6 ie"> <![endif]-->
<!--[if IE 7]>    <html class="ie7 ie"> <![endif]-->
<!--[if IE 8]>    <html class="ie8 ie"> <![endif]-->
<!--[if IE 9]>    <html class="ie9 ie"> <![endif]-->
<!--[if !IE]> --> <html> <!-- <![endif]-->

<!-- Mirrored from seop.illc.uva.nl/entries/information/ by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 22 Jun 2022 19:50:09 GMT -->
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>
Information (Stanford Encyclopedia of Philosophy)
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="robots" content="noarchive, noodp" />
<meta property="citation_title" content="Information" />
<meta property="citation_author" content="Adriaans, Pieter" />
<meta property="citation_publication_date" content="2012/10/26" />
<meta name="DC.title" content="Information" />
<meta name="DC.creator" content="Adriaans, Pieter" />
<meta name="DCTERMS.issued" content="2012-10-26" />
<meta name="DCTERMS.modified" content="2020-08-18" />

<!-- NOTE: Import webfonts using this link: -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300,600,200&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css" />

<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap-responsive.min.css" />
<link rel="stylesheet" type="text/css" href="../../css/font-awesome.min.css" />
<!--[if IE 7]> <link rel="stylesheet" type="text/css" href="../../css/font-awesome-ie7.min.css"> <![endif]-->
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/style.css" />
<link rel="stylesheet" type="text/css" media="print" href="../../css/print.css" />
<link rel="stylesheet" type="text/css" href="../../css/entry.css" />
<!--[if IE]> <link rel="stylesheet" type="text/css" href="../../css/ie.css" /> <![endif]-->
<script type="text/javascript" src="../../js/jquery-1.9.1.min.js"></script>
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>

<!-- NOTE: Javascript for sticky behavior needed on article and ToC pages -->
<script type="text/javascript" src="../../js/jquery-scrolltofixed-min.js"></script>
<script type="text/javascript" src="../../js/entry.js"></script>

<!-- SEP custom script -->
<script type="text/javascript" src="../../js/sep.js"></script>
</head>

<!-- NOTE: The nojs class is removed from the page if javascript is enabled. Otherwise, it drives the display when there is no javascript. -->
<body class="nojs article" id="pagetopright">
<div id="container">
<div id="header-wrapper">
  <div id="header">
    <div id="branding">
      <div id="site-logo"><a href="../../index.html"><img src="../../symbols/sep-man-red.png" alt="SEP logo" /></a></div>
      <div id="site-title"><a href="../../index.html">Stanford Encyclopedia of Philosophy</a></div>
    </div>
    <div id="navigation">
      <div class="navbar">
        <div class="navbar-inner">
          <div class="container">
            <button class="btn btn-navbar collapsed" data-target=".collapse-main-menu" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Menu </button>
            <div class="nav-collapse collapse-main-menu in collapse">
              <ul class="nav">
                <li class="dropdown open"><a id="drop1" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-book"></i> Browse</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop1">
                    <li><a href="../../contents.html">Table of Contents</a></li>
                    <li><a href="../../new.html">What's New</a></li>
                    <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
                    <li><a href="../../published.html">Chronological</a></li>
                    <li><a href="../../archives/index.html">Archives</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop2" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-info-sign"></i> About</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop2">
                    <li><a href="../../info.html">Editorial Information</a></li>
                    <li><a href="../../about.html">About the SEP</a></li>
                    <li><a href="../../board.html">Editorial Board</a></li>
                    <li><a href="../../cite.html">How to Cite the SEP</a></li>
                    <li><a href="../../special-characters.html">Special Characters</a></li>
                    <li><a href="../../tools/index.html">Advanced Tools</a></li>
                    <li><a href="../../contact.html">Contact</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop3" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-leaf"></i> Support SEP</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop3">
                    <li><a href="../../support/index.html">Support the SEP</a></li>
                    <li><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
                    <li><a href="../../support/donate.html">Make a Donation</a></li>
                    <li><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
                  </ul>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End navigation -->
    
    <div id="search">
      <form id="search-form" method="get" action="https://seop.illc.uva.nl/search/searcher.py">
        <input type="search" name="query" placeholder="Search SEP" />
        <div class="search-btn-wrapper"><button class="btn search-btn" type="submit"><i class="icon-search"></i></button></div>
      </form>
    </div>
    <!-- End search --> 
    
  </div>
  <!-- End header --> 
</div>
<!-- End header wrapper -->

<div id="content">

<!-- Begin article sidebar -->
<div id="article-sidebar" class="sticky">
  <div class="navbar">
    <div class="navbar-inner">
      <div class="container">
        <button class="btn btn-navbar" data-target=".collapse-sidebar" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Entry Navigation </button>
        <div id="article-nav" class="nav-collapse collapse-sidebar in collapse">
          <ul class="nav">
            <li><a href="#toc">Entry Contents</a></li>
            <li><a href="#Bib">Bibliography</a></li>
            <li><a href="#Aca">Academic Tools</a></li>
            <li><a href="https://leibniz.stanford.edu/friends/preview/information/">Friends PDF Preview <i class="icon-external-link"></i></a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=information">Author and Citation Info <i class="icon-external-link"></i></a> </li>
            <li><a href="#pagetopright" class="back-to-top">Back to Top <i class="icon-angle-up icon2x"></i></a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</div>
<!-- End article sidebar --> 

<!-- NOTE: Article content must have two wrapper divs: id="article" and id="article-content" -->
<div id="article">
<div id="article-content">

<!-- BEGIN ARTICLE HTML -->


<div id="aueditable"><!--DO NOT MODIFY THIS LINE AND ABOVE-->

<h1>Information</h1><div id="pubinfo"><em>First published Fri Oct 26, 2012; substantive revision Tue Aug 18, 2020</em></div>

<div id="preamble">

<p>
Philosophy of Information deals with the philosophical analysis of the
notion of information both from a historical and a systematic
perspective. With the emergence of the empiricist theory of knowledge
in early modern philosophy, the development of various mathematical
theories of information in the twentieth century and the rise of
information technology, the concept of &ldquo;information&rdquo; has
conquered a central place in the sciences and in society. This
interest also led to the emergence of a separate branch of philosophy
that analyzes information in all its guises (Adriaans &amp; van
Benthem 2008a,b; Lenski 2010; Floridi 2002, 2011). Information has
become a central category in both the sciences and the humanities and
the reflection on information influences a broad range of
philosophical disciplines varying from logic (Dretske 1981; van
Benthem &amp; van Rooij 2003; van Benthem 2006, see the entry on
 <a href="../logic-information/index.html">logic and information</a>),
 epistemology (Simondon 1989) to ethics (Floridi 1999) and esthetics
(Schmidhuber 1997a; Adriaans 2008) to ontology (Zuse 1969; Wheeler
1990; Schmidhuber 1997b; Wolfram 2002; Hutter 2010). </p>

<p>
There is no consensus about the exact nature of the field of
philosophy of information. Several authors have proposed a more or
less coherent philosophy of information as an attempt to rethink
philosophy from a new perspective: e.g., quantum physics
(Mugur-Sch&auml;chter 2002), 
 logic (Brenner 2008), semantic information (Floridi 2011; Adams &amp;
de Moraes 2016, see the entry on
 <a href="../information-semantic/index.html">semantic conceptions of information</a>), 
communication and message systems (Capurro &amp;
Holgate 2011) and meta-philosophy (Wu 2010, 2016). Others (Adriaans
&amp; van Benthem 2008a; Lenski 2010) see it more as a technical
discipline with deep roots in the history of philosophy and
consequences for various disciplines like methodology, epistemology
and ethics. Whatever one&rsquo;s interpretation of the nature of
philosophy of information is, it seems to imply an ambitious research
program consisting of many sub-projects varying from the
reinterpretation of the history of philosophy in the context of modern
theories of information, to an in depth analysis of the role of
information in science, the humanities and society as a whole. </p>

<p>
The term &ldquo;information&rdquo; in colloquial speech is currently
predominantly used as an abstract mass-noun used to denote any amount
of data, code or text that is stored, sent, received or manipulated in
any medium. The detailed history of both the term
&ldquo;information&rdquo; and the various concepts that come with it
is complex and for the larger part still has to be written (Seiffert
1968; Schnelle 1976; Capurro 1978, 2009; Capurro &amp; Hj&oslash;rland
2003). The exact meaning of the term &ldquo;information&rdquo; varies
in different philosophical traditions and its colloquial use varies
geographically and over different pragmatic contexts. Although an
analysis of the notion of information has been a theme in Western
philosophy from its early inception, the explicit analysis of
information as a philosophical concept is recent, and dates back to
the second half of the twentieth century. At this moment it is clear
that information is a pivotal concept in the sciences and humanities
and in our every day life. Everything we know about the world is based
on information we received or gathered and every science in principle
deals with information. There is a network of related concepts of
information, with roots in various disciplines like physics,
mathematics, logic, biology, economy and epistemology. All these
notions cluster around two central properties: </p>

<div class="indent">

<p><strong>Information is extensive.</strong> Central is the concept
of <em>additivity</em>: the combination of two independent datasets
with the same amount of information contains <em>twice</em> as much
information as the separate individual datasets. The notion of
extensiveness emerges naturally in our interactions with the world
around us when we count and measure objects and structures. Basic
conceptions of more abstract mathematical entities, like sets,
multisets and sequences, were developed early in history on the basis
of structural rules for the manipulation of symbols (Schmandt-Besserat
1992). The mathematical formalisation of extensiveness in terms of the
log function took place in the context of research in to
thermodynamics in the nineteenth (Boltzmann 1866) and early twentieth
century (Gibbs 1906). When coded in terms of more advanced
multi-dimensional numbers systems (complex numbers, quaternions,
octonions) the concept of extensiveness generalizes in to more subtle
notions of additivity that do not meet our everyday intuitions. Yet
they play an important role in recent developments of information
theory based on quantum physics (Von Neumann 1932; Redei &amp;
St&ouml;ltzner 2001, see entry on
 <a href="../qt-entangle/index.html">quantum entanglement and information</a>).
</p>

<p> <strong>Information reduces uncertainty.</strong> The amount of
information we get grows linearly with the amount by which it reduces
our uncertainty until the moment that we have received all possible
information and the amount of uncertainty is zero. The relation
between uncertainty and information was probably first formulated by
the empiricists (Locke 1689; Hume 1748). Hume explicitly observes that
a choice from a larger selection of possibilities gives more
information. This observation reached its canonical mathematical
formulation in the function proposed by Hartley (1928) that defines
the amount of information we get when we select an element from a
finite set. The only mathematical function that unifies these two
intuitions about extensiveness and probability is the one that defines
the information in terms of the negative log of the probability:
\(I(A)= -\log P(A)\) (Shannon 1948; Shannon &amp; Weaver 1949,
R&eacute;nyi 1961). </p>
</div>

<p>
The elegance of this formula however does not shield us from the
conceptual problems it harbors. In the twentieth century various
proposals for formalization of concepts of information were made: </p>

<ul class="jfy">

<li><strong>Qualitative Theories of Information</strong>

<ol>

<li><strong>Semantic Information:</strong> Bar-Hillel and Carnap
developed a theory of semantic Information (1953). Floridi (2002,
2003, 2011) defines semantic information as well-formed, meaningful
and truthful data. Formal entropy based definitions of information
(Fisher, Shannon, Quantum, Kolmogorov) work on a more general level
and do not necessarily measure information in meaningful truthful
datasets, although one might defend the view that in order to be
measurable the data must be well-formed (for a discussion see
 <a href="#LogiSemaInfo">section 6.6 on Logic and Semantic Information</a>). 
Semantic information is close to our everyday naive notion of
information as something that is conveyed by true statements about the
world. </li>

<li><strong>Information as a state of an agent:</strong> the formal
logical treatment of notions like knowledge and belief was initiated
by Hintikka (1962, 1973). Dretske (1981) and van Benthem &amp; van
Rooij (2003) studied these notions in the context of information
theory, cf. van Rooij (2003) on questions and answers, or Parikh &amp; Ramanujam
(2003) on general messaging. Also Dunn seems to have this notion in
mind when he defines information as &ldquo;what is left of knowledge
when one takes away belief, justification and truth&rdquo; (Dunn
2001: 423; 2008). Vigo proposed a Structure-Sensitive Theory of
Information based on the complexity of concept acquisition by agents
(Vigo 2011, 2012). </li>
</ol></li>

<li><strong>Quantitative Theories of Information</strong>

<ol>

<li><strong>Nyquist&rsquo;s function:</strong> Nyquist (1924) was
probably the first to express the amount of &ldquo;intelligence&rdquo;
that could be transmitted given a certain line speed of a telegraph
systems in terms of a log function: \(W= k \log m\), where <i>W</i> is
the speed of transmission, <i>K</i> is a constant, and <i>m</i> are
the different voltage levels one can choose from.</li>

<li><strong>Fisher information:</strong> the amount of information
that an observable random variable <i>X</i> carries about an unknown
parameter \(\theta\) upon which the probability of <i>X</i> depends
(Fisher 1925).</li>

<li><strong>The Hartley function:</strong> (Hartley 1928, R&eacute;nyi
1961, Vigo 2012). The amount of information we get when we select an
element from a finite set <i>S</i> under uniform distribution is the
logarithm of the cardinality of that set. </li>

<li><strong>Shannon information:</strong> the entropy, <i>H</i>, of a
discrete random variable <i>X</i> is a measure of the amount of
uncertainty associated with the value of <i>X</i> (Shannon 1948;
Shannon &amp; Weaver 1949).</li>

<li><strong>Kolmogorov complexity:</strong> the information in a
binary string <i>x</i> is the length of the shortest program <i>p</i>
that produces <i>x</i> on a reference universal Turing machine
<i>U</i> (Turing 1937; Solomonoff 1960, 1964a,b, 1997; Kolmogorov
1965; Chaitin 1969, 1987).</li>

<li><strong>Entropy measures in Physics:</strong> Although they are
not in all cases strictly measures of information, the different
notions of entropy defined in physics are closely related to
corresponding concepts of information. We mention <em>Boltzmann
Entropy</em> (Boltzmann, 1866) closely related to the Hartley Function
(Hartley 1928), <em>Gibbs Entropy</em> (Gibbs 1906) formally
equivalent to Shannon entropy and various generalizations like
<em>Tsallis Entropy</em> (Tsallis 1988) and <em>R&eacute;nyi
Entropy</em> (R&eacute;nyi 1961). </li>

<li><strong>Quantum Information:</strong> The qubit is a
generalization of the classical bit and is described by a quantum
state in a two-state quantum-mechanical system, which is formally
equivalent to a two-dimensional vector space over the complex numbers
(Von Neumann 1932; Redei &amp; St&ouml;ltzner 2001).</li>
</ol> </li>
</ul>

<p>
Until recently the possibility of a unification of these theories was
generally doubted (Adriaans &amp; van Benthem 2008a), but after two
decades of research, perspectives for unification seem better. </p>

<p>
The contours of a unified concept of information emerges along the
following lines: </p>

<ul class="jfy">

<li> Philosophy of information is a sub-discipline of philosophy,
intricately related to the philosophy of logic and mathematics.
Philosophy of <strong>semantic information</strong> (Floridi 2011,
D&rsquo;Alfonso 2012, Adams &amp; de Moraes, 2016) again is a
sub-discipline of philosophy of information (see the informational map
in the entry on
 <a href="../information-semantic/index.html">semantic conceptions of information</a>).
 From this perspective philosophy of information is interested in the
investigation of the subject at the most general level: data,
well-formed data, environmental data etc. Philosophy of semantic
information adds the dimensions of <em>meaning</em> and
<em>truthfulness</em>. It is possible to interpret quantitative
theories of information in the framework of a philosophy of semantic
information (see
 <a href="#LogiSemaInfo">section 6.5</a> for an in-depth discussion). </li>

<li> Various <strong>quantitative concepts of information</strong> are
associated with different narratives (counting, receiving messages,
gathering information, computing) rooted in the same basic
mathematical framework. Many problems in philosophy of information
center around related problems in philosophy of mathematics.
Conversions and reductions between various formal models have been
studied (Cover &amp; Thomas 2006; Gr&uuml;nwald &amp; Vit&aacute;nyi
2008; Bais &amp; Farmer 2008). The situation that seems to emerge is
not unlike the concept of energy: there are various formal
sub-theories about energy (kinetic, potential, electrical, chemical,
nuclear) with well-defined transformations between them. Apart from
that, the term &ldquo;energy&rdquo; is used loosely in colloquial
speech. </li>

<li> <strong>Agent based</strong> concepts of information emerge
naturally when we extend our interest from simple measurement and
symbol manipulation to the more complex paradigm of an agent with
knowledge, beliefs, intentions and freedom of choice. They are
associated with the deployment of other concepts of information. </li>
</ul>

<p>
The emergence of a coherent theory to measure information
quantitatively in the twentieth century is closely related to the
development of the theory of computing. Central in this context are
the notions of <strong>Universality</strong>, <strong>Turing
equivalence</strong> and <strong>Invariance:</strong> because the
concept of a Turing system defines the notion of a universal
programmable computer, all universal models of computation seem to
have the same power. This implies that all possible measures of
information definable for universal models of computation (Recursive
Functions, Turing Machine, Lambda Calculus etc.) are
<em>invariant</em> modulo an additive constant. This gives a perspective on a unified theory of
information that might dominate the research program for the years to
come.</p>
</div>

<div id="toc">
<!--Entry Contents-->
<ul>
<li><a href="#InfoCollSpee">1. Information in Colloquial Speech</a></li>
<li><a href="#HistTermConcInfo">2. History of the Term and the Concept of Information</a>
   <ul>
   <li><a href="#ClasPhil">2.1 Classical Philosophy</a></li>
   <li><a href="#MediPhil">2.2 Medieval Philosophy</a></li>
   <li><a href="#ModePhil">2.3 Modern Philosophy</a></li>
   <li><a href="#HistDeveMeanTermInfo">2.4 Historical Development of the Meaning of the Term &ldquo;Information&rdquo;</a></li>
   </ul></li>
<li><a href="#BuilBlocModeTheoInfo">3. Building Blocks of Modern Theories of Information</a>
   <ul>
   <li><a href="#Lang">3.1 Languages</a></li>
   <li><a href="#OptiCode">3.2 Optimal Codes</a></li>
   <li><a href="#Numb">3.3 Numbers</a></li>
   <li><a href="#Phys">3.4 Physics</a></li>
   </ul></li>
<li><a href="#DevePhilInfo">4. Developments in Philosophy of Information</a>
   <ul>
   <li><a href="#PoppInfoDegrFals">4.1 Popper: Information as Degree of Falsifiability</a></li>
   <li><a href="#ShanInfoDefiTermProb">4.2 Shannon: Information Defined in Terms of Probability</a></li>
   <li><a href="#SoloKolmChaiInfoLengProg">4.3 Solomonoff, Kolmogorov, Chaitin: Information as the Length of a Program</a></li>
   </ul></li>
<li><a href="#SystCons">5. Systematic Considerations</a>
   <ul>
   <li><a href="#PhilInfoExtePhilMath">5.1 Philosophy of Information as An Extension of Philosophy of Mathematics</a>
      <ul>
      <li><a href="#InfoNatuPhen">5.1.1 Information as a natural phenomenon</a></li>
      <li><a href="#SymbManiExteSetsMultStri">5.1.2 Symbol manipulation and extensiveness: sets, multisets and strings</a></li>
      <li><a href="#SetsNumb">5.1.3 Sets and numbers</a></li>
      <li><a href="#MeasInfoNumb">5.1.4 Measuring information in numbers</a></li>
      <li><a href="#MeasInfoProbSetsNumb">5.1.5 Measuring information and probabilities in sets of numbers</a></li>
      <li><a href="#PersForUnif">5.1.6 Perspectives for unification</a></li>
      <li><a href="#InfoProcFlowInfo">5.1.7 Information processing and the flow of information</a></li>
      <li><a href="#InfoPrimFact">5.1.8 Information, primes, and factors</a></li>
      <li><a href="#IncoArit">5.1.9 Incompleteness of arithmetic</a></li>
      </ul></li>
   <li><a href="#InfoSymbComp">5.2 Information and Symbolic Computation</a>
      <ul>
      <li><a href="#TuriMach">5.2.1 Turing machines</a></li>
      <li><a href="#UnivInva">5.2.2 Universality and invariance</a></li>
      </ul></li>
   <li><a href="#QuanInfoBeyo">5.3 Quantum Information and Beyond</a></li>
   </ul></li>
<li><a href="#AnomParaProb">6. Anomalies, Paradoxes, and Problems</a>
   <ul>
   <li><a href="#ParaSystSear">6.1 The Paradox of Systematic Search</a></li>
   <li><a href="#EffeSearFiniSets">6.2 Effective Search in Finite Sets</a></li>
   <li><a href="#PVersNPProbDescCompVersTimeComp">6.3 The <span>P</span> versus <span>NP</span> Problem, Descriptive Complexity Versus Time Complexity</a></li>
   <li><a href="#ModeSeleDataComp">6.4 Model Selection and Data Compression</a></li>
   <li><a href="#DeteTher">6.5 Determinism and Thermodynamics</a></li>
   <li><a href="#LogiSemaInfo">6.6 Logic and Semantic Information</a></li>
   <li><a href="#MeanComp">6.7 Meaning and Computation</a></li>
   </ul></li>
<li><a href="#Conc">7. Conclusion</a></li>
<li><a href="#Bib">Bibliography</a></li>
<li><a href="#Aca">Academic Tools</a></li>
<li><a href="#Oth">Other Internet Resources</a></li>
<li><a href="#Rel">Related Entries</a></li>
</ul>

<!--Entry Contents-->

<hr />
</div>

<div id="main-text">

<h2 id="InfoCollSpee">1. Information in Colloquial Speech</h2>

<p>
The lack of preciseness and the universal usefulness of the term
&ldquo;information&rdquo; go hand in hand. In our society, in which we
explore reality by means of instruments and installations of ever
increasing complexity (telescopes, cyclotrons) and communicate via
more advanced media (newspapers, radio, television, SMS, the
Internet), it is useful to have an abstract mass-noun for the
&ldquo;stuff&rdquo; that is created by the instruments and that
&ldquo;flows&rdquo; through these media. Historically this general
meaning emerged rather late and seems to be associated with the rise
of mass media and intelligence agencies (Devlin &amp; Rosenberg 2008;
Adriaans &amp; van Benthem 2008b).</p>

<p>
In present colloquial speech the term information is used in various
loosely defined and often even conflicting ways. Most people, for
instance, would consider the following inference <em>prima facie</em>
to be valid:</p>

<div class="indent">

<p>
If I get the information that <i>p</i> then I know that <i>p</i>.</p>
</div>

<p>
The same people would probably have no problems with the statement
that &ldquo;Secret services sometimes distribute false
information&rdquo;, or with the sentence &ldquo;The information
provided by the witnesses of the accident was vague and
conflicting&rdquo;. The first statement implies that information
necessarily is true, while the other statements allow for the
possibility that information is false, conflicting and vague. In
everyday communication these inconsistencies do not seem to create
great trouble and in general it is clear from the pragmatic context
what type of information is designated. These examples suffice to
argue that references to our intuitions as speakers of the English
language are of little help in the development of a rigorous
philosophical theory of information. There seems to be no pragmatic
pressure in everyday communication to converge to a more exact
definition of the notion of information.</p>

<h2 id="HistTermConcInfo">2. History of the Term and the Concept of Information</h2>

<p>
Until the second half of the twentieth century almost no modern
philosopher considered &ldquo;information&rdquo; to be an important
philosophical concept. The term has no lemma in the well-known
encyclopedia of Edwards (1967) and is not mentioned in Windelband
(1903). In this context the interest in &ldquo;Philosophy of
Information&rdquo; is a recent development. Yet, with hindsight from
the perspective of a history of ideas, reflection on the notion of
&ldquo;information&rdquo; has been a predominant theme in the history
of philosophy. The reconstruction of this history is relevant for the
study of information.</p>

<p>
A problem with any &ldquo;history of ideas&rdquo; approach is the
validation of the underlying assumption that the concept one is
studying has indeed continuity over the history of philosophy. In the
case of the historical analysis of information one might ask whether
the concept of &ldquo;<em>informatio</em>&rdquo; discussed by
Augustine has any connection to Shannon information, other than a
resemblance of the terms. At the same time one might ask whether
Locke&rsquo;s &ldquo;historical, plain method&rdquo; is an important
contribution to the emergence of the modern concept of information
although in his writings Locke hardly uses the term
&ldquo;information&rdquo; in a technical sense. As is shown below,
there is a conglomerate of ideas involving a notion of information
that has developed from antiquity till recent times, but further study
of the history of the concept of information is necessary. </p>

<p>
An important recurring theme in the early philosophical analysis of
knowledge is the paradigm of manipulating a piece of wax: either by
simply deforming it, by imprinting a signet ring in it or by writing
characters on it. The fact that wax can take different shapes and
secondary qualities (temperature, smell, touch) while the volume
(extension) stays the same, make it a rich source of analogies,
natural to Greek, Roman and medieval culture, where wax was used both
for sculpture, writing (wax tablets) and encaustic painting. One finds
this topic in writings of such diverse authors as Democritus, Plato,
Aristotle, Theophrastus, Cicero, Augustine, Avicenna, Duns Scotus,
Aquinas, Descartes and Locke.</p>

<h3 id="ClasPhil">2.1 Classical Philosophy</h3>

<p>
In classical philosophy &ldquo;information&rdquo; was a technical
notion associated with a theory of knowledge and ontology that
originated in Plato&rsquo;s (427&ndash;347 BCE) theory of forms,
developed in a number of his dialogues (<em>Phaedo, Phaedrus,
Symposium, Timaeus, Republic</em>). Various imperfect individual
horses in the physical world could be identified as horses, because
they participated in the static atemporal and aspatial idea of
&ldquo;horseness&rdquo; in the world of ideas or forms. When later
authors like Cicero (106&ndash;43 BCE) and Augustine (354&ndash;430
CE) discussed Platonic concepts in Latin they used the terms
<em>informare</em> and <em>informatio</em> as a translation for
technical Greek terms like <em>eidos</em> (essence), <em>idea</em>
(idea), <em>typos</em> (type), <em>morphe</em> (form) and
<em>prolepsis</em> (representation). The root &ldquo;form&rdquo; still
is recognizable in the word <em>in-form-ation</em> (Capurro &amp;
Hj&oslash;rland 2003). Plato&rsquo;s theory of forms was an attempt to
formulate a solution for various philosophical problems: the theory of
forms mediates between a static (Parmenides, ca. 450 BCE) and a
dynamic (Herakleitos, ca. 535&ndash;475 BCE) ontological conception of
reality and it offers a model to the study of the theory of human
knowledge. According to Theophrastus (371&ndash;287 BCE) the analogy
of the wax tablet goes back to Democritos (ca. 460&ndash;380/370 BCE)
(<em>De Sensibus</em> 50). In the <em>Theaetetus</em> (191c,d) Plato
compares the function of our memory with a wax tablet in which our
perceptions and thoughts are imprinted like a signet ring stamps
impressions in wax. Note that the metaphor of imprinting symbols in
wax is essentially spatial (extensive) and can not easily be
reconciled with the aspatial interpretation of ideas supported by
Plato.</p>

<p>
One gets a picture of the role the notion of &ldquo;form&rdquo; plays
in classical methodology if one considers Aristotle&rsquo;s
(384&ndash;322 BCE) doctrine of the four causes. In Aristotelian
methodology understanding an object implied understanding four
different aspects of it:</p>

<div class="indent">
<p><strong>Material Cause:</strong>: that as the result of whose
presence something comes into being&mdash;e.g., the bronze of a statue
and the silver of a cup, and the classes which contain these</p>

<p><strong>Formal Cause:</strong>: the form or pattern; that is, the
essential formula and the classes which contain it&mdash;e.g., the
ratio 2:1 and number in general is the cause of the octave-and the
parts of the formula.</p>

<p><strong>Efficient Cause:</strong>: the source of the first
beginning of change or rest; e.g., the man who plans is a cause, and
the father is the cause of the child, and in general that which
produces is the cause of that which is produced, and that which
changes of that which is changed.</p>

<p><strong>Final Cause:</strong>: the same as &ldquo;end&rdquo;; i.e.,
the final cause; e.g., as the &ldquo;end&rdquo; of walking is
health. For why does a man walk?  &ldquo;To be healthy&rdquo;, we say,
and by saying this we consider that we have supplied the
cause. (Aristotle, <em>Metaphysics</em> 1013a)</p>
</div>

<p>
Note that Aristotle, who rejects Plato&rsquo;s theory of forms as
atemporal aspatial entities, still uses &ldquo;form&rdquo; as a
technical concept. This passage states that knowing the form or
structure of an object, i.e., the <em>information</em>, is a necessary
condition for understanding it. In this sense information is a crucial
aspect of classical epistemology.</p>

<p>
The fact that the ratio 2:1 is cited as an example also illustrates
the deep connection between the notion of forms and the idea that the
world was governed by mathematical principles. Plato believed under
influence of an older Pythagorean (Pythagoras 572&ndash;ca. 500 BCE)
tradition that &ldquo;everything that emerges and happens in the
world&rdquo; could be measured by means of numbers (<em>Politicus</em>
285a). On various occasions Aristotle mentions the fact that Plato
associated ideas with numbers (Vogel 1968: 139). Although
formal mathematical theories about information only emerged in the
twentieth century, and one has to be careful not to interpret the
Greek notion of a number in any modern sense, the idea that
information was essentially a mathematical notion, dates back to
classical philosophy: the form of an entity was conceived as a
structure or pattern that could be described in terms of numbers. Such
a form had both an ontological and an epistemological aspect: it
explains the essence as well as the understandability of the object.
The concept of information thus from the very start of philosophical
reflection was already associated with epistemology, ontology and
mathematics.</p>

<p>
Two fundamental problems that are not explained by the classical
theory of ideas or forms are 1) the actual act of knowing an object
(i.e., if I see a horse in what way is the idea of a horse activated
in my mind) and 2) the process of thinking as manipulation of ideas.
Aristotle treats these issues in <em>De Anime</em>, invoking the
signet-ring-impression-in-wax analogy:</p>

<blockquote>

<p>
By a &ldquo;sense&rdquo; is meant what has the power of receiving into
itself the sensible forms of things without the matter. This must be
conceived of as taking place in the way in which a piece of wax takes
on the impress of a signet-ring without the iron or gold; we say that
what produces the impression is a signet of bronze or gold, but its
particular metallic constitution makes no difference: in a similar way
the sense is affected by what is coloured or flavoured or sounding,
but it is indifferent what in each case the substance is; what alone
matters is what quality it has, i.e., in what ratio its constituents
are combined. (<em>De Anime</em>, Book II, Chp. 12)</p>

<p>
Have not we already disposed of the difficulty about interaction
involving a common element, when we said that mind is in a sense
potentially whatever is thinkable, though actually it is nothing until
it has thought? What it thinks must be in it just as characters may be
said to be on a writing-tablet on which as yet nothing actually stands
written: this is exactly what happens with mind. (<em>De Anime</em>,
Book III, Chp. 4)</p>
</blockquote>

<p>
These passages are rich in influential ideas and can with hindsight be
read as programmatic for a philosophy of information: the process of
<em>informatio</em> can be conceived as the imprint of characters on a
wax tablet (<em>tabula rasa</em>), thinking can be analyzed in terms
of manipulation of symbols.</p>

<h3 id="MediPhil">2.2 Medieval Philosophy</h3>

<p>
Throughout the Middle Ages the reflection on the concept of
<em>informatio</em> is taken up by successive thinkers. Illustrative
for the Aristotelian influence is the passage of Augustine in <em>De
Trinitate</em> book XI. Here he analyzes vision as an analogy for the
understanding of the Trinity. There are three aspects: the corporeal
form in the outside world, the <em>informatio</em> by the sense of
vision, and the resulting form in the mind. For this process of
information Augustine uses the image of a signet ring making an
impression in wax (<em>De Trinitate</em>, XI Cap 2 par 3). Capurro
(2009) observes that this analysis can be interpreted as an early
version of the technical concept of &ldquo;sending a message&rdquo; in
modern information theory, but the idea is older and is a common topic
in Greek thought (Plato <em>Theaetetus</em> 191c,d; Aristotle <em>De
Anime</em>, Book II, Chp. 12, Book III, Chp. 4; Theophrastus <em>De
Sensibus</em> 50).</p>

<p>
The <em>tabula rasa</em> notion was later further developed in the
theory of knowledge of Avicenna (c. 980&ndash;1037 CE): </p>

<blockquote>

<p>
The human intellect at birth is rather like a <em>tabula rasa</em>, a
pure potentiality that is actualized through education and comes to
know. Knowledge is attained through empirical familiarity with objects
in this world from which one abstracts universal concepts. (Sajjad
2006
 [<a href="#Oth">Other Internet Resources [hereafter OIR]</a>])
 </p>
</blockquote>

<p>
The idea of a <em>tabula rasa</em> development of the human mind was
the topic of a novel Hayy ibn Yaqdhan by the Arabic Andalusian
philosopher Ibn Tufail (1105&ndash;1185 CE, known as
&ldquo;Abubacer&rdquo; or &ldquo;Ebn Tophail&rdquo; in the West). This
novel describes the development of an isolated child on a deserted
island. A later translation in Latin under the title <em>Philosophus
Autodidactus</em> (1761) influenced the empiricist John Locke in the
formulation of his <em>tabula rasa</em> doctrine.</p>

<p>
Apart from the permanent creative tension between theology and
philosophy, medieval thought, after the rediscovery of
Aristotle&rsquo;s <em>Metaphysics</em> in the twelfth century inspired
by Arabic scholars, can be characterized as an elaborate and subtle
interpretation and development of, mainly Aristotelian, classical
theory. Reflection on the notion of <em>informatio</em> is taken up,
under influence of Avicenna, by thinkers like Aquinas (1225&ndash;1274
CE) and Duns Scotus (1265/66&ndash;1308 CE). When Aquinas discusses
the question whether angels can interact with matter he refers to the
Aristotelian doctrine of hylomorphism (i.e., the theory that substance
consists of matter (<em>hylo</em> (wood), matter) and form
(<em>morph&egrave;</em>)). Here Aquinas translates this as the
in-formation of matter (<em>informatio materiae</em>) (<em>Summa
Theologiae,</em> 1a 110 2; Capurro 2009). Duns Scotus refers to
<em>informatio</em> in the technical sense when he discusses
Augustine&rsquo;s theory of vision in <em>De Trinitate</em>, XI Cap 2
par 3 (Duns Scotus, 1639, &ldquo;De imagine&rdquo;,
<em>Ordinatio</em>, I, d.3, p.3).</p>

<p>
The tension that already existed in classical philosophy between
Platonic idealism(<em>universalia ante res</em>) and Aristotelian
realism (<em>universalia in rebus</em>) is recaptured as the problem
of universals: do universal qualities like &ldquo;humanity&rdquo; or
the idea of a horse exist apart from the individual entities that
instantiate them? It is in the context of his rejection of universals
that Ockham (c. 1287&ndash;1347 CE) introduces his well-known razor:
entities should not be multiplied beyond necessity. Throughout their
writings Aquinas and Scotus use the Latin terms <em>informatio</em>
and <em>informare</em> in a technical sense, although this terminology
is not used by Ockham.</p>

<h3 id="ModePhil">2.3 Modern Philosophy</h3>

<p>
The history of the concept of information in modern philosophy is
complicated. Probably starting in the fourteenth century the term
&ldquo;information&rdquo; emerged in various developing European
languages in the general meaning of &ldquo;education&rdquo; and
&ldquo;inquiry&rdquo;. The French historical dictionary by Godefroy
(1881) gives <em>action de former, instruction, enqu&ecirc;te,
science, talent</em> as early meanings of &ldquo;information&rdquo;.
The term was also used explicitly for legal inquiries
(<em>Dictionnaire du Moyen Fran&ccedil;ais (1330&ndash;1500)</em>
2015). Because of this colloquial use the term
&ldquo;information&rdquo; loses its association with the concept of
&ldquo;form&rdquo; gradually and appears less and less in a formal
sense in philosophical texts.</p>

<p>
At the end of the Middle Ages society and science are changing
fundamentally (Hazard 1935; Ong 1958; Dijksterhuis 1986). In a long
complex process the Aristotelian methodology of the four causes was
transformed to serve the needs of experimental science:</p>

<ol>

<li>The Material Cause developed in to the modern notion of
matter.</li>

<li>The Formal Cause was reinterpreted as geometric form in
space.</li>

<li>The Efficient Cause was redefined as direct mechanical interaction
between material bodies.</li>

<li>The Final Cause was dismissed as unscientific. Because of this,
Newton&rsquo;s contemporaries had difficulty with the concept of the
force of gravity in his theory. Gravity as action at a distance seemed
to be a reintroduction of final causes.</li>
</ol>

<p>
In this changing context the analogy of the wax-impression is
reinterpreted. A proto-version of the modern concept of information as
the structure of a set or sequence of simple ideas is developed by the
empiricists, but since the technical meaning of the term
&ldquo;information&rdquo; is lost, this theory of knowledge is never
identified as a new &ldquo;theory of information&rdquo;.</p>

<p>
The consequence of this shift in methodology is that only phenomena
that can be explained in terms of mechanical interaction between
material bodies can be studied scientifically. This implies in a
modern sense: the reduction of intensive properties to measurable
extensive properties. For Galileo this insight is programmatic: </p>

<blockquote>

<p>
To excite in us tastes, odors, and sounds I believe that nothing is
required in external bodies except shapes, numbers, and slow or rapid
movements. (Galileo 1623 [1960: 276)
</p>
</blockquote>

<p>
These insights later led to the doctrine of the difference between
primary qualities (space, shape, velocity) and secondary qualities
(heat, taste, color etc.). In the context of philosophy of information
Galileo&rsquo;s observations on the secondary quality of
&ldquo;heat&rdquo; is of particular importance since they lay the
foundations for the study of thermodynamics in the nineteenth century:
</p>

<blockquote>

<p>
Having shown that many sensations which are supposed to be qualities
residing in external objects have no real existence save in us, and
outside ourselves are mere names, I now say that I am inclined to
believe heat to be of this character. Those materials which produce
heat in us and make us feel warmth, which are known by the general
name of &ldquo;fire,&rdquo; would then be a multitude of minute
particles having certain shapes and moving with certain velocities.
(Galileo 1623 [1960: 277)</p>
</blockquote>

<p>
A pivotal thinker in this transformation is Ren&eacute; Descartes
(1596&ndash;1650 CE). In his <em>Meditationes</em>, after
&ldquo;proving&rdquo; that the matter (<em>res extensa</em>) and mind
(<em>res cogitans</em>) are different substances (i.e., forms of being
existing independently), the question of the interaction between these
substances becomes an issue. The malleability of wax is for Descartes
an explicit argument against influence of the <em>res extensa</em> on
the <em>res cogitans</em> (<em>Meditationes</em> II, 15). The fact
that a piece of wax loses its form and other qualities easily when
heated, implies that the senses are not adequate for the
identification of objects in the world. True knowledge thus can only
be reached via &ldquo;inspection of the mind&rdquo;. Here the wax
metaphor that for more than 1500 years was used to <em>explain</em>
sensory impression is used to argue <em>against</em> the possibility
to reach knowledge via the senses. Since the essence of the <em>res
extensa</em> is extension, thinking fundamentally can not be
understood as a spatial process. Descartes still uses the terms
&ldquo;form&rdquo; and &ldquo;idea&rdquo; in the original scholastic
non-geometric (atemporal, aspatial) sense. An example is the short
formal proof of God&rsquo;s existence in the second answer to Mersenne
in the <em>Meditationes de Prima Philosophia</em></p>

<blockquote>

<p>
I use the term idea to refer to the <em>form</em> of any given
thought, immediate perception of which makes me aware of the thought.
<br />
(<em>Idea nomine intelligo cujuslibet cogitationis <em>formam</em>
illam, per cujus immediatam perceptionem ipsius ejusdem cogitationis
conscious sum</em>) </p>
</blockquote>

<p>
I call them &ldquo;ideas&rdquo; says Descartes </p>

<blockquote>

<p>
only in so far as they make a difference to the mind itself when they
<em>inform</em> that part of the brain.
<br />
(<em>sed tantum quatenus mentem ipsam in illam cerebri partem
conversam <strong>informant</strong></em>). (Descartes, 1641, <em>Ad
Secundas Objections, Rationes, Dei existentiam &amp; anime
distinctionem probantes, more Geometrico dispositae.</em>)</p>
</blockquote>

<p>
Because the <em>res extensa</em> and the <em>res cogitans</em> are
different substances, the act of thinking can never be emulated in
space: machines can not have the universal faculty of reason.
Descartes gives two separate motivations:</p>

<blockquote>

<p>
Of these the first is that they could never use words or other signs
arranged in such a manner as is competent to us in order to declare
our thoughts to others: (&hellip;) The second test is, that although
such machines might execute many things with equal or perhaps greater
perfection than any of us, they would, without doubt, fail in certain
others from which it could be discovered that they did not act from
knowledge, but solely from the disposition of their organs: for while
reason is an universal instrument that is alike available on every
occasion, these organs, on the contrary, need a particular arrangement
for each particular action; whence it must be morally impossible that
there should exist in any machine a diversity of organs sufficient to
enable it to act in all the occurrences of life, in the way in which
our reason enables us to act. (<em>Discourse de la
m&eacute;thode,</em> 1647)</p>
</blockquote>

<p>
The passage is relevant since it directly argues against the
possibility of artificial intelligence and it even might be
interpreted as arguing against the possibility of a universal Turing
machine: reason as a universal instrument can never be emulated in
space. This conception is in opposition to the modern concept of
information which as a measurable quantity is essentially spatial,
i.e., extensive (but in a sense different from that of Descartes).</p>

<p>
Descartes does not present a new interpretation of the notions of form
and idea, but he sets the stage for a debate about the nature of ideas
that evolves around two opposite positions:</p>

<div class="indent">
<p><strong>Rationalism:</strong> The Cartesian notion that ideas are
innate and thus <em>a priori</em>. This form of rationalism implies an
interpretation of the notion of ideas and forms as atemporal,
aspatial, but complex structures i.e., the idea of &ldquo;a
horse&rdquo; (i.e., with a head, body and legs). It also matches well
with the interpretation of the knowing subject as a created being
(<em>ens creatu</em>). God created man after his own image and thus
provided the human mind with an adequate set of ideas to understand
his creation. In this theory growth, of knowledge is <em>a priori</em>
limited. Creation of new ideas <em>ex nihilo</em> is impossible. This
view is difficult to reconcile with the concept of experimental
science.</p>

<p><strong>Empiricism:</strong> Concepts are constructed in the
mind <em>a posteriori</em> on the basis of ideas associated with
sensory impressions. This doctrine implies a new interpretation of the
concept of idea as:</p>

<blockquote>

<p>
whatsoever is the object of understanding when a man thinks &hellip;
whatever is meant by phantasm, notion, species, or whatever it is
which the mind can be employed about when thinking. (Locke 1689, bk I,
ch 1, para 8)</p>
</blockquote>

 
<p>
Here ideas are conceived as elementary building blocks
of human knowledge and reflection. This fits well with the demands of
experimental science. The downside is that the mind can never
formulate apodeictic truths about cause and effects and the essence of
observed entities, including its own identity. Human knowledge becomes
essentially probabilistic (Locke 1689: bk I, ch. 4, para 25).</p>

</div>

<p>
Locke&rsquo;s reinterpretation of the notion of idea as a
&ldquo;structural placeholder&rdquo; for any entity present in the
mind is an essential step in the emergence of the modern concept of
information. Since these ideas are not involved in the justification
of apodeictic knowledge, the necessity to stress the atemporal and
aspatial nature of ideas vanishes. The construction of concepts on the
basis of a <em>collection of elementary ideas</em> based in sensorial
experience opens the gate to a reconstruction of <em>knowledge as an
extensive property of an agent</em>: more ideas implies more probable
knowledge.</p>

<p>
In the second half of the seventeenth century formal theory of
probability is developed by researchers like Pascal (1623&ndash;1662),
Fermat (1601 or 1606&ndash;1665) and Christiaan Huygens
(1629&ndash;1695). The work <em>De ratiociniis in ludo aleae</em> of
Huygens was translated in to English by John Arbuthnot (1692). For
these authors, the world was essentially mechanistic and thus
deterministic, probability was a quality of human knowledge caused by
its imperfection:</p>

<blockquote>

<p>
It is impossible for a Die, with such determin&rsquo;d force and
direction, not to fall on such determin&rsquo;d side, only I
don&rsquo;t know the force and direction which makes it fall on such
determin&rsquo;d side, and therefore I call it Chance, wich is nothing
but the want of art;&hellip; (John Arbuthnot <em>Of the Laws of
Chance</em> (1692), preface) </p>
</blockquote>

<p>
This text probably influenced Hume, who was the first to marry formal
probability theory with theory of knowledge:</p>

<blockquote>

<p>
Though there be no such thing as Chance in the world; our ignorance of
the real cause of any event has the same influence on the
understanding, and begets a like species of belief or opinion.
(&hellip;) If a dye were marked with one figure or number of spots on
four sides, and with another figure or number of spots on the two
remaining sides, it would be more probable, that the former would turn
up than the latter; though, if it had a thousand sides marked in the
same manner, and only one side different, the probability would be
much higher, and our belief or expectation of the event more steady
and secure. This process of the thought or reasoning may seem trivial
and obvious; but to those who consider it more narrowly, it may,
perhaps, afford matter for curious speculation. (Hume 1748: Section
VI, &ldquo;On probability&rdquo; 1)</p>
</blockquote>

<p>
Here knowledge about the future as a degree of belief is measured in
terms of probability, which in its turn is explained in terms of the
number of configurations a deterministic system in the world can have.
The basic building blocks of a modern theory of information are in
place. With this new concept of knowledge empiricists laid the
foundation for the later development of thermodynamics as a reduction
of the secondary quality of heat to the primary qualities of
bodies.</p>

<p>
At the same time the term &ldquo;information&rdquo; seems to have lost
much of its technical meaning in the writings of the empiricists so
this new development is not designated as a new interpretation of the
notion of &ldquo;information&rdquo;. Locke sometimes uses the phrase
that our senses &ldquo;inform&rdquo; us about the world and
occasionally uses the word &ldquo;information&rdquo;. </p>

<blockquote>

<p>
For what information, what knowledge, carries this proposition in it,
viz. &ldquo;Lead is a metal&rdquo; to a man who knows the complex idea
the name lead stands for? (Locke 1689: bk IV, ch 8, para 4) </p>
</blockquote>

<p>
Hume seems to use information in the same casual way when he observes:
</p>

<blockquote>

<p>
Two objects, though perfectly resembling each other, and even
appearing in the same place at different times, may be numerically
different: And as the power, by which one object produces another, is
never discoverable merely from their idea, it is evident cause and
effect are relations, of which we receive information from experience,
and not from any abstract reasoning or reflection. (Hume 1739: Part
III, section 1)</p>
</blockquote>

<p>
The empiricists methodology is not without problems. The biggest issue
is that all knowledge becomes probabilistic and <em>a posteriori</em>.
Immanuel Kant (1724&ndash;1804) was one of the first to point out that
the human mind has a grasp of the meta-concepts of space, time and
causality that itself can never be understood as the result of a mere
combination of &ldquo;ideas&rdquo;. What is more, these intuitions
allow us to formulate scientific insights with certainty: i.e., the
fact that the sum of the angles of a triangle in Euclidean space is
180 degrees. This issue cannot be explained in the empirical
framework. If knowledge is created by means of combination of ideas
then there must exist an <em>a priori</em> synthesis of ideas in the
human mind. According to Kant, this implies that the human mind can
evaluate its own capability to formulate scientific judgments. In his
<em>Kritik der reinen Vernunft</em> (1781) Kant developed
transcendental philosophy as an investigation of the necessary
conditions of human knowledge. Although Kant&rsquo;s transcendental
program did not contribute directly to the development of the concept
of information, he did influence research in to the foundations of
mathematics and knowledge relevant for this subject in the nineteenth
and twentieth century: e.g., the work of Frege, Husserl, Russell,
Brouwer, L. Wittgenstein, G&ouml;del, Carnap, Popper and Quine.</p>

<h3 id="HistDeveMeanTermInfo">2.4 Historical Development of the Meaning of the Term &ldquo;Information&rdquo;</h3>

<p>
The history of the term &ldquo;information&rdquo; is intricately
related to the study of central problems in epistemology and ontology
in Western philosophy. After a start as a technical term in classical
and medieval texts the term &ldquo;information&rdquo; almost vanished
from the philosophical discourse in modern philosophy, but gained
popularity in colloquial speech. Gradually the term obtained the
status of an abstract mass-noun, a meaning that is orthogonal to the
classical process-oriented meaning. In this form it was picked up by
several researchers (Fisher 1925; Shannon 1948) in the twentieth
century who introduced formal methods to measure
&ldquo;information&rdquo;. This, in its turn, lead to a revival of the
philosophical interest in the concept of information. This complex
history seems to be one of the main reasons for the difficulties in
formulating a definition of a unified concept of information that
satisfies all our intuitions. At least three different meanings of the
word &ldquo;information&rdquo; are historically relevant:</p>

<div class="indent">
<p><strong>&ldquo;Information&rdquo; as the process of being
informed.</strong><br />
This is the oldest meaning one finds in the writings of authors
like Cicero (106&ndash;43 BCE) and Augustine (354&ndash;430 CE) and it
is lost in the modern discourse, although the association of
information with processes (i.e., computing, flowing or sending a
message) still exists. In classical philosophy one could say that when
I recognize a horse as such, then the &ldquo;form&rdquo; of a horse is
planted in my mind. This process is my &ldquo;information&rdquo; of
the nature of the horse. Also the act of teaching could be referred to
as the &ldquo;information&rdquo; of a pupil. In the same sense one
could say that a sculptor creates a sculpture by
&ldquo;informing&rdquo; a piece of marble. The task of the sculptor is
the &ldquo;information&rdquo; of the statue (Capurro &amp;
Hj&oslash;rland 2003). This process-oriented meaning survived quite
long in western European discourse: even in the eighteenth century
Robinson Crusoe could refer to the education of his servant Friday as
his &ldquo;information&rdquo; (Defoe 1719: 261). It is also used in
this sense by Berkeley: &ldquo;I love information upon all subjects
that come in my way, and especially upon those that are most
important&rdquo; (<em>Alciphron</em> Dialogue 1, Section 5, Paragraph
6/10, see Berkeley 1732).</p>

<p><strong>&ldquo;Information&rdquo; as a state of an
agent</strong>,<br />
i.e., as the result of the process of being informed. If one
teaches a pupil the theorem of Pythagoras then, after this process is
completed, the student can be said to &ldquo;have the information
about the theorem of Pythagoras&rdquo;. In this sense the term
&ldquo;information&rdquo; is the result of the same suspect form of
substantiation of a verb (<em>informare</em> \(\gt\)
<em>informatio</em>) as many other technical terms in philosophy
(substance, consciousness, subject, object). This sort of
term-formation is notorious for the conceptual difficulties it
generates. Can one derive the fact that I &ldquo;have&rdquo;
consciousness from the fact that I am conscious? Can one derive the
fact that I &ldquo;have&rdquo; information from the fact that I have
been informed? The transformation to this modern substantiated meaning
seems to have been gradual and seems to have been general in Western
Europe at least from the middle of the fifteenth century. In the
renaissance a scholar could be referred to as &ldquo;a man of
information&rdquo;, much in the same way as we now could say that
someone received an education (Adriaans &amp; van Benthem 2008b;
Capurro &amp; Hj&oslash;rland 2003). In &ldquo;Emma&rdquo; by Jane
Austen one can read: &ldquo;Mr. Martin, I suppose, is not a man of
information beyond the line of his own business. He does not
read&rdquo; (Austen 1815: 21).</p>

<p><strong>&ldquo;Information&rdquo; as the disposition to
inform</strong>,<br />
i.e., as a capacity of an object to inform an agent. When the act
of teaching me Pythagoras&rsquo; theorem leaves me with information
about this theorem, it is only natural to assume that a text in which
the theorem is explained actually &ldquo;contains&rdquo; this
information. The text has the capacity to inform me when I read it. In
the same sense, when I have received information from a teacher, I am
capable of transmitting this information to another student. Thus
information becomes something that can be stored and measured. This
last concept of information as an abstract mass-noun has gathered wide
acceptance in modern society and has found its definitive form in the
nineteenth century, allowing Sherlock Homes to make the following
observation: &ldquo;&hellip; friend Lestrade held information in his
hands the value of which he did not himself know&rdquo; (&ldquo;The
Adventure of the Noble Bachelor&rdquo;, Conan Doyle 1892). The
association with the technical philosophical notions like
&ldquo;form&rdquo; and &ldquo;informing&rdquo; has vanished from the
general consciousness although the association between information and
processes like storing, gathering, computing and teaching still
exist.</p>

</div>

<h2 id="BuilBlocModeTheoInfo">3. Building Blocks of Modern Theories of Information</h2>

<p>
With hindsight many notions that have to do with optimal code systems,
ideal languages and the association between computing and processing
language have been recurrent themes in the philosophical reflection
since the seventeenth century.</p>

<h3 id="Lang">3.1 Languages</h3>

<p>
One of the most elaborate proposals for a universal
&ldquo;philosophical&rdquo; language was made by bishop John Wilkins (Maat 2004):
&ldquo;An Essay towards a Real Character, and a Philosophical
Language&rdquo; (1668). Wilkins&rsquo; project consisted of an
elaborate system of symbols that supposedly were associated with
unambiguous concepts in reality. Proposals such as these made
philosophers sensitive to the deep connections between language and
thought. The empiricist methodology made it possible to conceive the
development of language as a system of conventional signs in terms of
associations between ideas in the human mind. The issue that currently
is known as the <em>symbol grounding problem</em> (how do arbitrary
signs acquire their inter-subjective meaning) was one of the most
heavily debated questions in the eighteenth century in the context of
the problem of the origin of languages. Diverse thinkers as Vico,
Condillac, Rousseau, Diderot, Herder and Haman made contributions. The
central question was whether language was given <em>a priori</em> (by
God) or whether it was constructed and hence an invention of man
himself. Typical was the contest issued by the Royal Prussian Academy
of Sciences in 1769:</p>

<blockquote>

<p>
<em>En supposant les hommes abandonn&eacute;s &agrave; leurs
facult&eacute;s naturelles, sont-ils en &eacute;tat d&rsquo;inventer
le langage? Et par quels moyens parviendront-ils
d&rsquo;eux-m&ecirc;mes &agrave; cette invention?</em> </p>

<p>
Assuming men abandoned to their natural faculties, are they able to
invent language and by what means will they come to this
 invention?<sup>[<a href="notes.html#note-1" id="ref-1">1</a>]</sup></p>
 </blockquote>

<p>
The controversy raged on for over a century without any conclusion and
in 1866 the Linguistic Society of Paris (<em>Soci&eacute;t&eacute; de
Linguistique de Paris</em>) banished the issue from its arena.
 <sup>[<a href="notes.html#note-2" id="ref-2">2</a>]</sup></p>
 
<p>
Philosophically more relevant is the work of Leibniz (1646&ndash;1716)
on a so-called <em>characteristica universalis</em>: the notion of a
universal logical calculus that would be the perfect vehicle for
scientific reasoning. A central presupposition in Leibniz&rsquo;
philosophy is that such a perfect language of science is in principle
possible because of the perfect nature of the world as God&rsquo;s
creation (<em>ratio essendi</em> = <em>ration cognoscendi,</em> the
origin of being is the origin of knowing). This principle was rejected
by Wolff (1679&ndash;1754) who suggested more heuristically oriented
<em>characteristica combinatoria</em> (van Peursen 1987). These ideas
had to wait for thinkers like Boole (1854, <em>An Investigation of the
Laws of Thought</em>), Frege (1879, <em>Begriffsschrift</em>), Peirce
(who in 1886 already suggested that electrical circuits could be used
to process logical operations) and Whitehead and Russell
(1910&ndash;1913, <em>Principia Mathematica</em>) to find a more
fruitful treatment.</p>

<h3 id="OptiCode">3.2 Optimal Codes</h3>

<p>
The fact that frequencies of letters vary in a language was known
since the invention of book printing. Printers needed many more
&ldquo;e&rdquo;s and &ldquo;t&rdquo;s than &ldquo;x&rdquo;s or
&ldquo;q&rdquo;s to typeset an English text. This knowledge was used
extensively to decode ciphers since the seventeenth century (Kahn
1967; Singh 1999). In 1844 an assistant of Samuel Morse, Alfred Vail,
determined the frequency of letters used in a local newspaper in
Morristown, New Jersey, and used them to optimize Morse code. Thus the
core of theory of optimal codes was already established long before
Shannon developed its mathematical foundation (Shannon 1948; Shannon
&amp; Weaver 1949). Historically important but philosophically less
relevant are the efforts of Charles Babbage to construct computing
machines (Difference Engine in 1821, and the Analytical Engine
1834&ndash;1871) and the attempt of Ada Lovelace (1815&ndash;1852) to
design what is considered to be the first programming language for the
Analytical Engine.</p>

<h3 id="Numb">3.3 Numbers</h3>

<p>
The simplest way of representing numbers is via a <em>unary
system</em>. Here the length of the representation of a number is
equal to the size of the number itself, i.e., the number
&ldquo;ten&rdquo; is represented as &ldquo;\\\\\\\\\\&rdquo;. The
classical Roman number system is an improvement since it contains
different symbols for different orders of magnitude (one = I, ten = X,
hundred = C, thousand = M). This system has enormous drawbacks since
in principle one needs an infinite amount of symbols to code the
natural numbers and because of this the same mathematical operations
(adding, multiplication etc.) take different forms at different orders
of magnitude. Around 500 CE the number zero was invented in India.
Using zero as a placeholder we can code an infinity of numbers with a
finite set of symbols (one = I, ten = 10, hundred = 100, thousand =
1000 etc.). From a modern perspective an infinite number of position
systems is possible as long as we have 0 as a placeholder and a finite
number of other symbols. Our normal decimal number system has ten
digits &ldquo;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&rdquo; and represents the
number two-hundred-and-fifty-five as &ldquo;255&rdquo;. In a binary
number system we only have the symbols &ldquo;0&rdquo; and
&ldquo;1&rdquo;. Here two-hundred-and-fifty-five is represented as
&ldquo;11111111&rdquo;. In a hexadecimal system with 16 symbols (0, 1,
2, 3, 4, 5, 6, 7, 8, 9, a, b, c, d, e, f) the same number can be
written as &ldquo;ff&rdquo;. Note that the length of these
representations differs considerable. Using this representation,
mathematical operations can be standardized irrespective of the order
of magnitude of numbers we are dealing with, i.e., the possibility of
a uniform algorithmic treatment of mathematical functions (addition,
subtraction, multiplication and division etc.) is associated with such
a position system.</p>

<p>
The concept of a positional number system was brought to Europe by the
Persian mathematician al-Khwarizmi (ca. 780&ndash;ca. 850 CE). His
main work on numbers (ca. 820 CE) was translated into Latin as
<em>Liber Algebrae et Almucabola</em> in the twelfth century, which
gave us amongst other things the term &ldquo;algebra&rdquo;. Our word
&ldquo;algorithm&rdquo; is derived from <em>Algoritmi</em>, the Latin
form of his name. Positional number systems simplified commercial and
scientific calculations.</p>

<p>
In 1544 Michael Stifel introduced the concept of the exponent of a
number in <em>Arithmetica integra</em> (1544). Thus 8 can be written
as \(2^3\) and 25 as \(5^2\). The notion of an exponent immediately
suggests the notion of a logarithm as its inverse function: \(\log_b
b^a) = a\). Stifel compared the arithmetic sequence:</p> 

\[
-3, -2, -1, 0, 1, 2, 3
\]

<p>
in which the term 1 have a difference of 1 with the geometric
sequence:</p> 

\[
\frac{1}{8}, \frac{1}{4}, \frac{1}{2} , 1, 2, 4, 8
\]

<p>
in which the terms have a ratio of 2. The exponent notation allowed
him to rewrite the values of the second table as:</p> 

\[
2^{-3}, 2^{-2}, 2^{-1}, 2^0 , 2^1 , 2^2, 2^3
\]

<p>
which combines the two tables. This arguably was the first logarithmic
table. A more definitive and practical theory of logarithms is
developed by John Napier (1550&ndash;1617) in his main work (Napier
1614). He coined the term logarithm (logos + arithmetic: ratio of
numbers). As is clear from the match between arithmetic and geometric
progressions, logarithms reduce products to sums:</p> 

\[
\log_b (xy) = \log_b (x) + \log_b (y)
\]

<p>
They also reduce divisions to differences:</p> 

\[
\log_b (x/y) = \log_b (x) - \log_b (y)
\]

<p>
and powers to products: </p> 

\[
\log_b (x^p) = p \log_b (x)
\]

<p>
After publication of the logarithmic tables by Briggs (1624) this new
technique of facilitating complex calculations rapidly gained
popularity.</p>

<h3 id="Phys">3.4 Physics</h3>

<p>
Galileo (1623) already had suggested that the analysis of phenomena
like heat and pressure could be reduced to the study of movements of
elementary particles. Within the empirical methodology this could be
conceived as the question how the sensory experience of the secondary
quality of heat of an object or a gas could be reduced to movements of
particles. Bernoulli (<em>Hydrodynamica</em> published in 1738) was
the first to develop a kinetic theory of gases in which
macroscopically observable phenomena are described in terms of
microstates of systems of particles that obey the laws of Newtonian
mechanics, but it was quite an intellectual effort to come up with an
adequate mathematical treatment. Clausius (1850) made a conclusive
step when he introduced the notion of the mean free path of a particle
between two collisions. This opened the way for a statistical
treatment by Maxwell who formulated his distribution in 1857, which
was the first statistical law in physics. The definitive formula that
tied all notions together (and that is engraved on his tombstone,
though the actual formula is due to Planck) was developed by
Boltzmann:</p> 

\[
S = k \log W
\]

<p>
It describes the entropy <i>S</i> of a system in terms of the
logarithm of the number of possible microstates <i>W</i>, consistent
with the observable macroscopic states of the system, where <i>k</i>
is the well-known Boltzmann constant. In all its simplicity the value
of this formula for modern science can hardly be overestimated. The
expression &ldquo;\(\log W\)&rdquo; can, from the perspective of
information theory, be interpreted in various ways:</p>

<ul class="jfy">

<li>As the amount of <em>entropy</em> in the system.</li>

<li>As the length of the <em>number</em> needed to count all possible
microstates consistent with macroscopic observations.</li>

<li>As the length of an optimal <em>index</em> we need to identify the
specific current unknown microstate of the system, i.e., it is a
measure of our &ldquo;lack of information&rdquo;.</li>

<li>As a measure for the <em>probability</em> of any typical specific
microstate of the system consistent with macroscopic
observations.</li>
</ul>

<p>
Thus it connects the additive nature of logarithm with the extensive
qualities of entropy, probability, typicality and information and it
is a fundamental step in the use of mathematics to analyze nature.
Later Gibbs (1906) refined the formula:</p> 

\[
S = -\sum_i p_i \ln p_i,
\]

<p>
where \(p_i\) is the probability that the system is in the
\(i^{\textrm{th}}\) microstate. This formula was adopted by Shannon
(1948; Shannon &amp; Weaver 1949) to characterize the communication
entropy of a system of messages. Although there is a close connection
between the mathematical treatment of entropy and information, the
exact interpretation of this fact has been a source of controversy
ever since (Harremo&euml;s &amp; Tops&oslash;e 2008; Bais &amp; Farmer
2008).</p>

<h2 id="DevePhilInfo">4. Developments in Philosophy of Information</h2>

<p>
The modern theories of information emerged in the middle of the
twentieth century in a specific intellectual climate in which the
distance between the sciences and parts of academic philosophy was
quite big. Some philosophers displayed a specific anti-scientific
attitude: Heidegger, &ldquo;<em>Die Wissenschaft denkt
nicht.</em>&rdquo; On the other hand the philosophers from the Wiener
Kreis overtly discredited traditional philosophy as dealing with
illusionary problems (Carnap 1928). The research program of logical
positivism was a rigorous reconstruction of philosophy based on a
combination of empiricism and the recent advances in logic. It is
perhaps because of this intellectual climate that early important
developments in the theory of information took place in isolation from
mainstream philosophical reflection. A landmark is the work of Dretske
in the early eighties (Dretske 1981). Since the turn of the century,
interest in Philosophy of Information has grown considerably, largely
under the influence of the work of Luciano Floridi on semantic
information. Also the rapid theoretical development of quantum
computing and the associated notion of quantum information have had it
repercussions on philosophical reflection.</p>

<h3 id="PoppInfoDegrFals">4.1 Popper: Information as Degree of Falsifiability</h3>

<p>
The research program of logical positivism of the Wiener Kreis in the
first half of the twentieth century revitalized the older project of
empiricism. Its ambition was to reconstruct scientific knowledge on
the basis of direct observations and logical relation between
statements about those observations. The old criticism of Kant on
empiricism was revitalized by Quine (1951). Within the framework of
logical positivism induction was invalid and causation could never be
established objectively. In his <em>Logik der Forschung</em> (1934)
Popper formulates his well-known demarcation criterion and he
positions this explicitly as a solution to Hume&rsquo;s problem of
induction (Popper 1934 [1977: 42]). Scientific theories formulated as
general laws can never be verified definitively, but they can be
falsified by only one observation. This implies that a theory is
&ldquo;more&rdquo; scientific if it is richer and provides more
opportunity to be falsified:</p>

<blockquote>

<p>
Thus it can be said that the amount of empirical information conveyed
by a theory, or its <em>empirical content</em>, increases with its
degree of falsifiability. (Popper 1934 [1977: 113], emphasis in
original) </p>
</blockquote>

<p>
This quote, in the context of Popper&rsquo;s research program, shows
that the ambition to measure the <em>amount of empirical information
in scientific theory conceived as a set of logical statements</em> was
already recognized as a philosophical problem more than a decade
before Shannon formulated his theory of information. Popper is aware
of the fact that the empirical content of a theory is related to its
falsifiability and that this in its turn has a relation with the
probability of the statements in the theory. Theories with more
empirical information are less probable. Popper distinguishes
<em>logical probability</em> from <em>numerical probability</em>
(&ldquo;which is employed in the theory of games and chance, and in
statistics&rdquo;; Popper 1934 [1977: 119]). In a passage that is
programmatic for the later development of the concept of information
he defines the notion of logical probability:</p>

<blockquote>

<p>
<em>The logical probability of a statement is complementary to its
falsifiability:</em> it increases with decreasing degree of
falsifiability. The logical probability 1 corresponds to the degree 0
of falsifiability and <em>vice versa</em>. (Popper 1934 [1977: 119],
emphasis in original) </p>

<p>
It is possible to interpret numerical probability as applying to a
subsequence (picked out from the logical probability relation) for
which a <em>system of measurement</em> can be defined, on the basis of
frequency estimates. (Popper 1934 [1977: 119], emphasis in original)
</p>
</blockquote>

<p>
Popper never succeeded in formulating a good formal theory to measure
this amount of information although in later writings he suggests that
Shannon&rsquo;s theory of information might be useful (Popper 1934
[1977], 404 [Appendix IX, from 1954]).
These issues were later developed
in philosophy of science. Theory of conformation studies induction
theory and the way in which evidence &ldquo;supports&rdquo; a certain
theory (Huber 2007
 [<a href="#Oth">OIR</a>]).
 Although the work of Carnap motivated important developments in both
philosophy of science and philosophy of information the connection
between the two disciplines seems to have been lost. There is no
mention of information theory or any of the more foundational work in
philosophy of information in Kuipers (2007a), but the two disciplines
certainly have overlapping domains. (See, e.g., the discussion of the
so-called Black Ravens Paradox by Kuipers (2007b) and Rathmanner &amp;
Hutter (2011).) </p>

<h3 id="ShanInfoDefiTermProb">4.2 Shannon: Information Defined in Terms of Probability</h3>

<p>
In two landmark papers Shannon (1948; Shannon &amp; Weaver 1949)
characterized the communication entropy of a system of messages
<i>A</i>:</p> 

\[
H(P) = -\sum_{i\in A} p_i \log_2 p_i
\]

<p>
Here \(p_i\) is the probability of message <i>i</i> in <i>A</i>. This
is exactly the formula for Gibb&rsquo;s entropy in physics. The use of
base-2 logarithms ensures that the code length is measured in bits
(binary digits). It is easily seen that the communication entropy of a
system is maximal when all the messages have equal probability and
thus are typical. </p>

<p>
The amount of information <i>I</i> in an individual message <i>x</i>
is given by: </p> 

\[
I(x) = -\log p_x
\]

<p>
This formula, that can be interpreted as the inverse of the Boltzmann
entropy, covers a number of our basic intuitions about
information:</p>

<ul class="jfy">

<li>A message <i>x</i> has a certain probability \(p_x\) between 0 and
1 of occurring. </li>

<li>If \(p_x = 1\) then \(I(x) = 0\). If we are certain to get a
message it literally contains no &ldquo;news&rdquo; at al. The lower
the probability of the message is, the more information it contains. A
message like &ldquo;The sun will rise tomorrow&rdquo; seems to contain
less information than the message &ldquo;Jesus was Caesar&rdquo;
exactly because the second statement is much less likely to be
defended by anyone (although it can be found on the web).</li>

<li>If two messages <i>x</i> and <i>y</i> are unrelated then \(I(x
\textrm{ and } y)=I(x) + I(y)\). Information is <em>extensive</em>.
The amount of information in two combined messages is equal to the sum
of the amount of information in the individual messages. </li>
</ul>

<p>
Information as the negative log of the probability is the only
mathematical function that exactly fulfills these constraints (Cover
&amp; Thomas 2006). Shannon offers a theoretical framework in which
binary strings can be interpreted as words in a (programming) language
containing a certain amount of information (see
 <a href="#Lang">3.1 Languages</a>).
 The expression \(-\log p_x\) exactly gives the length of an optimal
code for message <i>x</i> and as such formalizes the old intuition
that codes are more efficient when frequent letters get shorter
representations (see
 <a href="#OptiCode">3.2 Optimal codes</a>).
 Logarithms as a reduction of multiplication to addition (see
 <a href="#Numb">3.3 Numbers</a>)
 are a natural representation of extensive properties of systems and
already as such had been used by physicists in the nineteenth century
(see
 <a href="#Phys">3.4 Physics</a>).</p>
 
<p>
One aspect of information that Shannon&rsquo;s definition explicitly
does not cover is the actual content of the messages interpreted as
propositions. So the statement &ldquo;Jesus was Caesar&rdquo; and
&ldquo;The moon is made of green cheese&rdquo; may carry the same
amount of information while their meaning is totally different. A
large part of the effort in philosophy of information has been
directed to the formulation of more semantic theories of information
(Bar-Hillel &amp; Carnap 1953; Floridi 2002, 2003, 2011). Although
Shannon&rsquo;s proposals at first were almost completely ignored by
philosophers it has in the past decennia become apparent that their
impact on philosophical issues is big. Dretske (1981) was one of the
first to analyze the philosophical implications of Shannon&rsquo;s
theory, but the exact relation between various systems of logic and
theory of information are still unclear (see
<a href="#LogiSemaInfo">6.6 Logic and Semantic Information</a>). </p>

<h3 id="SoloKolmChaiInfoLengProg">4.3 Solomonoff, Kolmogorov, Chaitin: Information as the Length of a Program</h3>

<p>
This problem of relating a set of statements to a set of observations
and defining the corresponding probability was taken up by Carnap
(1945, 1950). He distinguished two forms of probability:
Probability\(_1\) or &ldquo;degree of confirmation&rdquo; \(P_1 (h ;
e)\) is a <em>logical</em> relation between two sentences, a
hypothesis <i>h</i> and a sentence <i>e</i> reporting a series of
observations. Statements of this type are either analytical or
contradictory. The second form, Probability\(_2\) or &ldquo;relative
frequency&rdquo;, is the statistical concept. In the words of his
student Solomonoff (1997):</p>

<blockquote>

<p>
Carnap&rsquo;s model of probability started with a long sequence of
symbols that was a description of the entire universe. Through his own
formal linguistic analysis, he was able to assign <em>a priori</em>
probabilities to any possible string of symbols that might represent
the universe.</p>
</blockquote>

<p>
The method for assigning probabilities Carnap used, was not universal
and depended heavily on the code systems used. A general theory of
induction using Bayes&rsquo; rule can only be developed when we can
assign a universal probability to &ldquo;any possible string&rdquo; of
symbols. In a paper in 1960 Solomonoff (1960, 1964a,b) was the first
to sketch an outline of a solution for this problem. He formulated the
notion of what is now called a <em>universal probability
distribution</em>: consider the set of all possible finite strings to
be programs for a universal Turing machine <i>U</i> and define the
probability of a string <i>x</i> of symbols in terms of the length of
the shortest program <i>p</i> that outputs <i>x</i> on <i>U</i>.</p>


<p>
This notion of Algorithmic Information Theory was invented
independently somewhat later separately by Kolmogorov (1965) and
Chaitin (1969). Levin (1974) developed a mathematical expression of
the universal <em>a priori</em> probability as a universal (that is,
maximal) lower semicomputable semimeasure <i>M</i>, and showed that
the negative logarithm of \(M(x)\) coincides with the Kolmogorov
complexity of <i>x</i> up to an additive logarithmic term. The actual
definition of the complexity measure is: </p>

<div class="indent">

<p>
<strong>Kolmogorov complexity</strong> The algorithmic complexity of a
string <i>x</i> is the length \(\cal{l}(p)\) of the smallest program
<i>p</i> that produces <i>x</i> when it runs on a universal Turing
machine <i>U</i>, noted as \(U(p)=x\): </p> 

\[K(x):=\min_p \{l(p), U(p)=x\}\]

</div>

<p>
Algorithmic Information Theory (a.k.a. Kolmogorov complexity theory)
has developed into a rich field of research with a wide range of
domains of applications many of which are philosophically relevant (Li
&amp; Vit&aacute;nyi 2019):</p>

<ul class="jfy">

<li>It provides us with a general theory of induction. The use of
Bayes&rsquo; rule allows for a modern reformulation of Ockham&rsquo;s
razor in terms of Minimum Description Length (Rissanen 1978, 1989;
Barron, Rissanen, &amp; Yu 1998; Gr&uuml;nwald 2007) and minimum
message length (Wallace 2005). Note that Domingos (1998) has argued
against the general validity of these principles.</li>

<li>It allows us to formulate probabilities and information content
for individual objects. Even individual natural numbers.</li>

<li>It lays the foundation for a theory of learning as data
compression (Adriaans 2007).</li>

<li>It gives a definition of randomness of a string in terms of
incompressibility. This in itself has led to a whole new domain of
research (Niess 2009; Downey &amp; Hirschfeld 2010).</li>

<li>It allows us to formulate an objective <em>a priori</em> measure
of the predictive value of a theory in terms of its randomness
deficiency: i.e., the best theory is the shortest theory that makes
the data look random conditional to the theory. (Vereshchagin &amp;
Vit&aacute;nyi 2004).</li>
</ul>

<p>
There are also down-sides:</p>

<ul class="jfy">

<li>Algorithmic complexity is uncomputable, although it can in a lot
of practical cases be approximated and commercial compression programs
in some cases come close to the theoretical optimum (Cilibrasi &amp;
Vit&aacute;nyi 2005).</li>

<li>Algorithmic complexity is an asymptotic measure (i.e., it gives a
value that is correct up to a constant). In some cases the value of
this constant is prohibitive for use in practical purposes.</li>

<li>Although the shortest theory is always the best one in terms of
randomness deficiency, incremental compression of data-sets is in
general not a good learning strategy since the randomness deficiency
does not decrease monotonically with the compression rate (Adriaans
&amp; Vit&aacute;nyi 2009).</li>

<li>The generality of the definitions provided by Algorithmic
Information Theory depends on the generality of the concept of a
universal Turing machine and thus ultimately on the interpretation of
the Church-Turing-Thesis.</li>

<li>The Kolmogorov complexity of an object does not take in to account
the amount of time it takes to actually compute the object. In this
context Levin proposed a variant of Kolmogorov complexity that
penalizes the computation time (Levin 1973, 1984):

<div class="indent">

<p>
<strong>Levin complexity</strong> The Levin complexity of a string
<i>x</i> is the sum of the length \(\cal{l}(p)\) and the logarithm of
the computation time of the smallest program <i>p</i> that produces
<i>x</i> when it runs on a universal Turing machine <i>U</i>, noted as
\(U(p)=x\): </p> 

\[Kt(x):=\min_p \{l(p) + \log(time(p)), U(p)=x\}\]

</div> </li>
</ul>

<p>
Algorithmic Information Theory has gained rapid acceptance as a
fundamental theory of information. The well-known introduction in
<em>Information Theory</em> by Cover and Thomas (2006) states:
&ldquo;&hellip; we consider Kolmogorov complexity (i.e., AIT) to be
more fundamental than Shannon entropy&rdquo; (2006: 3).</p>

<p>
The idea that algorithmic complexity theory is a foundation for a
general theory of artificial intelligence (and theory of knowledge)
has already been suggested by Solomonoff (1997) and Chaitin (1987).
Several authors have defended that data compression is a general
principle that governs human cognition (Chater &amp; Vit&aacute;nyi
2003; Wolff 2006). Hutter (2005, 2007a,b) argues that Solomonoff&rsquo;s formal and
complete theory essentially solves the induction problem. Hutter
(2007a) and Rathmanner &amp; Hutter (2011) enumerate a plethora of
classical philosophical and statistical problems around induction and
claim that Solomonoff&rsquo;s theory solves or avoids all these
problems. Probably because of its technical nature, the theory has
been largely ignored by the philosophical community. Yet, it stands
out as one of the most fundamental contributions to information theory
in the twentieth century and it is clearly relevant for a number of
philosophical issues, such as the problem of induction.</p>

<h2 id="SystCons">5. Systematic Considerations</h2>

<p>
In a mathematical sense information is associated with measuring
extensive properties of classes of systems with finite but unlimited
dimensions (systems of particles, texts, codes, networks, graphs,
games etc.). This suggests that a uniform treatment of various
theories of information is possible. In the Handbook of Philosophy of
Information three different forms of information are distinguished
(Adriaans &amp; van Benthem 2008b):</p>

<div class="indent">
<p><strong>Information-A:</strong><br />
Knowledge, logic, what is conveyed in informative answers</p>

<p><strong>Information-B:</strong><br />
Probabilistic, information-theoretic, measured quantitatively</p>

<p><strong>Information-C:</strong><br />
Algorithmic, code compression, measured quantitatively</p>
</div>

<p>
Because of recent development the connections between Information-B
(Shannon) and Information-C (Kolmogorov) are reasonably well
understood (Cover &amp; Thomas 2006). The historical material
presented in this article suggests that reflection on Information-A
(logic, knowledge) is historically much more interwoven than was
generally known up till now. The research program of logical
positivism can with hindsight be characterized as the attempt to marry
a possible worlds interpretation of logic with probabilistic reasoning
(Carnap 1945, 1950; Popper 1934; for a recent approach see Hutter et
al. 2013). Modern attempt to design a Bayesian epistemology (Bovens
&amp; Hartmann 2003) do not seem to be aware of the work done in the
first half of the twentieth century. However, an attempt to unify
Information-A and Information-B seems a viable exercise. Also the
connection between thermodynamics and information theory have become
much closer, amongst others, due to the work of Gell-Mann &amp; Lloyd
(2003) (see also: Bais and Farmer 2008). Verlinde (2011, 2017) even
presented a reduction of gravity to information (see the entry on
 <a href="../information-entropy/index.html">information processing and thermodynamic entropy</a>).
 </p>

<h3 id="PhilInfoExtePhilMath">5.1 Philosophy of Information as An Extension of Philosophy of Mathematics</h3>

<p>
With respect to the main definitions of the concept of information,
like Shannon Information, Kolmogorov complexity, semantic information
and quantum information, a unifying approach to a philosophy of
information is possible, when we interpret it as an extension to the
philosophy of mathematics. The answer to questions like &ldquo;What is
data?&rdquo; and &ldquo;What is information?&rdquo; then evolves from
one&rsquo;s answer to the related questions like &ldquo;What is a
set?&rdquo; and &ldquo;What is a number?&rdquo; With hindsight one can
observe that many open problems in the philosophy of mathematics
revolve around the notion of information. </p>

<p>
If we look at the foundations of information and computation there are
two notions that are crucial: the concept of a data set and the
concept of an algorithm. Once we accept these notions as fundamental
the rest of the theory data and computation unfolds quite naturally.
One can &ldquo;plug in&rdquo; one&rsquo;s favorite epistemological or
metaphysical stance here, but this does not really affect foundational
issues in the philosophy of computation and information. One might
sustain a Formalist, Platonic or intuitionistic view of the
mathematical universe (see entry on
 <a href="../philosophy-mathematics/index.html">philosophy of mathematics</a>)
 and still agree on the basic notion of what effective computation is.
The theory of computing, because of its finitistic and constructivist
nature, seems to live more or less on the common ground in which these
theories overlap. </p>

<h4 id="InfoNatuPhen">5.1.1 Information as a natural phenomenon</h4>

<p>
Information as a scientific concept emerges naturally in the context
of our every day dealing with nature when we measure things. Examples
are ordinary actions like measuring the size of an object with a
stick, counting using our fingers, drawing a straight line using a
piece of rope. These processes are the anchor points of abstract
concepts like length, distance, number, straight line that form the
building blocks of science. The fact that these concepts are rooted in
our concrete experience of reality guarantees their applicability and
usefulness. The earliest traces of information processing evolved
around the notions of counting, administration and accountancy.</p>

<div class="indent">

<p>
<strong>Example: Tally sticks</strong>
<br />
One of the most elementary information measuring devices is <em>unary
counting</em> using a tally stick. Tally sticks were already used
around 20,000 years ago. When a hypothetical prehistoric hunter killed
a deer he could have registered this fact by making a scratch
&ldquo;|&rdquo; on a piece of wood. Every stroke on such a stick
represents an object/item/event.  The process of unary counting is
based on the elementary operation of
<em>catenation of symbols</em> into <em>sequences</em>. This measuring
method illustrates a primitive version of the concept of
<em>extensiveness</em> of information: the length of the sequences is
a measure for the amount of items counted. Note that such a sequential
process of counting is non-commutative and non-associative. If
&ldquo;|&rdquo; is our basic symbol and \(\oplus\) our concatenation
operator then a sequence of signs has the form: </p> 

\[((\dots(| \oplus |) \dots) \oplus |)\oplus |)\]

<p>
A new symbol is always concatenated at the end of the sequence. </p>
</div>

<p>
This example helps to understand the importance of <em>context</em> in
the analysis of information. In itself a scratch on a stick may have
no meaning at all, but as soon as we decide that such a scratch
<em>represents</em> another object or event it becomes a
<em>meaningful symbol</em>. When we manipulate it in such a context we
process information. In principle a simple scratch can represent any
event or object we like: symbols are conventional.</p>

<div class="indent">

<p>
<strong>Definition:</strong> A <em>symbol</em> is a mark, sign or word
that indicates, signifies, or is understood as representing an idea,
object, or relationship.
 </p>
</div>

<p>
Symbols are the semantic anchors by which symbol manipulating systems
are tied to the world. Observe that the meta-statement: </p>

<div class="indent">

<p>
The symbol &ldquo;|&rdquo; signifies object <i>y</i>. </p>
</div>

<p>
if true, specifies semantic information:</p>

<ul class="jfy">

<li>It is <em>wellformed</em>: the statement has a specific syntax.
</li>

<li>It is <em>meaningful</em>: Only in the context where the scratch
&ldquo;|&rdquo; is actually made deliberately on, e.g., a tally stick
or in a rock to mark a well defined occurrence it has a meaning.</li>

<li>It is <em>truthful</em>.</li>
</ul>

<p>
Symbol manipulation can take many forms and is not restricted to
sequences. Many examples of different forms of information processing
can be found in prehistoric times. </p>

<div class="indent">

<p>
<strong>Example: Counting sheep in Mesopotamia</strong>
<br />
With the process of urbanization, early accounting systems emerged in
Mesopotamia around 8000 BCE using clay tokens to administer cattle
(Schmandt-Besserat 1992). Different shaped tokens were used for
different types of animals, e.g., sheep and goats. After the
registration the tokens were packed in a globular clay container, with
marks representing their content on the outside. The container was
baked to make the registration permanent. Thus early forms of writing
emerged. After 4000 BCE the tokens were mounted on a string to
preserve the order. </p>
</div>

<p>
The historical transformation from sets to strings is important. It is
a more sophisticated form of coding of information. Formally we can
distinguish several levels of complexity of token combination:</p>

<ul class="jfy">

<li>An <em>unordered</em> collection of <em>similar</em> tokens in a
container. This represents a <em>set</em>. The tokens can move freely
in the container. The volume of the tokens is the only relevant
quality. </li>

<li>An <em>unordered</em> collection of tokens of <em>different
types</em> in a container. This represents a so-called
<em>multiset</em>. Both volume and frequency are relevant.</li>

<li>An <em>ordered</em> collection of <em>typed</em> tokens on a
string. This represents a <em>sequence</em> of symbols. In this case
the length of the string is a relevant quality. </li>
</ul>

<h4 id="SymbManiExteSetsMultStri">5.1.2 Symbol manipulation and extensiveness: sets, multisets and strings</h4>

<p>
Sequences of symbols code more information than multisets and
multisets are more expressive than sets. Thus the emergence of writing
itself can be seen as a quest to find the most expressive
representation of administrative data. When measuring information in
sequences of messages it is important to distinguish the aspects of
<em>repetition</em>, <em>order</em> and <em>grouping</em>. The
extensive aspects of information can be studied in terms of such
structural operations (see entry on
 <a href="../logic-substructural/index.html">substructural logics</a>).
 We can study sets of messages in terms of operators defined on
sequences of symbols. </p>

<div class="indent">

<p>
<strong>Definition:</strong> Suppose <i>m</i>, <i>n</i>, <i>o</i>,
<i>p</i>, &hellip; are symbols and \(\oplus\) is a <em>tensor</em> or
<em>concatentation</em> operator. We define the class of sequences:
</p>

<ol>

<li> Any symbol is a sequence</li>

<li> If \(\alpha\) and \(\beta\) are sequences then \((\alpha
\oplus\beta)\)is a sequence</li>
</ol> For sequences we define the following basic properties on the
level of symbol concatenation:

<ol>

<li> <strong>Contraction:</strong> 

\[(m\ \oplus m) = m.\]

 Contraction destroys
information about <em>frequency</em> in the sequence. Physical
interpretation: two occurrences of the same symbol can collapse to one
occurrence when they are concatenated. </li>


<li> <strong>Commutativity:</strong> 

\[(m\ \oplus n) = (n\ \oplus\ m)\]

 Commutativity
destroys information about <em>order</em> in the sequence. Physical
interpretation: symbols may swap places when they are concatenated.
</li>


<li> <strong>Associativity:</strong> 

\[ (p\oplus (q \oplus r)) = ((p \oplus q)\oplus r)\ \]

 Associativity
destroys information about <em>nesting</em> in the sequence. Physical
interpretation: symbols may be regrouped when they are concatenated.
</li>
</ol>

<div class="indent">
<p>
<em>Observation</em>: 
Systems of sequences with contraction, commutativity and associativity
behave like sets. Consider the equation: </p> 

\[\{p,q\} \cup \{p,r\} = \{p,q,r\}\]

<p>
When we model the sets as two sequences \((p \oplus q)\) and \((p
\oplus r)\), the corresponding implication is: </p> 

\[(p \oplus q),(p \oplus r) \vdash ((p \oplus q) \oplus r)\]
</div>

<p>
<strong>Proof:</strong></p> 

\[
\begin{align}
((p \oplus q) &amp;\oplus (p \oplus r)) &amp; \tt{Concatenation}\\
((q \oplus p) &amp; \oplus (p \oplus r)) &amp;  \tt{Commutativity}\\
(((q \oplus p) \oplus p) &amp; \oplus r) &amp; \tt{Associativity}\\
((q \oplus (p \oplus p)) &amp; \oplus r) &amp; \tt{Associativity}\\
((q \oplus p) &amp; \oplus r) &amp; \tt{Contraction}\\
((p \oplus q) &amp; \oplus r) &amp; \tt{Commutativity}
\end{align}
\]

</div>

<p>
The structural aspects of sets, multisets and strings can be
formulated in terms of these properties: </p>

<div class="indent">
<p><strong>Sets</strong>: &nbsp;
Sequences of messages collapse into sets under <em>contraction</em>,
<em>commutativity</em> and <em>associativity</em>. A set is a
collection of objects in which each element occurs only once: </p>

\[\{a,b,c\} \cup \{b,c,d\} = \{a,b,c,d\}\]

<p>
and for which order is not relevant:</p> 

 \[\{a,b,c\} = \{b,c,a\}.\]

<p>
Sets are associated with our normal everyday <em>naive concept of
information</em> as new, previously unknown, information. We only
update our set if we get a message we have not seen previously. This
notion of information is <em>forgetful</em> both with respect to
sequence and frequency. The set of messages cannot be reconstructed.
This behavior is associated with the notion of <em>extensionality</em>
of sets: we are only interested in equality of elements, not in
frequency.</p>

<p><strong>Multisets</strong>: &nbsp;
Sequences of messages collapse into multisets under
<em>commutativity</em> and <em>associativity</em>. A multiset is a
collection of objects in which the same element can occur multiple
times</p> 

\[\{a,b,c\} \cup \{b,c,d\} = \{a,b,b,c,c,d\}\]

<p>
and for which order is not relevant:</p> 

\[\{a,b,a\} = \{b,a,a\}.\]

<p>
Multisets are associated with a <em>resource sensitive concept of
information</em> defined in <em>Shannon Information</em>. We are
interested in the <em>frequency</em> of the messages. This concept is
<em>forgetful</em> with regards to sequence. We update our set every
time we get a message, but we forget the structure of the sequence.
This behavior is associated with the notion of <em>extensiveness</em>
of information: we are both interested in equality of elements, and in
frequency.</p> 

<p><strong>Sequences</strong>: &nbsp;
Sequences are associative. Sequences are ordered multisets: \(aba \neq
baa\). The whole structure of the sequence of a message is stored.
Sequences are associated with <em>Kolmogorov complexity</em> defined
as the length of a sequence of symbols. </p>

</div>

<p>
Sets may be interpreted as spaces in which objects can move freely.
When the same objects are in each others vicinity they collapse in to
one object. Multisets can be interpreted as spaces in which objects
can move freely, with the constraint that the total number of objects
stays constant. This is the standard notion of extensiveness: the
total volume of a space stays constant, but the internal structure may
differ. Sequences may be interpreted as spaces in which objects have a
fixed location. In general a sequence contains more information than
the derived multiset, which contains more information than the
associated set.</p>

<div class="indent">

<p>
<em>Observation</em>: The interplay between the notion of
sequences and multisets can be interpreted as a formalisation of the
<em>malleability of a piece of wax</em> that pervades history of
philosophy as the paradigm of information. Different sequences (forms)
are representations of the same multiset (matter). The volume of the
piece of wax (length of the string) is constant and thus a measure for
the amount of information that can be represented in the wax (i.e.in
the sequence of symbols). In terms of quantum physics the stability of
the piece of wax seems to be an emergent property: the statistical
instability of objects on an atomic level seem to even out when large
quantities of them are manipulated. </p>
</div>

<h4 id="SetsNumb">5.1.3 Sets and numbers</h4>

<p>
The notion of a set in mathematics is considered to be fundamental.
Any identifiable collection of discrete objects can be considered to
be a set. The relation between theory of sets and the concept of
information becomes clear when we analyze the basic statement: </p>

\[
e \in A
\]

<p>
Which reads the object <i>e</i> is an element of the set <i>A</i>.
Observe that this statement, if true, represents a piece of semantic
information. It is wellformed, meaningful and truthful. (see entry on
 <a href="../information-semantic/index.html">semantic conceptions of information</a>)
 The concept of information is already at play in the basic building
blocks of mathematics.The philosophical question &ldquo;What are
sets?&rdquo; the answer to the <em>ti esti</em> question, is
determined <em>implicitly</em> by the Zermelo-Fraenkel axioms (see
entry on
 <a href="../set-theory/index.html">set theory</a>),
 the first of which is that of <em>extensionality</em>: </p>

<div class="indent">

<p>
Two sets are equal if they have the same elements. </p>
</div>

<p>
The idea that mathematical concepts are defined implicitly by a set of
axioms was proposed by Hilbert but is not uncontroversial (see entry
on the
 <a href="../frege-hilbert/index.html">Frege-Hilbert controversy</a>).
 The fact that the definition is implicit entails that we only have
<em>examples</em> of what sets are without the possibility to
formulate any positive predicate that defines them. Elements of a set
are not necessarily physical, nor abstract, nor spatial or temporal,
nor simple, nor real. The only prerequisite is the possibility to
formulate clear judgments about membership. This implicit definition
of the notion of a set is not unproblematic. We might define objects
that at first glance seem to be proper sets, which after scrutiny
appear to be internally inconsistent. This is the basis for: </p>

<div class="indent">

<p>
<strong>Russell&rsquo;s paradox</strong>:
 This paradox, which motivated a lot of research into the foundations
of mathematics, is a variant of the liars paradox attributed to the
Cretan philosopher Epeimenides (ca. 6 BCE) who apparently stated that
Cretans always lie. The crux of these paradoxes lies in the
combination of the notions of: <em>Universality</em>,
<em>Negation</em>, and <em>Self-reference</em>.</p>

<p>
Any person who is not Cretan can state that all Cretans always lie.
For a Cretan this is not possible because of the universal negative
self-referential nature of the statement. If the statement is true, he
is not lying which makes the statement untrue: a real paradox based on
self contradiction. Along the same lines Russel coined the concept of
the <em>set of all sets that are not member of themselves</em>, for
which membership cannot be determined. Apparently the <em>set of all
sets</em> is an inadmissible object within set theory. In general
there is in philosophy and mathematics a limit to the extent in which
a system can verify statements about itself within the system.
(For further discussion, see the entry on
 <a href="../russell-paradox/index.html">Russell&rsquo;s paradox</a>.)
</p>
</div>

<p>
The implicit definition of the concepts of sets, entails that the
class is essentially <em>open</em> itself. There are mathematical
definitions of objects of which it is unclear or highly controversial
whether they define a set or not. </p>

<p>
Modern philosophy of mathematics starts with the Frege-Russell theory
of numbers (Frege 1879, 1892, Goodstein 1957, see entry on
 <a href="../settheory-alternative/index.html">alternative axiomatic set theories</a>)
 in terms of sets. If we accept the notion of a class of objects as
valid and fundamental, together with the notion of a one-to-one
correspondence between classes of objects, then we can define numbers
as sets of equinumerous classes. </p>

<div class="indent">

<p>
<strong>Definition:</strong> Two sets <i>A</i>and <i>B</i> are
<em>equinumerous</em>, \(A \sim B\), if there exists a one-to-one
correspondence between them, i.e., a function \(f: A \rightarrow B\)
such that for every \(a \in A\) there is exactly one \(f(a) \in B\).
</p>
</div>

<p>
Any set of, say four, objects then becomes a representation of the
number 4 and for any other set of objects we can establish membership
to the equivalence class defining the number 4 by defining a one to
one correspondence to our example set.</p>

<div class="indent">

<p>
<strong>Definition:</strong> If <i>A</i> is a finite set, then
\(\mathcal{S}_A = \{X \mid X \sim A \}\) is the class of all sets
equinumerous with <i>A</i>. The associated <em>generalization
operation</em> is the <em>cardinality function</em>: \(|A|
=\mathcal{S}_A = \{X \mid X \sim A \} = n\). This defines a <em>natural
number</em> \(|A|= n \in \mathbb{N}\) associated with the set
<i>A</i>. </p>
</div>

<p>
We can reconstruct large parts of the mathematical universe by
selecting appropriate mathematical example objects to populate it,
beginning with the assumption that there is a single unique empty set
\(\emptyset\) which represents the number 0. This gives us the
existence of a set with only one member \(\{\varnothing\}\) to
represent the number 1 and repeating this construction,
\(\{\varnothing,\{\varnothing\}\}\) for 2, the whole set of natural
numbers \(\mathbb{N}\) emerges. Elementary arithmetic then is defined
on the basis of Peano&rsquo;s axioms:
 </p>

<ol>

<li>Zero is a number.</li>

<li> If <i>a</i> is a number, the successor of <i>a</i> is a
number.</li>

<li> Zero is not the successor of a number.</li>

<li>Two numbers of which the successors are equal are themselves
equal.</li>

<li>(induction axiom.) If a set <i>S</i> of numbers contains zero and
also the successor of every number in <i>S</i>, then every number is
in <i>S</i>.</li>
</ol>

<p>
The fragment of the mathematical universe that emerges is relatively
uncontroversial and both Platonists and constructivists might agree on
its basic merits. On the basis of Peano&rsquo;s axioms we can define
more complex functions like addition and multiplication which are
closed on \(\mathbb{N}\) and the inverse functions, subtraction and
division, which are not closed and lead to the set of whole numbers
\(\mathbb{Z}\) and the rational numbers \(\mathbb{Q}\). </p>

<h4 id="MeasInfoNumb">5.1.4 Measuring information in numbers</h4>

<p>
We can define the concept of information for a number <i>n</i> by
means of an unspecified function \(I(n)\). We observe that addition
and multiplication specify <em>multisets</em>: both are
<em>non-contractive</em> and <em>commutative</em> and
<em>associative</em>. Suppose we interpret the tensor operator
\(\oplus\) as multiplication \(\times\). It is natural to define the
<em>semantics</em> for \(I(m \times n)\) in terms of addition. If we
get both messages <i>m</i> and <i>n</i>, the total amount of
information in the combined messages is the sum of the amount of
information in the individual messages. This leads to the following
constraints: </p>

<div class="indent">

<p>
<strong>Definition:</strong> <em>Additivity Constraint</em>: </p>

\[ I(m \times n) = I(m) + I(n) \]

</div>

<p>
Furthermore we want bigger numbers to contain more information than
smaller ones, which gives a: </p>

<div class="indent">

<p>
<strong>Definition:</strong> <em>Monotonicity Constraint</em>: </p>

\[ I(m) \leq  I(m + 1) \]

</div>

<p>
We also want to select a certain number <i>a</i> as our <em>basic unit
of measurement</em>:</p>

<div class="indent">

<p>
<strong>Definition:</strong> <em>Normalization Constraint</em>: </p>

\[ I(a) = 1 \]

</div>

<p>
The following theorem is due to R&eacute;nyi (1961):</p>

<div class="indent">

<p>
<strong>Theorem:</strong> The Logarithm is the only mathematical
operation that satisfies Additivity, Monotonicity and Normalisation.
</p>

<p>
<em>Observation</em>: The logarithm \(\log_a n\) of a number
<i>n</i> characterizes our intuitions about the concept of information
in a number <i>n</i> <em>exactly</em>. When we decide that 1)
multisets are the right formalisation of the notion of extensiveness,
and 2) multiplication is the right operation to express additivity,
then the logarithm is the only measurement function that satisfies our
constraints. </p>
</div>

<p>
We define: </p>

<div class="indent">

<p>
<strong>Definition:</strong> For all natural numbers \(n \in
\mathbb{N}^{+}\)</p> 

\[
I(n) = \log_a n.
\] 

<ul>

<li>For \(a = 2\) our unit of measurement is the <em>bit</em> </li>

<li>For \(a = e\) (i.e., Euler&rsquo;s number) our unit of measurement
is the <em>gnat</em> </li>

<li>For \(a = 10\) our unit of measurement is the <em>Hartley</em>
</li>
</ul>
</div>

<h4 id="MeasInfoProbSetsNumb">5.1.5 Measuring information and probabilities in sets of numbers</h4>

<p>
For finite sets we can now specify the amount of information we get
when we know a certain element of a set conditional to knowing the set
as a whole.</p>

<div class="indent">

<p>
<strong>Definition:</strong> Suppose <i>S</i> is a finite set and we
have: </p> 

\[e \in S\]

<p>
then, </p> 

\[I(e \mid S) = \log_a |S| \]

<p>
i.e., the log of the cardinality of the set. </p>
</div>

<p>
The bigger the set, the harder the search is, the more information we
get when we find what we are looking for. Conversely, without any
further information the <em>probability</em> of selecting a certain
element of <i>S</i> is \(p_S(x) = \frac{1}{|S|}\). The associated
function is the so-called Hartley function: </p>

<div class="indent">

<p>
<strong>Definition:</strong> If a sample from a finite set S uniformly
at random is picked, the information revealed after the outcome is
known is given by the <em>Hartley function</em> (Hartley 1928): </p>

\[H_0(S)= \log_a |S|\]

</div>

<p>
The combination of these definitions gives a theorem that ties
together the notions of conditional information and probability: </p>

<div class="indent">

<p>
<strong>Unification Theorem:</strong> If <i>S</i> is a finite set
then, </p> 

\[I(x\mid S) = H_0(S)\] 

</div>

<p>
The information about an element <i>x</i> of a set <i>S</i>
conditional to the set is equal to the log of the probability that we select this
element <i>x</i> under uniform distribution, which is a measure of our
<em>ignorance</em> if we know the set but not which element of the set
is to be selected. </p>

<div class="indent">

<p>
<em>Observation</em>: Note that the Hartley function unifies
the concepts of <em>entropy</em> defined by Boltzmann \(S = k \log
W\), where <i>W</i> is the cardinality of the set of micro states of
system <i>S</i>, with the concept of <em>Shannon information</em>
\(I_S(x) = - \log p(x)\). If we consider <i>S</i> to be a set of
messages, then the probability that we select an element <i>x</i> from
the set (i.e., get a message from <i>S</i> ) under uniform
distribution <i>p</i>is \(\frac{1}{|S|}\). \(H_0(S)\) is also known as
the <em>Hartley Entropy</em> of <i>S</i>. </p>
</div>

<p>
Using these results we define the <em>conditional amount of
information</em> in a subset of a finite set as:</p>

<div class="indent">

<p>
<strong>Definition:</strong> If <i>A</i> is a finite set and <i>B</i>
is an arbitrary subset \(B \subset A\), with \(|A|=n\) and \(|B|=k\)
we have: </p> 

\[I(B\mid A)=\log_a {n \choose k}\]  

</div>

<p>
This is just an application of our basic definition of information:
the cardinality of the class of subsets of <i>A</i> with size <i>k</i>
is \({n \choose k}\). </p>

<p>
The formal properties of the concept of probability are specified by
the Kolmogorov Axioms of Probability: </p>

<div class="indent">

<p>
<strong>Definition:</strong> \(P(E)\) is the probability <i>P</i> that
some event <i>E</i> occurs. \((\Omega, F,P)\), with \(P(\Omega)=1\),
is a <em>probability space</em>, with sample space \(\Omega\),
<em>event space</em> and <em>probability measure</em>. </p>
</div>

<p>
Let \(P(E)\) be the probability <i>P</i> that some event <i>E</i>
occurs. Let \((\Omega, F,P)\), with \(P(\Omega)=1\), be a
<em>probability space</em>, with sample space \(\Omega\), <em>event
space</em> F and <em>probability measure</em> P.</p>

<ol>

<li>The probability of an event is a <em>non-negative real
number</em></li>

<li>There is a <em>unit of measure</em>. The probability that one of
the events in the event space will occur is 1: \(P(\Omega= 1)\)</li>

<li>Probability is <em>additive over sets of independent</em>: 

\[P \left(\bigcup^{\infty}_{i=1} E_i \right) = \sum^{\infty}_{i=1} P(E_i)\]

 </li>
</ol>

<p>
One of the consequences is <em>monotonicity</em>: if \(A \subseteq B\)
implies \(P(A) \leq P(B)\). Note that this is the same notion of
additivity as defined for the concept of information. At subatomic
level the Kolmogorov Axiom of additivity loses its validity in favor
of a more subtle notion (see
 <a href="#QuanInfoBeyo">section 5.3</a>).
 </p>

<h4 id="PersForUnif">5.1.6 Perspectives for unification</h4>

<p>
From a philosophical point of view the importance of this construction
lies in the fact that it leads to an ontologically neutral concept of
information based on a very limited robust base of axiomatic
assumptions: </p>

<ul class="jfy">

<li>It is <strong>reductionist</strong> in the sense that once one
accepts the concepts like classes and mappings, the definition of the
concept of <em>Information</em> in the context of more complex
mathematical concepts naturally emerges. </li>

<li>It is <strong>universal</strong> in the sense that the notion of a
set is universal and open. </li>

<li>It is <strong>semantic</strong> in the sense that the notion of a
set itself is a semantic concept. </li>

<li> It <strong>unifies</strong> a variety of notions (sets,
cardinality, numbers, probability, extensiveness, entropy and
information) in one coherent conceptual framework. </li>

<li>It is <strong>ontologically neutral</strong> in the sense that the
notion of a set or class does not imply any ontological constraint on
its possible members.</li>
</ul>

<p>
This shows how Shannon&rsquo;s theory of information and
Boltzmann&rsquo;s notion of entropy are rooted in more fundamental
mathematical concepts. The notions of a <em>set of messages</em> or a
<em>set of micro states</em> are specializations of the more general
mathematical concept of a <em>set</em>. The concept of information
already exists on this more fundamental level. Although many open
questions still remain, specifically in the context of the relation
between information theory and physics, perspectives on a unified
theory of information now look better than at the beginning of the
twenty-first century. </p>

<h4 id="InfoProcFlowInfo">5.1.7 Information processing and the flow of information</h4>

<p>
The definition of the amount of information in a number in therms of
logarithms allows us to classify other mathematical functions in terms
of their capacity to process information. The <em>Information
Efficiency</em> of a function is the difference between the amount of
information in the input of a function and the amount of information
in the output (Adriaans 2016
 [<a href="#Oth">OIR</a>]).
 It allows us to measure <em>how information flows</em> through a set
of functions. We use the shorthand \(f(\overline{x})\) for
\(f(x_1,x_2,\dots,x_k)\):</p>

<div class="indent">

<p>
<strong>Definition:</strong> <em>Information Efficiency of a
Function</em>: Let \(f: \mathbb{N}^k \rightarrow \mathbb{N}\) be a
function of <i>k</i> variables. We have: </p>

<ul class="jfy">

<li> the <em>input information</em> \(I(\overline{x})\) and </li>

<li> the <em>output information</em> \(I(f(\overline{x}))\). </li>

<li> The information efficiency of the expression \( f(\overline{x})\)
is 

\[\delta(f(\overline{x}))= I(f(\overline{x})) - I(\overline{x})\]

 </li>

<li> A function <i>f</i> is <em>information conserving</em> if
\(\delta(f(\overline{x}))=0\) i.e., it contains exactly the amount of
information in its input parameters, </li>

<li> it is <em>information discarding</em> if
\(\delta(f(\overline{x}))\lt 0\) and </li>

<li> it has <em>constant information</em> if \(\delta(f(\overline{x}))
= c\). </li>

<li> it is <em>information expanding</em> if
\(\delta(f(\overline{x}))\gt 0\). </li>
</ul>
</div>

<p>
In general deterministic information processing systems do not
<em>create</em> new information. They only <em>process</em> it. The
following <em>fundamental theorem about the interaction between
information and computation</em> is due to Adriaans and Van Emde Boas
(2011):</p>

<div class="indent">

<p>
<strong>Theorem:</strong> Deterministic programs do not expand
information. </p>
</div>

<p>
This is in line with both Shannon&rsquo;s theory and Kolmogorov
complexity. The outcome of a deterministic program is always the same,
so the probability of the outcome is 1 which gives under
Shannon&rsquo;s theory, 0 bits of <em>new</em> information. Likewise
for Kolmogorov complexity, the output of a program can never be more
complex than the length of the program itself, plus a constant. This
is analyzed in depth in Adriaans and Van Emde Boas (2011). In a
deterministic world it is the case that if: </p> 

\[\texttt{program(input)=output}\] then \[I(\texttt{output}) \leq
I(\texttt{program}) + I(\texttt{input})\]

<p>
The essence of information is uncertainty and a message that occurs
with probability &ldquo;1&rdquo; contains no information. The fact
that it might take a long time to compute the number is irrelevant as
long as the computation halts. Infinite computations are studied in
the theory of Scott domains (Abramsky &amp; Jung 1994). </p>

<p>
Estimating the information efficiency of elementary functions is not
trivial. The primitive recursive functions (see entry on
 <a href="../recursive-functions/index.html">recursive functions</a>)
 have one information expanding operation, <em>the increment operation</em>, one
information discarding operation, <em>choosing</em>, all the others
are information neutral. The information efficiency of more complex
operations is defined by a combination of counting and choosing. From
an information efficiency point of view the elementary arithmetical
functions are complex families of functions that describe computations
with the same outcome, but with different computational histories.</p>

<p>
Some arithmetical operations expand information, some have constant
information and some discard information. During the execution of
deterministic programs expansion of information may take place, but,
if the program is effective, the descriptive complexity of the output
is limited. The flow of information is determined by the succession of
types of operations, and by the balance between the complexity of the
operations and the number of variables. </p>

<p>
We briefly discuss the information efficiency of the two basic
recursive functions on two variables and their coding
possibilities:</p>

<p>
<strong>Addition</strong> 
Addition is associated with information storage in terms of
<em>sequences</em> or strings of symbols. It is <em>information
discarding</em> for natural numbers bigger than 1. We have \(\delta(a
+ b) \lt 0\) since \(\log (a + b) \lt \log a + \log b\). Still,
addition has information preserving qualities. If we add numbers with
different log units we can reconstruct the frequency of the units from
the resulting number: 

\[\begin{align}
232 &amp; = 200 + 30 + 2 \\
&amp; =  (2 \times 10^2) + (3 \times 10^1) + (2 \times 10^0)\\
&amp; = 100 + 100 + 10 + 10 + 10 + 1 + 1
\end{align}
\] 
</p>
<p>
Since the information in the building blocks, 100, 10 and 1, is given
the number representation can still be reconstructed. This implies
that natural numbers code in terms of <em>addition of powers of </em>
<i>k</i> in principle two types of information: value <em>and</em>
frequency. We can use this insight to code <em>complex typed</em>
information in <em>single</em> natural numbers. Basically it allows us
to code any natural numbers in a string of symbols of length \(\lceil
\log_k n \rceil \), which specifies a quantitative measure for the
amount of information in a number in terms of the length of its code.
See
 <a href="#Numb">section 3.3</a>
 for a historical analysis of the importance of the discovery of
position systems for information theory. </p>

<p>
<strong>Multiplication</strong> is by definition <em>information
conserving</em>. We have: \(\delta(a \times b) = 0\), since \(\log (a
\times b) = \log a + \log b\). Still multiplication does not preserve
all information in its input: the order of the operation is lost. This
is exactly what we want from an operator that characterizes an
extensive measure: only the <em>extensive</em> qualities of the
numbers are preserved. If we multiply two numbers \(3 \times 4\), then
the result, 12, allows us to reconstruct the original computation, in
so far as we can reduce all its components to their most elementary
values: \(2 \times 2 \times 3 = 12\). This leads to the observation
that some numbers act as <em>information building blocks</em> of other
numbers, which gives us the concept of a <em>prime number</em>: </p>

<div class="indent">

<p>
<strong>Definition:</strong> A <em>prime number</em> is a number that
is only divisible by itself or 1. </p>
</div>

<p>
The concept of a prime number gives rise to the <em>Fundamental
Theorem of Arithmetic</em>:</p>

<div class="indent">

<p>
<strong>Theorem:</strong> Every natural number <i>n</i> greater than 1
is a product of a multiset \(A_p\) of primes, and this
multiset is unique for <i>n</i>. </p>
</div>

<p>
The Fundamental Theorem of Arithmetic can be seen as a theorem about
conservation of information: for every natural number there is a set
of natural numbers that contains exactly that same amount of
information. The factors of a number form a so-called
<em>multiset</em>: a set that may contain multiple copies of the same
element: e.g., the number 12 defines the multiset \(\{2,2,3\}\) in
which the number 2 occurs twice. This makes multisets a powerful
device for coding information since it codes qualitative information
(i.e., the numbers 2 and 3) as well as quantitative information (i.e.,
the fact that the number 2 occurs twice and the number 3 only once).
This implies that natural numbers in terms of <em>multiplication of
primes</em> also code two types of information: value <em>and</em>
frequency. Again we can use this insight to code <em>complex
typed</em> information in <em>single</em> natural numbers. </p>

<h4 id="InfoPrimFact">5.1.8 Information, primes, and factors</h4>

<p>
Position based number representations using addition of powers are
straightforward and easy to handle and form the basis of most of our
mathematical functions. This is not the case for coding systems based
on multiplication. Many of the open questions in the philosophy of
mathematics and information arise in the context of the concepts of
the Fundamental Theorem of Arithmetic and Primes. We give a short
overview: </p>

<div class="indent">
<p><strong>(Ir)regularity of the set of primes.</strong><br />
Since antiquity it is known that there is an infinite number of
primes. The proof is simple. Suppose the set of primes <i>P</i> is
finite. Now multiply all elements of <i>P</i> and add 1. The resulting
number cannot be divided by any member of <i>P</i>, so <i>P</i> is
incomplete. An estimation of the density of the prime numbers given by
the <em>Prime Number Theorem</em> (see entry in <em>Encyclopaedia
Britannica</em> on Prime Number Theorem
 [<a href="#Oth">OIR</a>]).
 It states that the gaps between primes in the set of natural numbers
of size <i>n</i> is roughly \( \ln n\), where \(\ln\) is the natural
logarithm based on Euler&rsquo;s number <i>e</i>. A refinement of the
density estimation is given by the so-called <em>Riemannn
hypothesis</em>, formulated by him in 1859 (Goodman and Weisstein 2019
 [<a href="#Oth">OIR</a>]),
 which is commonly regarded as deepest unsolved problems in
mathematics, although most mathematicians consider the hypothesis to
be true.</p>

<p><strong>(In)efficiency of Factorization.</strong><br />
Since multiplication conserves information the function is, to an
extent, reversible. The process of finding the unique set of primes
for a certain natural number <i>n</i> is called
<em>factorization</em>. Observe that the use of the term
&ldquo;only&rdquo; in the definition of a prime number implies that
this is in fact a <em>negative</em> characterization: a number
<i>n</i> is prime if there exists no number between 1 and <i>n</i>
that divides it. This gives us an <em>effective</em> procedure for
factorization of a number <i>n</i> (simply try to divide <i>n</i> by
all numbers between 1 and \(n)\), but such techniques are not
<em>efficient</em>.</p>

<p>
If we use a position system to represent the number <i>n</i> then the
process of identifying factors of <i>n</i> by trial and error will
take a deterministic computer program at most <i>n</i> trials which
gives a computation time exponential in the length of the
representation of the number which is \(\lceil \log n \rceil \).
Factorization by trial and error of a relatively simple number, of,
say, two hundred digits, which codes a rather small message, could
easily take a computer of the size of our whole universe longer than
the time passed since the big bang. So, although theoretically
feasible, such algorithms are completely unpractical. </p>

<p>
Factorization is possibly an example of so-called <em>trapdoor</em>
one-to-one function which is easy to compute from one side but very
difficult in its inverse. Whether factorization is really difficult,
remains an open question, although most mathematicians believe the
problem to be hard.
Note that factorization in this context can be seen as the process of
decoding a message. If factorization is hard it can be used as an
encryption technique. Classical encryption techniques, like RSA, are based on multiplying
codes with large prime numbers. Suppose Alice has a message encoded as
a large number <i>m</i> and she knows Bob has access to a large prime
<i>p</i>. She sends the number \(p \times m = n\) to Bob. Since Bob
knows <i>p</i> he can easily reconstruct <i>m</i> by computing \(m =
n/p\). Since factorization is difficult any other person that receives
the message <i>n</i> will have a hard time reconstructing
<i>m</i>.</p> 


<p><strong>Primality testing versus Factorization.</strong><br />
Although at this moment efficient techniques for factorization on
classical computers are not known to exist, there is an efficient
algorithm that decides for us whether a number is prime or not: the
so-called AKS primality test (Agrawal et al. 2004). So, we might know
a number is not prime, while we still do not have access to its set of
factors. </p>

<p><strong>Classical- versus Quantum Computing.</strong><br />
Theoretically factorization is efficient on quantum computers using
Shor&rsquo;s algorithm (Shor 1997). This algorithm has a non-classical
quantum subroutine, embedded in a deterministic classical program.
Collections of quantum bits can be modeled in terms of complex higher
dimensional vector-spaces, that, in principle, allow us to analyze an
exponential number \(2^n\) of correlations between collections of
<i>n</i> objects. Currently it is not clear whether larger quantum
computers will be stable enough to facilitate practical applications,
but that the world at quantum level has relevant computational
possibilities can not be doubted anymore, e.g., quantum random
generators are available as a commercial product (see
<em>Wikipedia</em> entry on Hardware random number generator
 [<a href="#Oth">OIR</a>]).
 As soon as viable quantum computers become available almost all of
the current encryption techniques become useless, although they can be
replaced by quantum versions of encryption techniques (see
the entry on <a href="../qt-quantcomp/index.html">Quantum Computiong</a>).</p>
</div>

<div class="indent">


</div>

<p>
There is an infinite number of observations we can make about the set
\(\mathbb{N}\) that are not implied directly by the axioms, but
involve a considerable amount of computation. </p>

<h4 id="IncoArit">5.1.9 Incompleteness of arithmetic</h4>

<p>
In a landmark paper in 1931 Kurt G&ouml;del proved that any consistent
formal system that contains elementary arithmetic is fundamentally
incomplete in the sense that it contains true statements that cannot
be proved within the system. In a philosophical context this implies
that the semantics of a formal system rich enough to contain
elementary mathematics cannot be defined in terms of mathematical
functions within the system, i.e., there are statements that contain
semantic information about the system in the sense of being
<em>well-formed</em>, <em>meaningful</em> and <em>truthful</em>
without being <em>provable</em>. </p>

<p>
Central is the concept of a <em>Recursive Function.</em> (see entry on
 <a href="../recursive-functions/index.html">recursive functions</a>).
 Such functions are defined on numbers. G&ouml;del&rsquo;s notion of a
recursive function is closest to what we would associate with
computation in every day life. Basically they are elementary
arithmetical functions operating on natural numbers like addition,
subtraction, multiplication and division and all other functions that
can be defined on top of these. </p>

<p>
We give the basic structure of the proof. Suppose <i>F</i> is a formal
system, with the following components: </p>

<ul class="jfy">

<li> It has a finite set of symbols</li>

<li> It has a syntax that enables us to combine the symbols in to
well-formed formulas</li>

<li> It has a set of deterministic rules that allows us to derive new
statements from given statements</li>

<li> It contains elementary arithmetic as specified by Peano&rsquo;s
axioms (see section <a href="#SetsNumb">5.1.3</a> above).</li>
 </ul>

<p>
Assume furthermore that <i>F</i> is consistent, i.e., it will never
derive false statements form true ones. In his proof G&ouml;del used
the coding possibilities of multiplication to construct an image of
the system (see the discussion of 
 <a href="../goedel-incompleteness/sup1.html">G&ouml;del numbering</a>
 from the entry on G&ouml;del&rsquo;s Incompleteness Theorems).
 According to the fundamental theorem of arithmetic 
 any number can be uniquely factored in to its primes. This defines a
one-to-one relationship between multisets of numbers and numbers: the
number 12 can be constructed on the basis of the multiset
\(\{2,2,3\}\) as \(12=2 \times 2\times 3\) and vice versa. This allows
us to code any sequence of symbols as a specific individual number in
the following way:</p>

<ul class="jfy">

<li> A unique number is assigned to every symbol</li>

<li> Prime numbers locate the position of the symbol in a string</li>

<li> The actual number of the same primes in the set of prime factors
defines the symbol</li>
</ul>

<p>
On the basis of this we can code any sequence of symbols as a
so-called G&ouml;del number, e.g., the number: </p> 

\[2 \times 3 \times 3 \times 5 \times 5 \times 7 = 3150\] 

<p>
codes the multiset \(\{2,3,3,5,5,7\}\), which represents the string
&ldquo;abba&rdquo; under the assumption \(a=1\), \(b=2\). With this
observation conditions close to those that lead to the paradox of
Russel are satisfied: elementary arithmetic itself is rich enough to
express: <em>Universality</em>, <em>Negation</em>, and
<em>Self-reference</em>.</p>

<p>
Since arithmetic is consistent this does not lead to paradoxes, but to
incompleteness. By a construction related to the liars paradox
G&ouml;del proved that such a system must contain statements that are
true but not provable: there are true sentences of the form &ldquo;I
am not provable&rdquo;. </p>

<div class="indent">

<p>
<strong>Theorem:</strong> Any formal system that contains elementary
arithmetic is fundamentally <em>incomplete</em>. It contains
statements that are <em>true</em> but not <em>provable</em>. </p>
</div>

<p>
In the context of philosophy of information the incompleteness of
mathematics is a direct consequence of the rich possibilities of the
natural numbers to code information. In principle any deterministic
formal system can be represented in terms of elementary arithmetical
functions. Consequently, If such a system itself contains arithmetic
as a sub system, it contains a infinite chain of endomorphisms (i.e.,
images of itself). Such a system is capable of reasoning about its own
functions and proofs but since it is consistent (and thus the
construction of paradoxes is not possible within the system) it is by
necessity incomplete. </p>

<h3 id="InfoSymbComp">5.2 Information and Symbolic Computation</h3>

<p>
Recursive functions are abstract relations defined on natural numbers.
In principle they can be defined without any reference to space and
time. Such <em>functions</em> must be distinguished from the
<em>operations</em> that we use to compute them. These operations
mainly depend on the type of <em>symbolic representations</em> that we
choose for them. We can represent the number seven as unary number
\(|||||||\), binary number 111, Roman number VII, or Arabic number 7
and depending on our choice other types of <em>sequential symbol
manipulation</em> can be used to compute the addition two plus five is
seven, which can be represented as: 

\[
\begin{align}
|| + ||||| &amp; = ||||||| \\
10 + 101 &amp;  = 111 \\
\textrm{II} + \textrm{V} &amp; = \textrm{VII}\\
2 + 5 &amp;= 7 \\
\end{align}
\]  

 Consequently we can
read these four sentences as four statements of the <em>same</em>
mathematical truth, or as statements specifying the results of four
<em>different</em> operations. </p>

<div class="indent">

<p>
<em>Observation</em>: There are (at least) two different
perspectives from which we can study the notion of computation. The
semantics of the symbols is different under these interpretations.
</p>

<ul class="jfy">

<li>The <strong>Recursive Function Paradigm</strong> studies
computation in terms of <em>abstract functions</em> on <em>natural
numbers</em> outside space and time. When interpreted as a
mathematical fact, the \(+\) sign in \(10 + 101 = 111\) signifies the
<em>mathematical function called addition</em> and the \(=\) sign
specifies <em>equality</em>. </li>

<li>The <strong>Symbol Manipulation Paradigm</strong> studies
computation in terms of <em>sequential operations</em> on <em>spatial
representations of strings of symbols</em>. When interpreted as an
operation the \(+\) sign in \(10 + 101 = 111\) signifies the <em>input
for a sequential process of symbol manipulation</em> and the \(=\)
sign specifies the <em>result of that operation</em> or
<em>output</em>. Such an algorithm could have the following form:

\[
\begin{aligned}
\tt{   10}\\ 
\tt{+ 101}\\ \hline
\tt{  111}
\end{aligned}\]

 </li>
</ul>

</div>

<p>
This leads to the following tentative definition:</p>

<div class="indent">

<p>
<strong>Definition:</strong> Deterministic Computing on a Macroscopic
Scale can be defined as the local, sequential, manipulation of
discrete objects according to deterministic rules. </p>
</div>

<p>
In nature there are many other ways to perform such computations. One
could use an abacus, study chemical processes or simply manipulate
sequences of pebbles on a beach. The fact that the objects we
manipulate are discrete together with the observation that the dataset
is self-referential implies that the data domain is in principle
Dedekind Infinite:</p>

<div class="indent">

<p>
<strong>Definition:</strong> A set <i>S</i> is Dedekind Infinite if it
has a bijection \(f: S \rightarrow S^{\prime}\) to a proper subset
\(S^{\prime} \subset S\). </p>
</div>

<p>
Since the data elements are discrete and finite the data domain will
be countable infinite and therefore isomorphic to the set of natural
numbers. </p>

<div class="indent">

<p>
<strong>Definition:</strong> An infinite set <i>S</i> is
<em>countable</em> if there exists a bijection with the set of natural
numbers \(\mathbb{N}\). </p>
</div>

<p>
For infinite countable sets the notion of information is defined as
follows: </p>

<div class="indent">

<p>
<strong>Definition:</strong> Suppose <i>S</i> is countable and
infinite and the function \(f:S \rightarrow \mathbb{N}\) defines a
one-to-one correspondence, then: 

\[I(a\mid S,f) = \log f(a)\]

 i.e., the amount of
information in an index of <i>a</i> in <i>S</i> given <i>f</i>. </p>
</div>

<p>
Note that the correspondence <i>f</i> is specified explicitly. As soon
as such an index function is defined for a class of objects in the
real world, the manipulation of these objects can be interpreted a
form of computing.</p>

<h4 id="TuriMach">5.2.1 Turing machines</h4>

<p>
Once we choose a finite set of symbols and our operational rules the system starts
to produce statements about the world.</p>

<div class="indent">

<p>
<em>Observation</em>: The meta-sentence:</p>

<div class="indent">

<p>
The sign &ldquo;0&rdquo; is the symbol for zero.</p>
</div>

<p>
specifies <em>semantic information</em> in the same sense as the
statement \(e \in A\) does for sets (see
 <a href="#LogiSemaInfo">section 6.6</a>).
 The statement is <em>wellformed</em>, <em>meaningful</em> and
<em>truthful</em>. </p>
</div>

<p>
We can study symbol manipulation in general on an abstract level,
without any semantic implications. Such a theory was published by Alan
Turing (1912&ndash;1954). Turing developed a general theory of
computing focusing on the actual operations on symbols a mathematician
performs (Turing 1936). For him a computer was an abstraction of a
real mathematician sitting behind a desk, receiving problems written
down on an in-tray (the inut), solving them according to fixed rules
(the process) and leaving them to be picked up in an out-tray (the
output). </p>

<p>
Turing first formulated the notion of a general theory of computing
along these lines. He proposed abstract machines that operate on
infinite tapes with three symbols: blank \((b)\), zero \((0)\) and one
\((1)\). Consequently the data domain for Turing machines is the set
of relevant tape configurations, which can be associated with the set
of binary strings, consisting of zero&rsquo;s and one&rsquo;s. The
machines can read and write symbols on the tape and they have a
transition function that determines their actions under various
conditions. On an abstract level Turing machines operate like
functions. </p>

<div class="indent">

<p>
<strong>Definition:</strong> If \(T_i\) is a <em>Turing machine</em>
with index <i>i</i> and <i>x</i> is a string of zero&rsquo;s and
one&rsquo;s on the tape that function as the <em>input</em> then
\(T_i(x)\) indicates the tape configuration after the machine has
stopped, i.e., its <em>output</em>. </p>
</div>

<p>
There is an infinite number of Turing machines. Turing discovered that
there are so-called universal Turing machines \(U_j\) that can emulate
any other Turing machine \(T_i\). </p>

<div class="indent">

<p>
<strong>Definition:</strong> The expression \(U_j(\overline{T_i}x)\)
denotes the result of the emulation of the computation \(T_i(x)\) by
\(U_j\) after reading the self-delimiting description
\(\overline{T_i}\) of machine \(T_j\). </p>
</div>

<p>
The self-delimiting code is necessary because the input for \(U_j\) is
coded as one string \(\overline{T_i}x\). The universal machine \(U_j\)
separates the input string \(\overline{T_i}x\) in to its two
constituent parts: the description of the machine \(\overline{T_i}\)
and the input for this machine <i>x</i>. </p>

<p>
The self-referential nature of general computational systems allows us
to construct machines that emulate other machines. This suggests the
possible existence of a &lsquo;super machine&rsquo; that emulates all
possible computations on all possible machines and predicts their
outcome. Using a technique called diagonalization, where one analyzes
an enumeration of all possible machines running on descriptions of all
possible machines, Turing proved that such a machine can not exist.
More formally: </p>

<div class="indent">

<p>
<strong>Theorem:</strong> There is no Turing machine that predicts for
any other Turing machine whether it stops on a certain input or not.
</p>
</div>

<p>
This implies that for a certain universal machine \(U_i\) the set of
inputs on which it stops in finite time, is uncomputable. In recent years the notion of infinite computations on Turing machines has also been studied (Hamkins and Lewis 2000.) Not every
machine will stop on every input, but in some case infinite computations compute useful output (consider the infinite expansion of the number pi). </p>

<div class="indent">

<p>
<strong>Definition:</strong> The <em>Halting set</em> is the set of
combinations of Turing machines \(T_i\) and inputs <i>x</i> such that
the computation \(T_i(x)\) stops. </p>
</div>

<p>
The existence of universal Turing machines indicates that the class
embodies a notion of <em>universal computing</em>: any computation
that can be performed on a specific Turing machine can also be
performed on any other universal Turing machine. This is the
mathematical foundation of the concept of a general programmable
computer. These observations have bearing on the theory of
information: certain measures of information, like Kolmogorov
complexity, are defined, but not computable. </p>

<p>
The proof of the existence uncomputable functions in the class of
Turing machines is similar to the incompleteness result of G&ouml;del
for elementary arithmetic. Since Turing machines were defined to study
the notion of computation and thus contain elementary arithmetic. The
class of Turing machines is in itself rich enough to express:
<em>Universality</em>, <em>Negation</em> and <em>Self-reference</em>.
Consequently Turing machines can model universal negative statements
about themselves. Turing&rsquo;s uncomputability proof is also
motivated by the liars paradox, and the notion of a machine that stops
on a certain input is similar to the notion of a proof that exists for
a certain statement. At the same time Turing machines satisfy the
conditions of G&ouml;del&rsquo;s theorem: they can be modeled as a
formal system <i>F</i> that contains elementary Peano arithmetic.</p>

<div class="indent">

<p>
<em>Observation</em>: Since they can emulate each other, the
<em>Recursive Function Paradigm</em> and the <em>Symbol Manipulation
Paradigm</em> have the <em>same computational strength</em>. Any
function that can be computed in one paradigm can also by definition
be computed in the other. </p>
</div>

<p>
This insight can be generalized:</p>

<div class="indent">

<p>
<strong>Definition:</strong> An infinite set of computational
functions is <em>Turing complete</em> if it has the same computational
power as the general class of Turing machines. In this case it is
called Turing equivalent. Such a system is, like the class of Turing
machines, universal: it can emulate any computable function. </p>
</div>

<p>
The philosophical implications of this observation are strong and
rich, not only for the theory of computing but also for our
understanding of the concept of information. </p>

<h4 id="UnivInva">5.2.2 Universality and invariance</h4>

<p>
There is an intricate ration between the notion of universal computing
and that of information. Precisely the fact that Turing Systems are
universal allows us to say that they process information, because
their universality entails invariance:</p>

<div class="indent">

<p>
<strong>Small Invariance Theorem:</strong> The concept of information
in a string <i>x</i> measured as the length of the smallest string of
symbols <i>s</i> of a program for a universal Turing machine <i>U</i>
such that \(U(s)= x\) is invariant, modulo an additive constant, under selection of
different universal Turing machines </p>
</div>

<p>
<strong>Proof:</strong> The proof is simple and relevant for
philosophy of information. Let \(l(x)\) be the length of the string of
symbols <i>x</i>. Suppose we have two different universal Turing
machines \(U_j\) and \(U_k\). Since they are universal they can both
emulate the computation \(T_i(x)\) of Turing machine \(T_i\) on input
<i>x</i>:</p> 

\[U_j(\overline{T}_i^jx)\]

 

\[U_k(\overline{T}_i^kx)\]

<p>
Here \(l(\overline{T}_i^j)\) is the length of the code for \(T_i\) on
\(U_j\) and \(l(\overline{T}_i^k)\) is the length of the code for
\(T_i\) on \(U_k\). Suppose \(l(\overline{T}_i^jx) \ll
l(\overline{T}_i^kx)\), i.e., the code for \(T_i\) on \(U_k\) is much
less efficient that on \(U_j\). Observe that the code for \(U_j\) has
constant length, i.e., \(l(\overline{U}_j^k)=c\). Since \(U_k\) is
universal we can compute: </p> 

\[U_k(\overline{U}_j^k \ \overline{T}_i^jx)\]

<p>
The length of the input for this computation is: </p> 

\[l(\overline{U}_j^k \ \overline{T}_i^jx) = c + l(\overline{T}_i^jx)\] 

<p>
Consequently the specification of the input for the computation
\(T_i(x)\) on the universal machine \(U_k\) never needs to longer than
a constant. \(\Box\) </p>

<p>
This proof forms the basis of the theory of Kolmogorov complexity and
is originally due to Solomonoff (1964a,b) and discovered independently
by Kolmogorov (1965) and Chaitin (1969). Note that this notion of
invariance can be generalized over the class of Turing Complete
Systems:</p>

<div class="indent">

<p>
<strong>Big Invariance Theorem:</strong> The concept of information
measured in terms of the length of the input of a computation is invariant, modulo an additive constant, for for Turing Complete systems. </p>

<p>
<strong>Proof:</strong> Suppose we have a Turing Complete system
<i>F</i>. By Definition any computation \(T_i(x)\) on a Turing machine
can be emulated in <i>F</i> and vice versa. There will be a special
universal Turing machine \(U_F\) that emulates the computation
\(T_i(x)\) in <i>F</i>: \(U_F(\overline{T}_i^Fx)\). In principle
\(\overline{T}_i^F\) might use a very inefficient way to code programs
such that \(\overline{T}_i^F\) can have any length. Observe that the
code for any other universal machine \(U_j\) emulated by \(U_F\) has
constant length, i.e., \(l(\overline{U}_j^F)=c\). Since \(U_F\) is
universal we can also compute: </p> 

\[U_F(\overline{U}_j^F \ \overline{T}_i^jx)\]

<p>
The length of the input for this computation is: 

\[l(\overline{U}_j^F \ \overline{T}_i^jx) = c + l(\overline{T}_i^jx)\] 

Consequently the specification of the input for the computation
\(T_i(x)\) on the universal machine \(U_F\) never needs to be longer
than a constant. \(\Box\) </p>
</div>

<p>
How strong this result is becomes clear when we analyze the class of
Turing complete systems in more detail. In the first half of the
twentieth century three fundamentally different proposals for a
general theory of computation were formulated: G&ouml;del&rsquo;s
recursive functions ( G&ouml;del 1931), Turing&rsquo;s automata
(Turing 1937) and Church&rsquo;s Lambda Calculus (Church 1936). Each
of these proposals in its own way clarifies aspects of the notion of
computing. Later much more examples followed. The class of Turing
equivalent systems is diverse. Apart from obvious candidates like all
general purpose programming languages (C, Fortran, Prolog, etc.) it
also contains some unexpected elements like various games (e.g.,
Magic: The Gathering [Churchill 2012
 <a href="#Oth">OIR</a>]).
 The table below gives an overview of some conceptually interesting
systems: </p>

<div>

<p>
<strong>An overview of some Turing Complete systems</strong> </p>

<table class="cellpad-med-dense centered">
<tr>
  <td><strong>System</strong></td>
  <td><strong>Data Domain</strong></td> </tr>
<tr>
  <td>General Recursive Functions</td>
  <td>Natural Numbers</td> </tr>
<tr>
  <td>Turing machines and their generalizations</td>
  <td>Strings of symbols</td> </tr>
<tr>
  <td>Diophantine Equations</td>
  <td>Integers</td> </tr>
<tr>
  <td>Lambda calculus</td>
  <td>Terms</td> </tr>
<tr>
  <td>Type-0 languages</td>
  <td>Sentences</td> </tr>
<tr>
  <td>Billiard Ball Computing</td>
  <td>Ideal Billiard Balls</td> </tr>
<tr>
  <td>Cellular automata</td>
  <td>Cells in one dimension</td> </tr>
<tr>
  <td>Conway&rsquo;s game of life </td>
  <td>Cells in two dimensions</td> </tr>
</table>
</div>

<p>
We make the following: </p>

<div class="indent">

<p>
<em>Observation</em>: The class of Turing equivalent systems
is open, because it is defined in terms of purely operational mappings
between computations. </p>
</div>

<p>
A direct consequence of this observation is: </p>

<div class="indent">

<p>
<em>Observation</em>: The general theory of computation and
information defined by the class of Complete Turing machines is
ontologically neutral. </p>
</div>

<p>
It is not possible to derive any necessary qualities of computational
systems and data domains beyond the fact that they are general
mathematical operations and structures. Data domains on which Turing
equivalent systems are defined are not necessarily physical, nor
temporal, nor spatial, not binary or digital. At any moment a new
member for the class can be introduced. We know that there are
computational systems that are weaker than the class of Turing
machines (e.g., regular languages). We cannot rule out the possibility
that one-day we come across a system that is stronger. The thesis that
such a system does not exist is known as the Church-Turing thesis (see
entry on
 <a href="../church-turing/index.html">Church-Turing thesis</a>):</p>
 

<div class="indent">

<p>
<strong>Church-Turing Thesis:</strong> The class of Turing machines
characterizes the notion of algorithmic computing exactly. </p>
</div>

<p>
We give an overview of the arguments for and against the thesis:</p>

<p>
<em>Arguments in favor of the thesis</em>: The theory of Turing
machines seems to be the most general theory possible that we can
formulate since it is based on a very limited set of assumptions about
what computing is. The fact that it is universal also points in the
direction of its generality. It is difficult to conceive in what sense
a more powerful system could be &ldquo;more&rdquo; universal. Even if
we could think of such a more powerful system, the in- and output for
such a system would have to be finite and discrete and the computation
time also finite. So, in the end, any computation would have the form
of a finite function between finite data sets, and, in principle, all
such relations can be modeled on Turing machines. The fact that all
known systems of computation we have defined so far have the same
power also corroborates the thesis. </p>

<p>
<em>Arguments against the thesis</em>: The thesis is, in its present
form, unprovable. The class of Turing Complete systems is open. It is
defined on the basis of the existence of equivalence relations between known
systems. In this sense it does not define the notion of computing
intrinsically. It doesn&rsquo;t not provide us with a philosophical
theory that defines what computing <em>exactly is</em>. Consequently
it does not allow us to exclude any system from the class <em>a
priori</em>. At any time a proposal for a notion of computation might
emerge that is fundamentally stronger. What is more, nature provides
us with stronger notions of computing in the form of quantum
computing. Quantum bits are really a generalization of the normal
concept of bits that is associated with symbol manipulation, although
in the end quantum computing does not seem to necessitate us to
redefine the notion of computing so far. We can never rule out that
research in physics, biology or chemistry will define systems that
will force us to do so. Indeed various authors have suggested such
systems but there is currently no consensus on convincing candidates
(Davis 2006). Dershowitz and Gurevich (2008) claim to have vindicated
the hypothesis, but this result is not generally accepted (see the
discussion on &ldquo;Computability &ndash; What would it mean to
disprove the Church-Turing thesis&rdquo;, in the
 <a href="#Oth">Other Internet Resources [OIR]</a>).
 </p>

<p>
Being Turing complete seems to be quite a natural condition for a
(formal) system. Any system that is sufficiently rich to represent the
natural numbers and elementary arithmetical operations is Turing
complete. What is needed is a finite set of operations defined on a
set of discrete finite data elements that is rich enough to make the
system self-referential: its operations can be described by its data
elements. This explains, in part, why we can use mathematics to
describe our world. The abstract notion of computation defined as
functions on numbers in the abstract world mathematics and the
concrete notion of computing by manipulation objects in our every day
world around us coincide. The concepts of information end computation
implied by the <em>Recursive Function Paradigm</em> and the <em>Symbol
Manipulation Paradigm</em> are the same. </p>

<div class="indent">

<p>
<em>Observation</em>: If one accepts the fact that the
Church-Turing thesis is open, this implies that the question about the
existence of a universal notion of information is also open. At this
stage of the research it is not possible to specify the <em>a
priori</em> conditions for such a general theory. </p>
</div>

<h3 id="QuanInfoBeyo">5.3 Quantum Information and Beyond</h3>

<p>
We have a reasonable understanding of the concept of classical
computing, but the implications of quantum physics for computing and
information may determine the philosophical research agenda for
decades to come if not longer. Still it is already clear that the
research has repercussions for traditional philosophical positions:
the Laplacian view (Laplace 1814 [1902]) that the universe is
essentially deterministic seems to be falsified by empirical
observations. Quantum random generators are commercially available
(see <em>Wikipedia</em> entry on Hardware random number generator
 [<a href="#Oth">OIR</a>])
 and quantum fluctuations do affect neurological, biological and
physical processes at a macroscopic scale (Albrecht &amp; Phillips
2014). Our universe is effectively a process that generates
information permanently. Classical deterministic computing seems to be
too weak a concept to understand its structure. </p>

<p>
Standard computing on a macroscopic scale can be defined as <em>local,
sequential, manipulation of discrete objects according to
deterministic rules</em>. Is has a natural interpretation in
operations on the set of natural numbers <i>N</i> and a natural
measurement function in the log operation \(\log: \mathbb{N}
\rightarrow \mathbb{R}\) associating a real number to every natural
number. The definition gives us an adequate information measure for
countable infinite sets, including number classes like the integers
\(\mathbb{Z}\), closed under <em>subtraction</em>, and the rational
numbers \(\mathbb{Q}\), closed under <em>division</em>.</p>

<p>
The operation of <em>multiplication</em> with the associated
<em>logarithmic</em> function characterizes our intuitions about
additivity of the concept of information exactly. It leads to a
natural bijection between the set of natural numbers \(\mathbb{N}\)
and the set of multisets of numbers (i.e., sets of prime factors). The
notion of a multiset is associated with the properties of
<em>commutativity</em> and <em>associativity</em>. This program can be
extended to other classes of numbers when we study division algebras
in higher dimensions. The following table gives an overview of some
relevant number classes together with the <em>properties of the
operation of multiplication</em> for these classes: </p>

<table class="cellpad-small-dense centered vert-top small hrules">
<tr>
  <td><strong>Number Class</strong></td>
  <td><strong>Symbol</strong></td>
  <td><strong>Dimen&shy;sions</strong></td>
  <td><strong>Coun&shy;table</strong></td>
  <td><strong>Linear</strong></td>
  <td><strong>Commu&shy;tative</strong></td>
  <td><strong>Associ&shy;ative</strong></td> </tr>
<tr>
  <td>Natural numbers</td>
  <td>\(\mathbb{N}\)</td>
  <td>1</td>
  <td>Yes</td>
  <td>Yes</td>
  <td>Yes</td>
  <td>Yes</td> </tr>
<tr>
  <td> Integers</td>
  <td>\(\mathbb{Z}\)</td>
  <td>1</td>
  <td>Yes</td>
  <td>Yes</td>
  <td>Yes</td>
  <td>Yes</td> </tr>
<tr>
  <td>Rational numbers</td>
  <td>\(\mathbb{Q}\)</td>
  <td>1</td>
  <td>Yes</td>
  <td>Yes</td>
  <td>Yes</td>
  <td>Yes</td> </tr>
<tr>
  <td>Real numbers</td>
  <td>\(\mathbb{R}\)</td>
  <td>1</td>
  <td>No</td>
  <td>Yes</td>
  <td>Yes</td>
  <td>Yes</td> </tr>
<tr>
  <td>Complex numbers</td>
  <td>\(\mathbb{C}\)</td>
  <td>2</td>
  <td>No</td>
  <td>No</td>
  <td>Yes</td>
  <td>Yes</td> </tr>
<tr>
  <td> Quaternions </td>
  <td>\(\mathbb{H}\)</td>
  <td>4</td>
  <td>No</td>
  <td>No</td>
  <td>No</td>
  <td>Yes</td> </tr>
<tr>
  <td> Octonions </td>
  <td>\(\mathbb{O}\)</td>
  <td>8</td>
  <td>No</td>
  <td>No</td>
  <td>No</td>
  <td>No</td> </tr>
</table>

<p>
The table is ordered in terms of increasing generality. Starting from
the set of natural numbers \(\mathbb{N}\), various extensions are
possible taking into account closure under subtraction,
\(\mathbb{Z}\), and division, \(\mathbb{Q}\). This are the number
classes for which we have adequate finite symbolic representations on
a macroscopic scale. For elements of the real numbers \(\mathbb{R}\)
such a representations are not available. The real numbers
\(\mathbb{R}\) introduce the aspect of manipulation of infinite
amounts of information in one operation.</p>

<div class="indent">

<p>
<em>Observation</em>: For <em>almost all</em> \(e \in
\mathbb{R}\) we have \(I(e) = \infty\). </p>
</div>

<p>
More complex division algebras can be defined when we introduce
imaginary numbers as negative squares \(i^2 = -1\). We can now define
complex numbers: \(a + bi\), where <i>a</i> is the real part and
\(bi\) the imaginary part. Complex numbers can be interpreted as
vectors in a two dimensional plane. Consequently they lack the notion
of a strict <em>linear order</em> between symbols. Addition is quite
straightforward:</p> 

\[(a + bi) + (c + di) = (a + b) + (c + d)i\] 

<p>
Multiplication follows the normal distribution rule but the result is
less intuitive since it involves a negative term generated by
\(i^2\):</p> 

\[(a + bi) (c + di) = (ac - bd) + (bc + ad)i\] 

<p>
In this context multiplication ceases to be a purely extensive
operation:</p>

<div class="indent">
</div>

<p>
More complicated numbers systems with generalizations of this type of
multiplication in 4 and 8 dimensions can be defined. Kervaire (1958)
and Bott &amp; Milnor (1958) independently proved that the only four
division algebras built on the reals are \(\mathbb{R}\),
\(\mathbb{C}\), \(\mathbb{H}\) and \(\mathbb{O}\), so the table gives
a comprehensive view of all possible algebra&rsquo;s that define a
notion of extensiveness. For each of the number classes in the table a
separate theory of information measurement, based on the properties of
multiplication, can be developed. For the countable classes
\(\mathbb{N}\), \(\mathbb{Z}\) and \(\mathbb{Q}\) these theories ware
equivalent to the standard concept of information implied by the
notion of Turing equivalence. Up to the real numbers these theories
satisfy our intuitive notions of extensiveness of information. For
complex numbers the notion of <em>information efficiency</em> of
multiplication is destroyed. The quaternions lack the property of
<em>commutativity</em> and the octonions that of
<em>associativity</em>. These models are not just abstract
constructions since the algebras play an important role in our
descriptions of nature:</p>

<ol>

<li>Complex numbers are used to specify the mathematical models of
quantum physics (Nielsen &amp; Chuang 2000).</li>

<li>Quaternions do the same for Einstein&rsquo;s special theory of
relativity (De Leo 1996).</li>

<li>Some physicists believe octonions form a theoretical basis for a
unified theory of strong and electromagnetic forces (e.g., Furey
2015).</li>
</ol>

<p>
We briefly discuss the application of vector spaces in quantum
physics. Classical information is measured in bits. Implementation of
bits in nature involves macroscopic physical systems with at least two
different stable states and a low energy reversible transition process
(i.e., switches, relays, transistors). The most fundamental way to
store information in nature on an atomic level involves qubits. The
qubit is described by a state vector in a two-level quantum-mechanical
system, which is formally equivalent to a two-dimensional vector space
over the complex numbers (Von Neumann 1932; Nielsen &amp; Chuang
2000). Quantum algorithms have, in some cases, a fundamentally lower
complexity (e.g., Shor&rsquo;s algorithm for factorization of integers
(Shor 1997)).</p>

<div class="indent">

<p>
<strong>Definition:</strong> The <em>quantum bit</em>, or
<em>qubit</em>, is a generalization of the classical bit. The quantum
state of qubit is represented as the linear superposition of two
orthonormal basis vectors:</p> 

\[\ket{0} = \begin{bmatrix}1 \\ 0 \end{bmatrix}, \ket{1} =
\begin{bmatrix}0 \\ 1 \end{bmatrix} \]

<p>
Here the so-called Dirac or &ldquo;bra-ket&rdquo; notion is used:
where \(\ket{0}\) and \(\ket{1}\) are pronounced as &ldquo;ket
0&rdquo; and &ldquo;ket 1&rdquo;. The two vectors together form the
<em>computational basis</em> \(\{\ket{0}, \ket{1}\}\), which
defines a vector in a two-dimensional <em>Hilbert space</em>. A
combination of <i>n</i> qubits is represented by a <em>superposition
vector</em> in a \(2^n\) dimensional Hilbert space, e.g.:</p>

\[\ket{00} =  \begin{bmatrix}1 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \ket{01} =  \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \ket{10} =  \begin{bmatrix}0 \\ 0 \\ 1 \\ 0 \end{bmatrix}, \ket{11} =  \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \end{bmatrix} \]

<p>
 A pure qubit is a coherent <em>superposition</em> of the
basis states: </p>

\[\ket{\psi} = \alpha\ket{0} + \beta\ket{1}\]

<p>
where \(\alpha\) and \(\beta\) are complex numbers, with the
constraint:</p> 

\[|\alpha|^2 + |\beta|^2 = 1\]

<p>
In this way the values can be interpreted as probabilities:
\(|\alpha|^2\) is the probability that the qubit has value 0 and
\(|\beta|^2\) is the probability that the qubit has value 1. </p>
</div>

<p>
Under this mathematical model our intuitions about computing as local,
sequential, manipulation of discrete objects according to
deterministic rules evolve in to a much richer paradigm:</p>

<ol>

<li><strong> Infinite information </strong> The introduction of
<em>real numbers</em> facilitates the manipulation of objects of
infinite descriptive complexity, although there is currently no
indication that this expressivity is actually necessary in quantum
physics. </li>

<li> <strong>Non-classical probability</strong> <em>Complex
numbers</em> facilitate a richer notion of extensiveness in which
probabilities cease to be classical. The third axiom of Kolmogorov
loses its validity in favor of probabilities that enhance or suppress
each other, consequently extensiveness of information is lost. </li>

<li> <strong>Superposition and Entanglement </strong> The
representation of qubits in terms of complex high dimensional vector
spaces implies that qubits cease to be isolated discrete objects.
Quantum bits can be in superposition, a situation in which they are in
two discrete states at the same time. Quantum bits fluctuate and
consequently they <em>generate</em> information. Moreover quantum
states of qubits can be correlated even when the information bearers
are separated by a long distance in space. This phenomenon, known as
<em>entanglement</em> destroys the property of <em>locality</em> of
classical computing (see the entry on
 <a href="../qt-entangle/index.html">quantum entanglement and information</a>).
 </li>
</ol>

<p>
From this analysis it is clear that the description of our universe at
very small (and very large) scales involves mathematical models that
are alien to our experience of reality in everyday life. The
properties that allow us to understand the world (the existence of
stable, discrete objects that preserve their identity in space and
time) seem to be <em>emergent</em> aspects of a much more complex
reality that is incomprehensible to us outside its mathematical
formulation. Yet, at a macroscopic level, the universe facilitates
elementary processes, like counting, measuring lengths, and the
manipulation of symbols, that allow us to develop a consistent
hierarchy of mathematical models some of which seems to describe the
deeper underlying structure of reality. </p>

<p>
In a sense the same mathematical properties that drove the development
of elementary accounting systems in Mesopotamia four thousand years
ago, still help us to penetrate in to the world of subatomic
structures. In the past decennia information seems to have become a
vital concept in physics. Seth Lloyd and others (Zuse 1969; Wheeler
1990; Schmidhuber 1997b; Wolfram 2002; Hutter 2010) have analyzed
computational models of various physical systems. The notion of
information seems to play a major role in the analysis of black holes
(Lloyd &amp; Ng 2004; Bekenstein 1994
 [<a href="#Oth">OIR</a>]).
 Erik Verlinde (2011, 2017) has proposed a theory in which gravity is
analyzed in terms of information. For the moment these models seem to
be purely descriptive without any possibility of empirical
verification. </p>

<h2 id="AnomParaProb">6. Anomalies, Paradoxes, and Problems</h2>

<p>
Some of the fundamental issues in philosophy of Information are
closely related to existing philosophical problems, others seem to be
new. In this paragraph we discuss a number of observations that may
determine the future research agenda. Some relevant questions are:</p>

<ol>

<li>Are there uniquely identifying descriptions that do not contain
all information about the object they refer to?</li>

<li>Does computation create new information? </li>

<li>Is there a difference between construction and systematic search?
</li>
</ol>

<p>
Since Frege most mathematicians seem to believe that the answer to the
first question is positive (Frege 1879, 1892). The descriptions
&ldquo;The morning star&rdquo; and &ldquo;The evening star&rdquo; are
associated with <em>procedures</em> to identify the planet Venus, but
they do not give access to all information about the object itself. If
this were so the discovery that the evening star is in fact also the
morning star would be uninformative. If we want to maintain this
position we get into conflict, because in terms of information theory
the answer to the second question is negative (see
 <a href="#InfoProcFlowInfo">section 5.1.7</a>).
 Yet this observation is highly counter intuitive, because it implies
that we never can construct new information on the basis of
deterministic computation, which leads to the third question. These
issues cluster around one of the fundamental open problems of
Philosophy of Information: </p>

<div class="indent">

<p>
<strong>Open problem</strong> What is the interaction between
Information and Computation? </p>
</div>

<p>
Why would we compute at all, if according to our known information
measures, deterministic computing does not produce new information?
The question could be rephrased as: should we use Kolmogorov or Levin
complexity (Levin 1973, 1974, 1984) as our basic information measure?
In fact both choices lead to relevant, but fundamentally different,
theories of information. When using the Levin measure, computing
generates information and the answer to the three questions above is a
&ldquo;yes&rdquo;, when using Kolmogorov this is not the case. The
questions are related to many problems both in mathematics and
computer science. Related issues like approximation, computability and
partial information are also studied in the context of Scott domains
(Abramsky &amp; Jung 1994). Below we discuss some relevant
observations. </p>

<h3 id="ParaSystSear">6.1 The Paradox of Systematic Search</h3>

<p>
The essence of information is the fact that it reduces uncertainty.
This observation leads to problems in opaque contexts, for instance,
when we search an object. This is illustrated by Meno&rsquo;s paradox
(see entry on
 <a href="../epistemic-paradoxes/index.html">epistemic paradoxes</a>):</p>
 
<blockquote>

<p>
<em>And how will you enquire, Socrates, into that which you do not
know? What will you put forth as the subject of enquiry? And if you
find what you want, how will you ever know that this is the thing
which you did not know?</em> (Plato, Meno, 80d1-4) </p>
</blockquote>

<p>
The paradox is related to other open problems in computer science and
philosophy. Suppose that John is looking for a unicorn. It is very
unlikely that unicorns exist, so, in terms of Shannon&rsquo;s theory,
John gets a lot of information if he finds one. Yet from a descriptive
Kolmogorov point of view, John does not get new information, since he
already knows what unicorns are. The related <em>paradox of systematic
search</em> might be formulated as follows:</p>

<blockquote>

<p>
Any information that can be found by means of systematic search has no
value, since we are certain to find it, given enough time.
Consequently information only has value as long as we are uncertain
about its existence, but then, since we already know what we are
looking for, we get no new information when we find out that it
exists. </p>

<p>
<strong>Example:</strong> Goldbach conjectured in 1742
that every even number bigger than
2 could be written as the sum of two primes. Until today this
conjecture remains unproved. Consider the term &ldquo;The first number
that violates Goldbach&rsquo;s conjecture&rdquo;. It does not give us
all information about the number, since the number might not exist.
The prefix &ldquo;the first&rdquo; ensures the description, if it
exists, is unique, and it gives us an algorithm to find the number. It
is a <em>partial uniquely identifying</em> description. This algorithm
is only <em>effective</em> if the number really exists, otherwise it
will run forever. If we find the number this will be great news, but
from the perspective of descriptive complexity the number itself will
be totally uninteresting, since we already know the relevant
properties to find it. Observe that, even if we have a number <i>n</i>
that is a counter example to Goldbach&rsquo;s conjecture, it might be
difficult to verify this: we might have to check almost all primes \(
\leq n\). This can be done <em>effectively</em> (we will always get a
result) but not, as far as we know, <em>efficiently</em> (it might
take &ldquo;close&rdquo; to <i>n</i> different computations) . </p>
</blockquote>

<p>
A possible solution is to specify the constraint that it is
<em>illegal</em> to measure the information content of an object in
terms of partial descriptions, but this would destroy our theory of
descriptive complexity. Note that the complexity of an object is the
length of the shortest program that produces an object on a universal
Turing machine. In this sense the phrase &ldquo;the first number that
violates Goldbach&rsquo;s conjecture&rdquo; is a perfect description
of a program, and it adequately measures the descriptive complexity of
such a number. The short description reflects the fact that the
number, if it exists, is very special, and thus it has a high
possibility to occur in some mathematical context. </p>

<p>
There are relations which well-studied philosophical problems like the
Anselm&rsquo;s ontological argument for God&rsquo;s existence and the
Kantian counter claim that existence is not a predicate. In order to
avoid similar problems Russell proposed to interpret unique
descriptions existentially (Russell 1905): A sentence like &ldquo;The
king of France is bald&rdquo; would have the following logical
structure: </p> 

\[\exists (x) (KF(x) \wedge \forall (y)(KF(y) \rightarrow x=y) \wedge B(x))\]

<p>
This interpretation does not help us to analyze decision problems that
deal with existence. Suppose the predicate <i>L</i> is true of
<i>x</i> if I&rsquo;m looking for <i>x</i>, then the logical structure
of the phrase &ldquo;I&rsquo;m looking for the king of France&rdquo;
would be: </p> 

\[\exists (x) (KF(x) \wedge
\forall (y)(KF(y) \rightarrow x=y) \wedge L(x)),\]

<p>
i.e., if the king of France does not exist it cannot be true that I am
looking for him, which is unsatisfactory. Kripke (1971) criticized
Russell&rsquo;s solution and proposed his so-called causal theory of
reference in which a name get its reference by an initial act of
&ldquo;baptism&rdquo;. It then becomes a <em>rigid designator</em>
(see entry on
 <a href="../rigid-designators/index.html">rigid designators</a>)
 that can be followed back to that original act via causal chains. In
this way <em>ad hoc</em> descriptions like &ldquo;John was the fourth
person to step out of the elevator this morning&rdquo; can establish a
semantics for a name.</p>

<p>
In the context of mathematics and information theory the corresponding
concept is that of names, constructive predicates and ad-hoc
predicates of numbers. For any number there will be in principle an
infinite number of true statements about that number. Since elementary
arithmetic is incomplete there will be statements about numbers that
are true but unprovable. In the limit a vanishing fragment of numbers
will have true predicates that actually compress their description.
Consider the following statements:</p>

<ol>

<li> The symbol &ldquo;8&rdquo; is the name for the number eight.
</li>

<li> The number <i>x</i> is the 1000th Fibonacci number. </li>

<li> The number <i>x</i> is the first number that violates the
Goldbach conjecture. </li>
</ol>

<p>
The first statement simply specifies a name for a number. The second
statement gives a partial description that is constructive,
information compressing and unique. The 1000th Fibonacci number has
209 digits, so the description &ldquo;the 1000th Fibonacci
number&rdquo; is much more efficient than the actual name of the
number. Moreover, we have an algorithm to construct the number. This
might not be that case for the description in the third statement. We
do not know whether the first number that violates the Goldbach
conjecture exists, but if it does, the description might well be
<em>ad hoc</em> and thus gives us no clue to construct the number.
This rise to the conjecture that there are <em>data compressing
effective ad hoc descriptions</em>:</p>

<div class="indent">

<p>
<strong>Conjecture:</strong> There exist numbers that are compressed
by non-constructive unique effective descriptions, i.e., the validity
of the description can be checked effectively given the number, but
the number cannot be constructed effectively from the description,
except by means of systematic search. </p>
</div>

<p>
The conjecture is a more general variant of the so-called
<span>P</span> vs. <span>NP</span> thesis (see
 <a href="#PVersNPProbDescCompVersTimeComp">section 6.3</a>).
 If one replaces the term &ldquo;effective&rdquo; with the term
&ldquo;efficient&rdquo; one gets a formulation of the \(\textrm{P}
\neq \textrm{NP}\) thesis.</p>

<h3 id="EffeSearFiniSets">6.2 Effective Search in Finite Sets</h3>

<p>
When we restrict ourselves to effective search in finite sets, the
problem of partial descriptions, and construction versus search
remain. It seems natural to assume that when one has a definition of a
set of numbers, then one also has all the information about the
members of the set and about its subsets, but this is not true. In
general the computation of the amount of information in a set of
numbers is a highly non-trivial issue. We give some results:</p>

<div class="indent">

<p>
<strong>Lemma</strong> A subset \(A \subset S\) of a set <i>S</i> can
contain more information conditional to the set than the set itself.
</p>
</div>

<p>
<strong>Proof:</strong> Consider the set <i>S</i> of all natural
numbers smaller than <i>n</i>. The descriptive complexity of this set
in bits is \( \log_2 n + c\). Now construct <i>A</i> by selecting half
of the elements of <i>S</i> randomly. Observe that: </p>

\[I(A\mid S)=\log_2 {n \choose {n/2}}\]

<p>
We have:</p>

\[
\lim_{n \rightarrow \infty} 
\frac{I(A\mid S)}
{n}
 = 
\lim_{n \rightarrow \infty} 
\frac{\log_2 {n \choose {n/2}}}
{n}
 = 1\]

<p>
The conditional descriptive complexity of this set will be: \(I(A\mid
S) \approx n + c \gg \log n + c\). \(\Box\)</p>

<p>
A direct consequence is that we can lose information when we merge two
sets. An even stronger result is: </p>

<div class="indent">

<p>
<strong>Lemma:</strong> An element of a set can contain more
information than the set itself. </p>
</div>

<p>
<strong>Proof:</strong> Consider the set <i>S</i> of natural numbers
smaller then \(2^n\). The cardinality of <i>S</i> is \(2^n\). The
descriptive complexity of this set is \(\log n + c\) bits, but for
half of the elements of <i>S</i> we need <i>n</i> bits to describe
them. \(\Box\)</p>

<p>
In this case the description of the set itself is highly compressible,
but it still contains non-compressible elements. When we merge or
split sets of numbers, or add or remove elements, the effects on the
amount of information are in general hard to predict and might even be
uncomputable: </p>

<div class="indent">

<p>
<strong>Theorem:</strong> Information is not monotone under set
theoretical operations </p>
</div>

<p>
<strong>Proof:</strong> Immediate consequence of the lemmas above.
\(\Box\)</p>

<p>
This shows how the notion of information pervades our everyday life.
When John has two apples in his pocket it seems that he can do
whatever he wants with them, but, in fact, as soon as he chooses one
of the two, he has created (new) information. The consequences for
search problems are clear: we can always effectively perform bounded
search on the elements and the set of subsets of a set. Consequently
when we search for such a set of subsets by means of partial
descriptions then the result generates (new) information. This
analysis prima facie appears to force us to accept that in mathematics
there are simple descriptions that allow us to identify complex
objects by means of systematic search. When we look for the object we
have only little information about it, when we finally find it our
information increases to the set of full facts about the object
searched. This is in conflict with our current theories of information
(Shannon and Kolmogorov): any description that allows us to identify
an object effectively by deterministic search contains all relevant
information about the object. The time complexity of the search
process then is irrelevant. </p>

<h3 id="PVersNPProbDescCompVersTimeComp">6.3 The <span>P</span> versus <span>NP</span> Problem, Descriptive Complexity Versus Time Complexity</h3>

<p>
In the past decennia mathematicians have been pondering about a
related question: suppose it <em>would</em> be easy to check whether I
have found what I&rsquo;m looking for, how hard can it be to find such
an object? In mathematics and computer science there seems to be a
considerable class of decision problems that cannot be solved
constructively in polynomial time, \(t(x)=x^c\), where <i>c</i> is a
constant and <i>x</i> is the length of the input), but only through
systematic search of a large part of the solution space, which might
take exponential time, \(t(x)=c^x\). This difference roughly coincides
with the separation of problems that are computationally feasible from
those that are not.</p>

<p>
The issue of the existence of such problems has been framed as the
possible equivalence of the class <span>P</span> of decision problems
that can be solved in time polynomial to the input to the class
<span>NP</span> of problems for which the solution can be checked in
time polynomial to the input. (Garey &amp; Johnson 1979; see also Cook
2000
 [<a href="#Oth">OIR</a>]
 for a good introduction.) </p>

<div class="indent">

<p>
<strong>Example:</strong> A well-known example in the class
<span>NP</span> is the so-called <em>subset sum</em> problem: given a
finite set of natural numbers <i>S</i>, is there a subset
\(S^{\prime}\subseteq S\) that sums up to some number <i>k</i>? It is
clear that when someone proposes a solution \(X \subseteq S\) to this
problem we can easily check whether the elements of <i>X</i> add up to
<i>k</i>, but we might have to check almost all subsets of <i>S</i> in
order to find such a solution ourselves. </p>
</div>

<p>
This is an example of a so-called decision problem. The answer is a
simple &ldquo;yes&rdquo; or &ldquo;no&rdquo;, but it might be hard to
find the answer. Observe that the formulation of the question
conditional to <i>S</i> has descriptive complexity \(\log k + c\),
whereas most random subsets of <i>S</i> have a conditional descriptive
complexity of \(|S|\). So any subset \(S^{\prime}\) that adds up to
<i>k</i> might have a descriptive complexity that is bigger then the
formulation of the search problem. In this sense search seems to
generate information. The problem is that if such a set exists the
search process is bounded, and thus effective, which means that the
phrase &ldquo;the first subset of <i>S</i> that adds up to
<i>k</i>&rdquo; is an adequate description. If \(\textrm{P} =
\textrm{NP}\) then the Kolmogorov complexity and the Levin complexity
of the set \(S^{\prime}\) we find roughly coincide, if \(P \neq
\textit{NP}\) then in some cases \(Kt(S^{\prime}) \gg K(S^{\prime})\).
Both positions, the theory that search generates new information and
the theory that it does not, are counterintuitive from different
perspectives.</p>

<p>
The <span>P</span> vs. <span>NP</span> problem, that appears to be
very hard, has been a rich source of research in computer science and
mathematics although relatively little has been published on its
philosophical relevance. That a solution might have profound
philosophical impact is illustrated by a quote from Scott
Aaronson:</p>

<blockquote>

<p>
If P = NP, then the world would be a profoundly different place than
we usually assume it to be. There would be no special value in
&ldquo;creative leaps,&rdquo; no fundamental gap between solving a
problem and recognizing the solution once it&rsquo;s found. Everyone
who could appreciate a symphony would be Mozart; everyone who could
follow a step-by-step argument would be Gauss&hellip;. 
(Aaronson 2006 &ndash; in the Other Internet Resources)</p>
 </blockquote>

<p>
In fact, if \(\textrm{P}=\textrm{NP}\) then every object that has a description that is not too large and easy to check is also easy to find. </p>

<h3 id="ModeSeleDataComp">6.4 Model Selection and Data Compression</h3>

<p>
In current scientific methodology the sequential aspects of the
scientific process are formalized in terms of the empirical cycle,
which according to de Groot (1969) has the following stages:</p>

<ol>

<li>Observation: The observation of a phenomenon and inquiry
concerning its causes.</li>

<li>Induction: The formulation of hypotheses&mdash;generalized
explanations for the phenomenon.</li>

<li> Deduction: The formulation of experiments that will test the
hypotheses (i.e., confirm them if true, refute them if false).</li>

<li> Testing: The procedures by which the hypotheses are tested and
data are collected.</li>

<li> Evaluation: The interpretation of the data and the formulation of
a theory&mdash;an abductive argument that presents the results of the
experiment as the most reasonable explanation for the phenomenon.</li>
</ol>

<p>
In the context of information theory the set of observations will be a
data set and we can construct models by observing regularities in this
data set. Science aims at the construction of true models of our
reality. It is in this sense a semantical venture. In the 21-st
century the process of theory formation and testing will for the
largest part be done automatically by computers working on large
databases with observations. Turing award winner Jim Grey framed the
emerging discipline of e-science as the fourth data-driven paradigm of
science. The others are empirical, theoretical and computational. As
such the process of automatic theory construction on the basis of data
is part of the methodology of science and consequently of philosophy
of information (Adriaans &amp; Zantinge 1996; Bell, Hey, &amp; Szalay
2009; Hey, Tansley, and Tolle 2009). Many well-known learning
algorithms, like decision tree induction, support vector machines,
normalized information distance and neural networks, use entropy based
information measures to extract meaningful and useful models out of
large data bases. The very name of the discipline Knowledge Discovery
in Databases (KDD) is witness to the ambition of the Big Data research
program. We quote:</p>

<blockquote>

<p>
At an abstract level, the KDD field is concerned with the development
of methods and techniques for making sense of data. The basic problem
addressed by the KDD process is one of mapping low-level data (which
are typically too voluminous to understand and digest easily) into
other forms that might be more compact (for example, a short report),
more abstract (for example, a descriptive approximation or model of
the process that generated the data), or more useful (for example, a
predictive model for estimating the value of future cases). At the
core of the process is the application of specific data-mining methods
for pattern discovery and extraction. (Fayyad, Piatetsky-Shapiro,
&amp; Smyth 1996: 37)</p>
</blockquote>

<p>
Much of the current research focuses on the issue of selecting an
optimal computational model for a data set. The theory of Kolmogorov
complexity is an interesting methodological foundation to study
learning and theory construction as a form of data compression. The
intuition is that the shortest theory that still explains the data is
also the best model for generalization of the observations. A crucial
distinction in this context is the one between <em>one- and two-part
code optimization</em>: </p>

<ul>

<li>

<p>
<strong>One-part Code Optimization:</strong> The methodological
aspects of the theory of Kolmogorov complexity become clear if we
follow its definition. We begin with a well-formed dataset <i>y</i>
and select an appropriate universal machine \(U_j\). The expression
\(U_j(\overline{T_i}x)= y\) is a true sentence that gives us
information about <i>y</i>. The first move in the development of a
theory of measurement is to force all expressiveness to the
instructional or procedural part of the sentence by a restriction to
sentences that describe computations on empty input: </p> 

 \[U_j(\overline{T_i}\emptyset)= y\]

<p>
This restriction is vital for the proof of invariance. From this, in
principle infinite, class of sentences we can measure the length when
represented as a program. We select the ones (there might be more than
one) of the form \(\overline{T_i}\) that are shortest. The length
\(\mathit{l}(\overline{T_i})\) of such a shortest description is a
measure for the information content of <i>y</i>. It is asymptotic in
the sense that, when the data set <i>y</i> grows to an infinite
length, the information content assigned by the choice of another
Turing machine will never vary by more than a constant in the limit.
Kolmogorov complexity measures the information content of a data set
in terms of the shortest description of the set of instructions that
produces the data set on a universal computing device.</p></li>

<li>

<p>
<strong>Two-part Code Optimization:</strong> Note that by restricting
ourselves to programs with empty input and the focus on the <em>length
</em> of programs instead of their <em>content</em> we gain the
quality of invariance for our measure, but we also lose a lot of
expressiveness. The information in the actual program that produces
the data set is neglected. Subsequent research therefore has focused
on techniques to make the explanatory power, hidden in the Kolmogorov
complexity measure, explicit. </p></li>
</ul>

<p>
A possible approach is suggested by an interpretation of Bayes&rsquo;
law. If we combine Shannon&rsquo;s notion of an optimal code with
Bayes&rsquo; law, we get a rough theory about optimal model selection.
Let \(\mathcal{H}\) be a set of hypotheses and let <i>x</i> be a data
set. Using Bayes&rsquo; law, the optimal computational model under
this distribution would be: </p> 

\[\begin{equation} M_{\textit{map}}(x) = \textit{argmax}_{M \in \mathcal{H}} \frac{P(M) P(x\mid M)}{P(x)} \end{equation} \]

<p>
This is equivalent to optimizing: </p> 

\[
\begin{equation}\label{OptimalIbE} \textit{argmin}_{M \in \mathcal{H}} - \log P(M) - \log P(x\mid M) \end{equation}
\]

<p>
Here \(-\log P(M)\) can be interpreted as the length of the optimal
<em>model code</em> in Shannon&rsquo;s sense and \(- \log P(x\mid M)\) as
the length of the optimal <em>data-to-model code</em>; i.e., the data
interpreted with help of the model. This insight is canonized in the
so-called:</p>

<div class="indent">

<p>
<strong>Minimum Description Length (MDL) Principle:</strong> The best
theory to explain a data set is the one that minimizes the sum in bits
of a description of the theory (model code) and of the data set
encoded with the theory (the data to model code). </p>
</div>

<p>
The MDL principle is often referred to as a modern version of
Ockham&rsquo;s razor (see entry on
 <a href="../ockham/index.html">William of Ockham</a>),
 although in its original form Ockham&rsquo;s razor is an ontological
principle and has little to do with data compression. In many cases
MDL is a valid heuristic tool and the mathematical properties of the
theory have been studied extensively (Gr&uuml;nwald 2007). Still MDL,
Ockham&rsquo;s razor and two-part code optimization have been the
subject of considerable debate in the past decennia (e.g., Domingos
1998; McAllister 2003). </p>

<p>
The philosophical implications of the work initiated by Solomonoff,
Kolmogorov and Chaitin in the sixties of the 20-th century are
fundamental and diverse. The universal distribution <i>m</i> proposed
by Solomonoff, for instance, codifies all possible mathematical
knowledge and when updated on the basis of empirical observations
would in principle converge to an optimal scientific model of our
world. In this sense the choice of a universal Turing machine as basis
for our theory of information measurement has philosophical
importance, specifically for methodology of science. A choice for a
universal Turing machine can be seen as a choice of <em>a set of
bias</em> for our methodology. There are roughly two schools:</p>

<ul class="jfy">

<li><strong>Poor machine:</strong> choose a small universal Turing
machine. If the machine is small it is also general and universal,
since there is no room to encode any bias in to the machine. Moreover
a restriction to small machines gives small overhead when emulating
one machine on the other so the version of Kolmogorov complexity you
get gives a measurement with a smaller asymptotic margin. Hutter
explicitly defends the choice of &ldquo;natural&rdquo; small machines
(Hutter 2005; Rathmanner &amp; Hutter 2011), but also Li and
Vit&aacute;nyi (2019) seem to suggest the use of small models. </li>

<li><strong>Rich machine:</strong> choose a big machine that
explicitly reflects what you already know about the world. For
Solomonoff, the inventor of algorithmic complexity, the choice of a
universal Turing machine is the choice for a universal prior. He
defends an evolutionary approach to learning in which an agent
constantly adapts the prior to what he already has discovered. The
selection of your reference Turing machine uniquely characterizes your
<em>a priori</em> information (Solomonoff 1997). </li>
</ul>

<p>
Both approaches have their value. For rigid mathematical proofs the
poor machine approach is often best. For practical applications on
finite data sets the rich model strategy often gets much better
results, since a poor machine would have to &ldquo;re-invent the
wheel&rdquo; every time it compresses a data set. This leads to the
conclusion that Kolmogorov complexity inherently contains a theory
about scientific bias and as such implies a methodology in which the
class of admissible universal models should be explicitly formulated
and motivated <em>a priori</em>. In the past decennia there have been
a number of proposals to define a formal unit of measurement of the
amount of structural (or model-) information in a data set.</p>

<ul>

<li>Aesthetic measure (Birkhoff 1950)</li>

<li>Sophistication (Koppel 1987; Antunes et al. 2006; Antunes &amp;
Fortnow 2003)</li>

<li>Logical Depth (Bennet 1988)</li>

<li>Effective complexity (Gell-Mann, Lloyd 2003)</li>

<li>Meaningful Information (Vit&aacute;nyi 2006)</li>

<li>Self-dissimilarity (Wolpert &amp; Macready 2007)</li>

<li>Computational Depth (Antunes et al. 2006)</li>

<li>Facticity (Adriaans 2008)</li>
</ul>

<p>
Three intuitions dominate the research. A string is
&ldquo;interesting&rdquo; when &hellip; </p>

<ul class="jfy">

<li>a certain amount of computation is involved in its creation
(Sophistication, Computational Depth);</li>

<li>there is a balance between the model-code and the data-code under
two-part code optimization (effective complexity, facticity);</li>

<li>it has internal phase transitions (self-dissimilarity).</li>
</ul>

<p>
Such models penalize both maximal entropy and low information content.
The exact relationship between these intuitions is unclear. The
problem of meaningful information has been researched extensively in
the past years, but the ambition to formulate a universal method for
model selection based on compression techniques seems to be misguided:
</p>

<div class="indent">

<p>
<em>Observation</em>: A measure of meaningful information
based on two-part code optimization can never be <em>invariant</em> in
the sense of Kolmogorov complexity (Bloem et al. 2015). 
</p>
</div>

<p>
This appears to be the case even if we restrict ourselves to weaker
computational models like total functions, but more research is
necessary. There seems to be no <em>a priori</em> mathematical
justification for the approach, although two-part code optimization
continues to be a valid approach in an empirical setting of data sets
that have been created on the basis of repeated
observations. Phenomena that might be related to a theory of
structural information and that currently are ill-understood are:
phase transitions in the hardness of satisfiability problems related
to their complexity (Simon &amp; Dubois 1989; Crawford &amp; Auton
1993) and phase transitions in the expressiveness of Turing machines
related to their complexity (Crutchfield &amp; Young 1989, 1990;
Langton 1990; Dufort &amp; Lumsden 1994). </p>

<h3 id="DeteTher">6.5 Determinism and Thermodynamics</h3>

<p>
Many basic concepts of information theory were developed in the
nineteenth century in the context of the emerging science of
thermodynamics. There is a reasonable understanding of the
relationship between Kolmogorov Complexity and Shannon information (Li
&amp; Vit&aacute;nyi 2008; Gr&uuml;nwald &amp; Vit&aacute;nyi 2008;
Cover &amp; Thomas 2006), but the unification between the notion of
entropy in thermodynamics and Shannon-Kolmogorov information is very
incomplete apart from some very <em>ad hoc</em> insights
(Harremo&euml;s &amp; Tops&oslash;e 2008; Bais &amp; Farmer 2008).
Fredkin and Toffoli (1982) have proposed so-called billiard ball
computers to study reversible systems in thermodynamics (Durand-Lose
2002) (see the entry on
 <a href="../information-entropy/index.html">information processing and thermodynamic entropy</a>).
 Possible theoretical models could with high probability be
corroborated with feasible experiments (e.g., Joule&rsquo;s adiabatic
expansion, see Adriaans 2008). </p>

<p>
Questions that emerge are:</p>

<ul class="jfy">

<li>What is a computational process from a thermodynamical point of
view? </li>

<li> Can a thermodynamic theory of computing serve as a theory of
non-equilibrium dynamics? </li>

<li>Is the expressiveness of real numbers necessary for a physical
description of our universe? </li>
</ul>

<p>
These problems seem to be hard because 150 years of research in
thermodynamics still leaves us with a lot of conceptual unclarities in
the heart of the theory of thermodynamics itself (see entry on
 <a href="../time-thermo/index.html">thermodynamic asymmetry in time</a>).</p>
 
<p>
Real numbers are not accessible to us in finite computational
processes yet they do play a role in our analysis of thermodynamic
processes. The most elegant models of physical systems are based on
functions in continuous spaces. In such models almost all points in
space carry an infinite amount of information. Yet, the cornerstone of
thermodynamics is that a finite amount of space has finite entropy.
There is, on the basis of the theory of quantum information, no
fundamental reason to assume that the expressiveness of real numbers
is never used in nature itself on this level. This problem is related
to questions studied in philosophy of mathematics (an intuitionistic
versus a more platonic view). The issue is central in some of the more
philosophical discussions on the nature of computation and information
(Putnam 1988; Searle 1990). The problem is also related to the notion
of phase transitions in the description of nature (e.g.,
thermodynamics versus statistical mechanics) and to the idea of levels
of abstraction (Floridi 2002). </p>

<p>
In the past decade some progress has been made in the analysis of
these questions. A basic insight is that the interaction between time
and computational processes can be understood at an abstract
mathematical level, without the burden of some intended physical
application (Adriaans &amp; van Emde Boas 2011). Central is the
insight that deterministic programs do not generate new information.
Consequently deterministic computational models of physical systems
can never give an account of the growth of information or entropy in
nature:</p>

<div class="indent">

<p>
<em>Observation</em>: The Laplacian assumption that the
universe can be described as a deterministic computer is, given the
fundamental theorem of Adriaans and van Emde Boas (2011) and the
assumption that quantum physics as a essentially stochastic
description of the structure of our reality, incorrect. </p>
</div>

<p>
A statistical reduction of thermodynamics to a deterministic theory
like Newtonian physics leads to a notion of entropy that is
<em>fundamentally different</em> from the information processed by
deterministic computers. From this perspective the mathematical models
of thermodynamics, which are basically differential equations on
spaces of real numbers, seem to operate on a level that is not
expressive enough. More advanced mathematical models, taking in to
account quantum effects, might resolve some of the conceptual
difficulties. At a subatomic level nature seems to be inherently
probabilistic. If probabilistic quantum effects play a role in the
behavior of real billiard balls, then the debate whether entropy
increases in an abstract gas, made out of ideal balls, seems a bit
academic. There is reason to assume that stochastic phenomena at
quantum level are a source of probability at a macroscopic scale
(Albrecht &amp; Phillips 2014). From this perspective the universe is
a constant source of, literally, astronomical amounts of information
at any scale. </p>

<h3 id="LogiSemaInfo">6.6 Logic and Semantic Information</h3>

<p>
Logical and computational approaches to the understanding of
information both have their roots in the &ldquo;linguistic turn&rdquo;
that characterized the philosophical research in the beginning of the
twentieth century and the elementary research questions originate from
the work of Frege (1879, 1892, see the entry on
 <a href="../logic-information/index.html">logic and information</a>).
 The ambition to quantify information in <em>sets of true
sentences</em>, as apparent in the work of researchers like Popper,
Carnap, Solomonoff, Kolmogorov, Chaitin, Rissanen, Koppel,
Schmidthuber, Li, Vit&aacute;nyi and Hutter is an inherently semantic
research program. In fact, Shannon&rsquo;s theory of information is
the only modern approach that explicitly claims to be non-semantic.
More recent quantitative information measures like Kolmogorov
complexity (with its ambition to codify all scientific knowledge in
terms of a universal distribution) and quantum information (with its
concept of <em>observation</em> of physical systems) inherently assume
a semantic component. At the same time it is possible to develop
quantitative versions of semantic theories (see entry on
 <a href="../information-semantic/index.html">semantic conceptions of information</a>).
 </p>

<p>
The central intuition of algorithmic complexity theory that an
intension or meaning of an object can be a computation, was originally
formulated by Frege (1879, 1892). The expressions &ldquo;1 + 4&rdquo;
and &ldquo;2 + 3&rdquo; have the same extension (<em>Bedeutung</em>)
&ldquo;5&rdquo;, but a different intension (<em>Sinn</em>). In this
sense one mathematical object can have an infinity of different
meanings. There are opaque contexts in which such a distinction is
necessary. Consider the sentence &ldquo;John knows that \(\log_2 2^2 =
2\)&rdquo;. Clearly the fact that \(\log_2 2^2\) represents a specific
computation is relevant here. The sentence &ldquo;John knows that \(2
= 2\)&rdquo; seems to have a different meaning.</p>

<p>
Dunn (2001, 2008) has pointed out that the analysis of information in
logic is intricately related to the notions of intension and
extension. The distinction between intension and extension is already
anticipated in the 
 <a href="../port-royal-logic/index.html"><em>Port Royal Logic</em></a> (1662)
and the writings of Mill (1843),
Boole (1847) and Peirce (1868) but was systematically introduced in
logic by Frege (1879, 1892). In a modern sense the extension of a
predicate, say &ldquo;<i>X</i> is a bachelor&rdquo;, is simply the set
of bachelors in our domain. The intension is associated with the
meaning of the predicate and allows us to derive from the fact that
&ldquo;John is a bachelor&rdquo; the facts that &ldquo;John is
male&rdquo; and &ldquo;John is unmarried&rdquo;. It is clear that this
phenomenon has a relation with both the possible world interpretation
of modal operators and the notion of information. A bachelor is by
necessity also male, i.e., in every possible world in which John is a
bachelor he is also male, consequently: If someone gives me the
information that John is a bachelor I get the information that he is
male and unmarried for free. </p>

<p>
The possible world interpretation of modal operators (Kripke 1959) is
related to the notion of &ldquo;state description&rdquo; introduced by
Carnap (1947). A state description is a conjunction that contains
exactly one of each atomic sentence or its negation (see
 <a href="#SoloKolmChaiInfoLengProg">section 4.3</a>).
 The ambition to define a good probability measure for state
descriptions was one of the motivations for Solomonoff (1960, 1997) to
develop algorithmic information theory. From this perspective
Kolmogorov complexity, with its separation of data types (programs,
data, machines) and its focus on true sentences describing effects of
processes is basically a semantic theory. This is immediately clear if
we evaluate the expression:</p> 

\[U_j(\overline{T_i}x)= y\]

<p>
As is explained in
 <a href="#TuriMach">section 5.2.1</a>
 the expression \(U_j(\overline{T_i}x)\) denotes the result of the
emulation of the computation \(T_i(x)\) by \(U_j\) after reading the
self-delimiting description \(\overline{T_i}\) of machine \(T_j\).
This expression can be interpreted as a piece of <em>semantic
information</em> in the context of the <em>informational map</em> (See
entry on
 <a href="../information-semantic/index.html">semantic conceptions of information</a>)
 as follows: </p>

<ul class="jfy">

<li>The universal Turing machine \(U_j\) is a <strong>context</strong>
is which the computation takes place. It can be interpreted as a
<strong>possible computational world</strong> in a modal
interpretation of computational semantics.</li>

<li>The sequences of symbols \(\overline{T_i}x\) and <i>y</i> are
<strong>well-formed data</strong>. </li>

<li>The sequence \(\overline{T_i}\) is a self-delimiting
<strong>description of a program</strong> and it can be interpreted as
a piece of well-formed <strong>instructional data</strong>.</li>

<li>The sequence \(\overline{T_i}x\) is an <strong>intension</strong>.
The sequence <i>y</i> is the corresponding <strong>extension</strong>.
</li>

<li>The expression \(U_j(\overline{T_i}x)= y\) states the result of
the program \(\overline{T_i}x\) in world \(U_j\) is <i>y</i>. It is a
<strong>true sentence</strong>.</li>
</ul>

<p>
The logical structure of the sentence \(U_j(\overline{T_i}x)= y\) is
comparable to a true sentence like: </p>

<div class="indent">

<p>
In the context of empirical observations on planet earth, the bright
star you can see in the morning in the eastern sky is Venus </p>
</div>

<p>
<em>Mutatis mutandis</em> one could develop the following
interpretation: \(U_j\) can be seen as a context that, for instance,
codifies a <em>bias</em> for scientific observations on earth,
<i>y</i> is the <em>extension</em> Venus, \(\overline{T_i}x\) is the
<em>intension</em> &ldquo;the bright star you can see in the morning
in the eastern sky&rdquo;. The intension consists of \(T_i\), which
can be interpreted as some general astronomical observation routine
(e.g., instructional data), and <i>x</i> provides the well-formed data
that tells one where to look (bright star in the morning in the
eastern sky). </p>

<p>
This suggests a possible unification between more truth oriented
theories of information and computational approaches in terms of the
informational map presented in the entry of
 <a href="../information-semantic/index.html">semantic conceptions of information</a>.
 We delineate some research questions:</p>

<ul class="jfy">

<li>What is a good logical system (or set of systems) that formalizes
our intuitions of the relation between concepts like
&ldquo;knowing&rdquo;, &ldquo;believing&rdquo; and &ldquo;being
informed of&rdquo;. There are proposals by: Dretske (1981), van
Benthem (2006; van Benthem &amp; de Rooij 2003), Floridi (2003, 2011)
and others. A careful mapping of these concepts onto our current
landscape of known logics (structural, modal) might clarify the
strengths and weaknesses of different proposals. </li>

<li>It is unclear what the <em>specific difference</em> (in the
Aristotelian sense) is that separates <em>environmental data</em> from
other data, e.g., if one uses pebbles on a beach to count the number
of dolphins one has observed, then it might be impossible for the
uninformed passer by to judge whether this collection of stones is
environmental data or not.</li>

<li>The category of <em>instructional data</em> seems to be too narrow
since it pins us down on a specific interpretation of what computing
is. For the most part Turing equivalent computational paradigms are
not instructional, although one might defend the view that programs
for Turing machines are such data.</li>

<li>It is unclear how we can cope with the <em>ontological
duality</em> that is inherent to the self referential aspects of
Turing complete systems: Turing machines operate on data that <em>at
the same time</em> act as representations of programs, i.e.,
instructional <em>and</em> non-instructional. </li>

<li>It is unclear how a theory that defines information exclusively in
terms of true statements can deal with fundamental issues in quantum
physics. How can an inconsistent logical model in which Schrodinger's
cat is at the same time dead and alive contain any information in such
a theory? </li>
</ul>

<h3 id="MeanComp">6.7 Meaning and Computation</h3>

<p>
Ever since Descartes, the idea that the meaningful world, we perceive
around us, can be reduced to physical processes has been a predominant
theme in western philosophy. The corresponding philosophical
self-reflection in history neatly follows the technical developments
from: Is the human mind an automaton, to is the mind a Turing machine
and, eventually, is the mind a quantum computer? It is not the place
here to discuss these matters extensively, but the corresponding
problem in philosophy of information is relevant:</p>

<div class="indent">

<p>
<strong>Open problem:</strong> Can meaning be reduced to computation?
</p>
</div>

<p>
The question is interwoven with more general issues in philosophy and
its answer directly forces a choice between a more
<em>positivistic</em> or a more <em>hermeneutical</em> approach to
philosophy, with consequences for theory of knowledge, metaphysics,
aesthetics and ethics. It also effects direct practical decisions we
take on a daily basis. Should the actions of a medical doctor be
guided by evidence based medicine or by the notion of
<em>caritas</em>? Is a patient a conscious human being that wants to
lead a meaningful life, or is he ultimately just a system that needs
to be repaired? </p>

<p>
The idea that meaning is essentially a computational phenomenon may
seem extreme, but here are many discussions and theories in science,
philosophy and culture that implicitly assume such a view. In popular
culture, e.g., there is a remarkable collection of movies and books in
which we find evil computers that are conscious of themselves (2001,
<em>A Space Odyssey</em>), individuals that upload their consciousness
to a computer (1992, <em>The Lawnmower Man</em>), and fight battles in
virtual realities (1999, <em>The Matrix</em>). In philosophy the
position of Bostrom (2003), who defends the view that it is very
likely that we already live in a computer simulation, is illustrative.
There are many ways to argue the pros and cons of the reduction of
meaning to computation. We give an overview of possible arguments for
the two extreme positions: </p>

<ul>

<li>

<p>
<em>Meaning is an emergent aspect of computation</em>: Science is our
best effort to develop a valid objective theoretical description of
the universe based on intersubjectively verifiable repeated
observations. Science tells us that our reality at a small scale
consists of elementary particles whose behavior is described by exact
mathematical models. At an elementary level these particles interact
and exchange information. These processes are essentially
computational. At this most basic level of description there is no
room for a subjective notion of meaning. There is no reason to deny
that we as human being experience a meaningful world, but as such this
must be an emergent aspect of nature. At a fundamental level it does
not exist. We can describe our universe as a big quantum computer. We
can estimate the information storage content of our universe to be
\(10^{92}\) bits and the number of computational steps it made since
the big bang as \(10^{123}\) (Lloyd 2000; Lloyd &amp; Ng 2004). As
human beings we are just subsystems of the universe with an estimated
complexity of roughly \(10^{30}\) bits. It might be technically
impossible, but there seems to be no theoretical objection against the
idea that we can in principle construct an exact copy of a human
being, either as a direct physical copy or as a simulation in a
computer. Such an &ldquo;artificial&rdquo; person will experience a
meaningful world, but the experience will be emergent. </p> </li>

<li>

<p>
<em>Meaning is ontologically rooted in our individual experience of
the world and thus irreducible</em>: The reason scientific theories
eliminate most semantic aspects of our world, is caused by the very
nature of methodology of science itself. The essence of meaning and
the associated emotions is that they are rooted in our individual
experience of the world. By focusing on repeated observations of
similar events by different observers scientific methodology excludes
the possibility of an analysis of the concept of meaning <em>a
priori</em>. Empirical scientific methodology is valuable in the sense
that it allows us to abstract from the individual differences of
conscious observers, but there is no reason to reduce our ontology to
the phenomena studied by empirical science. Isolated individual events
and observations are by definition not open to experimental analysis
and this seems to be the point of demarcation between science and the
humanities. In disciplines like history, literature, visual art and
ethics we predominantly analyze individual events and individual
objects. The closer these are to our individual existence, the more
meaning they have for us. There is no reason to doubt the fact that
sentences like &ldquo;Guernica is a masterpiece that shows the
atrocities of war&rdquo; or &ldquo;McEnroe played such an inspired
match that he deserved to win&rdquo; uttered in the right context
convey meaningful information. The view that this information content
ultimately should be understood in terms of computational processes
seems too extreme to be viable. </p></li>
</ul>

<p>
Apart from that, a discipline like physics, that until recently
overlooked about 68% of the energy in the universe and 27% of the
matter, that has no unified theory of elementary forces and only
explains the fundamental aspects of our world in terms of mathematical
models that lack any intuitive foundation, for the moment does not
seem to converge to a model that could be an adequate basis for a
reductionistic metaphysics. </p>

<p>
As soon as one defines information in terms of true statements, some
meanings become computational and others lack that feature. In the
context of empirical science we can study groups of researchers that
aim at the construction of theories generalizing structural
information in data sets of repeated observations. Such processes of
theory construction and <em>intersubjective verification and
falsification</em> have an inherent computational component. In fact,
this notion of intersubjective verification seems an essential element
of mathematics. This is the main cause of the fact that central
questions of humanities are not open for quantitative analysis: We can
disagree on the question whether one painting is more beautiful than
the other, but not on the fact that there are two paintings. </p>

<p>
It is clear that computation as a conceptual model pays a role in many
scientific disciplines varying from cognition (Chater &amp;
Vit&aacute;nyi 2003), to biology (see entry on
 <a href="../information-biological/index.html">biological information</a>)
 and physics (Lloyd &amp; Ng 2004; Verlinde 2011, 2017). Extracting
meaningful models out of data sets by means of computation is the
driving force behind the Big Data revolution (Adriaans &amp; Zantinge
1996; Bell, Hey, &amp; Szalay 2009; Hey, Tansley, &amp; Tolle 2009).
Everything that multinationals like Google and Facebook
&ldquo;know&rdquo; about individuals is extracted from large data
bases by means of computational processes, and it cannot be denied
that this kind of &ldquo;knowledge&rdquo; has a considerable amount of
impact on society. The research question &ldquo;How can we construct
meaningful data out of large data sets by means of computation?&rdquo;
is a fundamental meta-problem of science in the twenty-first century
and as such part of philosophy of information, but there is no strict
necessity for a reductionistic view. </p>

<h2 id="Conc">7. Conclusion</h2>

<p>
The first domain that could benefit from philosophy of information is
of course philosophy itself. The concept of information potentially
has an impact on almost all philosophical main disciplines, ranging
from logic, theory of knowledge, to ontology and even ethics and
esthetics (see introduction above). Philosophy of science and
philosophy of information, with their interest in the problem of
induction and theory formation, probably both could benefit from
closer cooperation (see
 <a href="#PoppInfoDegrFals">4.1 Popper: Information as degree of falsifiability</a>).
 The concept of information plays an important role in the history of
philosophy that is not completely understood (see
 <a href="#HistTermConcInfo">2. History of the term and the concept of information</a>).
 </p>

<p>
As information has become a central issue in almost all of the
sciences and humanities this development will also impact
philosophical reflection in these areas. Archaeologists, linguists,
physicists, astronomers all deal with information. The first thing a
scientist has to do before he can formulate a theory is gathering
information. The application possibilities are abundant. Datamining
and the handling of extremely large data sets seems to be an essential
for almost every empirical discipline in the twenty-first century.</p>

<p>
In biology we have found out that information is essential for the
organization of life itself and for the propagation of complex
organisms (see entry on
 <a href="../information-biological/index.html">biological information</a>).
 One of the main problems is that current models do not explain the
complexity of life well. Valiant has started a research program that
studies evolution as a form of computational learning (Valiant 2009)
in order to explain this discrepancy. Aaronson (2013) has argued
explicitly for a closer cooperation between complexity theory and
philosophy.</p>

<p>
Until recently the general opinion was that the various notions of
information were more or less isolated but in recent years
considerable progress has been made in the understanding of the
relationship between these concepts. Cover and Thomas (2006), for
instance, see a perfect match between Kolmogorov complexity and
Shannon information. Similar observations have been made by
Gr&uuml;nwald and Vit&aacute;nyi (2008). Also the connections that
exist between the theory of thermodynamics and information theory have
been studied (Bais &amp; Farmer 2008; Harremo&euml;s &amp;
Tops&oslash;e 2008) and it is clear that the connections between
physics and information theory are much more elaborate than a mere
<em>ad hoc</em> similarity between the formal treatment of entropy and
information suggests (Gell-Mann &amp; Lloyd 2003; Verlinde (2011,
2017). Quantum computing is at this moment not developed to a point
where it is effectively more powerful than classical computing, but
this threshold might be passed in the coming years. From the point of
view of philosophy many conceptual problems of quantum physics and
information theory seem to merge into one field of related questions:
</p>

<ul class="jfy">

<li>What is the relation between information and computation?</li>

<li>Is computation in the real world fundamentally
non-deterministic?</li>

<li> What is the relation between symbol manipulation on a macroscopic
scale and the world of quantum physics? </li>

<li> What is a good model of quantum computing and how do we control
its power? </li>

<li> Is there information beyond the world of quanta?</li>
</ul>

<p>
The notion of information has become central in both our society and
in the sciences. Information technology plays a pivotal role in the
way we organize our lives. It also has become a basic category in the
sciences and the humanities. Philosophy of information, both as a
historical and a systematic discipline, offers a new perspective on
old philosophical problems and also suggests new research domains.</p>
</div>

<div id="bibliography">

<h2 id="Bib">Bibliography</h2>

<ul class="hanging">

<li>Aaronson, Scott, 2013, &ldquo;Why Philosophers Should Care About
Computational Complexity&rdquo;, in <em>Computability: Turing,
G&ouml;del, Church, and Beyond</em>, Brian Jack Copeland, Carl J.
Posy, and Oron Shagrir (eds.), Cambridge, MA: The MIT Press.
 [<a href="http://philsci-archive.pitt.edu/8748/" target="other">Aaronson 2013 preprint available online</a>]</li>
 
<li>Abramsky, Samson and Achim Jung, 1994, &ldquo;Domain
theory&rdquo;, in <em>Handbook of Logic in Computer Science (vol. 3):
Semantic Structure</em>, Samson Abramsky, Dov M. Gabbay, and Thomas S.
E. Maibaum (eds.),. Oxford University Press. pp. 1&ndash;168.</li>

<li>Adams, Fred and Jo&atilde;o Antonio de Moraes, 2016, &ldquo;Is
There a Philosophy of Information?&rdquo;, <em>Topoi</em>, 35(1):
161&ndash;171. doi:10.1007/s11245-014-9252-9</li>

<li>Adriaans, Pieter, 2007, &ldquo;Learning as Data
Compression&rdquo;, in <em>Computation and Logic in the Real
World</em>, S. Barry Cooper, Benedikt L&ouml;we, and Andrea Sorbi
(eds.), (Lecture Notes in Computer Science: Volume 4497), Berlin,
Heidelberg: Springer Berlin Heidelberg, 11&ndash;24.
doi:10.1007/978-3-540-73001-9_2</li>

<li>&ndash;&ndash;&ndash;, 2008, &ldquo;Between Order and Chaos: The
Quest for Meaningful Information&rdquo;, <em>Theory of Computing
Systems</em> (Special Issue: Computation and Logic in the Real World;
Guest Editors: S. Barry Cooper, Elvira Mayordomo and Andrea Sorbi),
45(4): 650&ndash;674. doi:10.1007/s00224-009-9173-y</li>

<li>Adriaans, Pieter and Peter van Emde Boas, 2011,
&ldquo;Computation, Information, and the Arrow of Time&rdquo;, in
<em>Computability in Context: Computation and Logic in the Real
World</em>, by S Barry Cooper and Andrea Sorbi (eds), London: Imperial
College Press, 1&ndash;17. doi:10.1142/9781848162778_0001</li>

<li>Adriaans, Pieter and Johan van Benthem, 2008a,
&ldquo;Introduction: Information Is What Information Does&rdquo;, in
Adriaans &amp; van Benthem 2008b: 3&ndash;26.
doi:10.1016/B978-0-444-51726-5.50006-6</li>

<li>&ndash;&ndash;&ndash; (eds.), 2008b, <em>Philosophy of
Information</em>, (Handbook of the Philosophy of Science 8),
Amsterdam: Elsevier. doi:10.1016/C2009-0-16481-4</li>

<li>Adriaans, Pieter and Paul M.B. Vit&aacute;nyi, 2009,
&ldquo;Approximation of the Two-Part MDL Code&rdquo;, <em>IEEE
Transactions on Information Theory</em>, 55(1): 444&ndash;457.
doi:10.1109/TIT.2008.2008152</li>

<li>Adriaans, Pieter and Dolf Zantinge, 1996, <em>Data Mining</em>,
Harlow, England: Addison-Wesley.</li>

<li>Agrawal, Manindra, Neeraj Kayal, and Nitin Saxena, 2004,
&ldquo;PRIMES Is in P&rdquo;, <em>Annals of Mathematics</em>, 160(2):
781&ndash;793. doi:10.4007/annals.2004.160.781</li>

<li>Albrecht, Andreas and Daniel Phillips, 2014, &ldquo;Origin of
Probabilities and Their Application to the Multiverse&rdquo;,
<em>Physical Review D</em>, 90(12): 123514. doi:10.1103/PhysRevD.90.
123514</li>

<li>Antunes, Lu&iacute;s and Lance Fortnow, 2003,
&ldquo;Sophistication Revisited&rdquo;, in <em>Proceedings of the 30th
International Colloquium on Automata, Languages and Programming</em>
(Lecture Notes in Computer Science: Volume 2719), Jos C. M. Baeten,
Jan Karel Lenstra, Joachim Parrow, and Gerhard J. Woeginger (eds.),
Berlin: Springer, pp. 267&ndash;277. doi:10.1007/3-540-45061-0_23</li>

<li>Antunes, Luis, Lance Fortnow, Dieter van Melkebeek, and N.V.
Vinodchandran, 2006, &ldquo;Computational Depth: Concept and
Applications&rdquo;, <em>Theoretical Computer Science</em>, 354(3):
391&ndash;404. doi:10.1016/j.tcs.2005.11.033</li>

<li>Aquinas, St. Thomas, 1265&ndash;1274, <em>Summa
Theologiae</em>.</li>

<li>Arbuthnot, John, 1692, <em>Of the Laws of Chance, or, a method of
Calculation of the Hazards of Game, Plainly demonstrated, And applied
to Games as present most in Use</em>, translation of Huygens&rsquo;
<em>De Ratiociniis in Ludo Aleae</em>, 1657.</li>

<li>Aristotle. <em>Aristotle in 23 Volumes</em>, Vols. 17, 18,
translated by Hugh Tredennick, Cambridge, MA: Harvard University
Press; London, William Heinemann Ltd. 1933, 1989.</li>

<li>Austen, Jane, 1815, <em>Emma</em>, London: Richard Bentley and
Son.</li>

<li>Bar-Hillel, Yehoshua and Rudolf Carnap, 1953, &ldquo;Semantic
Information&rdquo;, <em>The British Journal for the Philosophy of
Science</em>, 4(14): 147&ndash;157. doi:10.1093/bjps/IV.14.147</li>

<li>Bais, F. Alexander and J. Doyne Farmer, 2008, &ldquo;The Physics
of Information&rdquo;, Adriaans and van Benthem 2008b: 609&ndash;683.
doi:10.1016/B978-0-444-51726-5.50020-0</li>

<li>Barron, Andrew, Jorma Rissanen, and Bin Yu, 1998, &ldquo;The
Minimum Description Length Principle in Coding and Modeling&rdquo;,
<em>IEEE Transactions on Information Theory</em>, 44(6):
2743&ndash;2760. doi:10.1109/18.720554</li>

<li>Barwise, Jon and John Perry, 1983, <em>Situations and
Attitudes</em>, Cambridge, MA: MIT Press.</li>

<li>Bell, Gordon, Tony Hey, and Alex Szalay, 2009, &ldquo;Computer
Science: Beyond the Data Deluge&rdquo;, <em>Science</em>, 323(5919):
1297&ndash;1298. doi:10.1126/science.1170411</li>

<li>Bennett, C. H., 1988, &ldquo;Logical Depth and Physical
Complexity&rdquo;, in Rolf Herken (ed.), <em>The Universal Turing
Machine: A Half-Century Survey</em>, Oxford: Oxford University Press,
pp. 227&ndash;257.</li>

<li>Berkeley, George, 1732, <em>Alciphron: Or the Minute
Philosopher</em>, Edinburgh: Thomas Nelson, 1948&ndash;57.</li>

<li>Bernoulli, Danielis, 1738, <em>Hydrodynamica</em>, Argentorati:
sumptibus Johannis Reinholdi Dulseckeri.
 [<a href="http://doi.org/10.3931/e-rara-3911" target="other">Bernoulli 1738 available online</a>]</li>
 
<li>Birkhoff, George David, 1950, <em>Collected Mathematical
Papers</em>, New York: American Mathematical Society.</li>

<li>Bloem, Peter, Steven de Rooij, and Pieter Adriaans, 2015,
&ldquo;Two Problems for Sophistication&rdquo;, in <em>Algorithmic
Learning Theory</em>, (Lecture Notes in Computer Science 9355),
Kamalika Chaudhuri, Claudio Gentile, and Sandra Zilles (eds.), Cham:
Springer International Publishing, 379&ndash;394.
doi:10.1007/978-3-319-24486-0_25</li>

<li>Boltzmann, Ludwig, 1866, &ldquo;&Uuml;ber die Mechanische
Bedeutung des Zweiten Hauptsatzes der W&auml;rmetheorie&rdquo;,
<em>Wiener Berichte</em>, 53: 195&ndash;220.</li>

<li>Boole, George, 1847, <em>Mathematical Analysis of Logic: Being an
Essay towards a Calculus of Deductive Reasoning</em>, Cambridge:
Macmillan, Barclay, &amp; Macmillan.
 [<a href="http://archive.org/details/mathematicalanal00booluoft" target="other">Boole 1847 available online</a>].</li>
 
<li>&ndash;&ndash;&ndash;, 1854, <em>An Investigation of the Laws of
Thought: On which are Founded the Mathematical Theories of Logic and
Probabilities</em>, London: Walton and Maberly.</li>

<li>Bostrom, Nick, 2003, &ldquo;Are We Living in a Computer
Simulation?&rdquo;, <em>The Philosophical Quarterly</em>, 53(211):
243&ndash;255. doi:10.1111/1467-9213.00309</li>

<li>Bott, R. and J. Milnor, 1958, &ldquo;On the Parallelizability of
the Spheres&rdquo;, <em>Bulletin of the American Mathematical
Society</em>, 64(3): 87&ndash;89.
doi:10.1090/S0002-9904-1958-10166-4</li>

<li>Bovens, Luc and Stephan Hartmann, 2003, <em>Bayesian
Epistemology</em>, Oxford: Oxford University Press.
doi:10.1093/0199269750.001.0001</li>

<li>Brenner, Joseph E., 2008, <em>Logic in Reality</em>, Dordrecht:
Springer Netherlands. doi:10.1007/978-1-4020-8375-4</li>

<li>Briggs, Henry, 1624, <em>Arithmetica Logarithmica</em>, London:
Gulielmus Iones. </li>

<li>Capurro, Rafael, 1978, <em>Information. Ein Beitrag zur
etymologischen und ideengeschichtlichen Begr&uuml;ndung des
Informationsbegriffs</em> (Information: A contribution to the
foundation of the concept of information based on its etymology and in
the history of ideas), Munich, Germany: Saur.
 [<a href="http://www.capurro.de/info.html" target="other">Capurro 1978 available online</a>].</li>
 
<li>&ndash;&ndash;&ndash;, 2009, &ldquo;Past, Present, and Future of
the Concept of Information&rdquo;, <em>TripleC: Communication,
Capitalism &amp; Critique</em>, 7(2): 125&ndash;141.
doi:10.31269/triplec.v7i2.113</li>

<li>Capurro, Rafael and Birger Hj&oslash;rland, 2003, &ldquo;The
Concept of Information&rdquo;, in Blaise Cronin (ed.), <em>Annual
Review of Information Science and Technology (ARIST)</em>, 37:
343&ndash;411 (Chapter 8). doi:10.1002/aris.1440370109</li>

<li>Capurro, Rafael and John Holgate (eds.), 2011, <em>Messages and
Messengers: Angeletics as an Approach to the Phenomenology of
Communication</em> (<em>Von Boten Und Botschaften</em>,
(Schriftenreihe Des International Center for Information Ethics 5),
M&uuml;nchen: Fink.</li>

<li>Carnap, Rudolf, 1928, <em>Scheinprobleme in der Philosophie</em>
(Pseudoproblems of Philosophy), Berlin: Weltkreis-Verlag.</li>

<li>&ndash;&ndash;&ndash;, 1945, &ldquo;The Two Concepts of
Probability: The Problem of Probability&rdquo;, <em>Philosophy and
Phenomenological Research</em>, 5(4): 513&ndash;532.
doi:10.2307/2102817</li>

<li>&ndash;&ndash;&ndash;, 1947, <em>Meaning and Necessity</em>,
Chicago: The University of Chicago Press.</li>

<li>&ndash;&ndash;&ndash;, 1950, <em>Logical Foundations of
Probability</em>, Chicago: The University of Chicago Press.</li>

<li>Chaitin, Gregory J., 1969, &ldquo;On the Length of Programs for
Computing Finite Binary Sequences: Statistical Considerations&rdquo;,
<em>Journal of the ACM</em>, 16(1): 145&ndash;159.
doi:10.1145/321495.321506</li>

<li>&ndash;&ndash;&ndash;, 1987, <em>Algorithmic Information
Theory</em>, Cambridge: Cambridge University Press.
doi:10.1017/CBO9780511608858</li>

<li>Chater, Nick and Paul Vit&aacute;nyi, 2003, &ldquo;Simplicity: A
Unifying Principle in Cognitive Science?&rdquo;, <em>Trends in
Cognitive Sciences</em>, 7(1): 19&ndash;22.
doi:10.1016/S1364-6613(02)00005-0</li>

<li>Church, Alonzo, 1936, &ldquo;An Unsolvable Problem of Elementary
Number Theory&rdquo;, <em>American Journal of Mathematics</em> 58(2):
345&ndash;363. doi:10.2307/2371045 </li>

<li>Cilibrasi, Rudi and Paul M.B. Vitanyi, 2005, &ldquo;Clustering by
Compression&rdquo;, <em>IEEE Transactions on Information Theory</em>,
51(4): 1523&ndash;1545. doi:10.1109/TIT.2005.844059</li>

<li>Clausius, R., 1850, &ldquo;Ueber die bewegende Kraft der
W&auml;rme und die Gesetze, welche sich daraus f&uuml;r die
W&auml;rmelehre selbst ableiten lassen&rdquo;, <em>Annalen der Physik
und Chemie</em>, 155(3): 368&ndash;397.
doi:10.1002/andp.18501550306</li>

<li>Conan Doyle, Arthur, 1892, &ldquo;The Adventure of the Noble
Bachelor&rdquo;, in <em>The Adventures of Sherlock Holmes</em>,
London: George Newnes Ltd, story 10.</li>

<li>Cover, Thomas M. and Joy A. Thomas, 2006, <em>Elements of
Information Theory</em>, second edition, New York: John Wiley &amp;
Sons. </li>

<li>Crawford, James M. and Larry D. Auton, 1993, &ldquo;Experimental
Results on the Crossover Point in Satisfiability Problems&rdquo;,
<em>Proceedings of the Eleventh National Conference on Artificial
Intelligence</em>, AAAI Press, pp. 21&ndash;27.
 [<a href="https://www.aaai.org/Library/AAAI/1993/aaai93-004.php" target="other">Crawford &amp; Auton 1993 available online</a>]</li>
 
<li>Crutchfield, James P. and Karl Young, 1989, &ldquo;Inferring
Statistical Complexity&rdquo;, <em>Physical Review Letters</em>,
63(2): 105&ndash;108. doi:10.1103/PhysRevLett.63.105</li>

<li>&ndash;&ndash;&ndash;, 1990, &ldquo;Computation at the Onset of
Chaos&rdquo;, in <em>Entropy, Complexity, and the Physics of
Information</em>, W. Zurek, editor, SFI Studies in the Sciences of
Complexity, VIII, Reading, MA: Addison-Wesley, pp. 223&ndash;269.
 [<a href="http://csc.ucdavis.edu/~cmg/compmech/pubs/CompOnsetTitlePage.htm" target="other">Crutchfield &amp; Young 1990 available online</a>]</li>
 
<li>D&rsquo;Alfonso, Simon, 2012, &ldquo;Towards a Framework for
Semantic Information&rdquo;, Ph.D. Thesis, Department of Philosophy,
School of Historical and Philosophical Studies, The University of
Melbourne.
 <a href="http://minerva-access.unimelb.edu.au/handle/11343/37818" target="other">D&rsquo;Alfonso 2012 available online</a></li>
 
<li>Davis, Martin, 2006, &ldquo;Why There Is No Such Discipline as
Hypercomputation&rdquo;, <em>Applied Mathematics and Computation</em>,
178(1): 4&ndash;7. doi:10.1016/j.amc.2005.09.066</li>

<li>Defoe, Daniel, 1719, <em>The Life and Strange Surprising
Adventures of Robinson Crusoe of York, Mariner: who lived Eight and
Twenty Years, all alone in an uninhabited Island on the coast of
America, near the Mouth of the Great River of Oroonoque; Having been
cast on Shore by Shipwreck, wherein all the Men perished but himself.
With An Account how he was at last as strangely deliver&rsquo;d by
Pirates. Written by Himself</em>, London: W. Taylor.
 [<a href="http://www.pierre-marteau.com/editions/1719-robinson-crusoe.html" target="other">Defoe 1719 available online</a>]</li>
 
<li>De Leo, Stefano, 1996, &ldquo;Quaternions and Special
Relativity&rdquo;, <em>Journal of Mathematical Physics</em>, 37(6):
2955&ndash;2968. doi:10.1063/1.531548</li>

<li>Dershowitz, Nachum and Yuri Gurevich, 2008, &ldquo;A Natural
Axiomatization of Computability and Proof of Church&rsquo;s
Thesis&rdquo;, <em>Bulletin of Symbolic Logic</em>, 14(3):
299&ndash;350. doi:10.2178/bsl/1231081370</li>

<li>Descartes, Ren&eacute;, 1641, <em>Meditationes de Prima
Philosophia</em> (Meditations on First Philosophy), Paris. </li>

<li>&ndash;&ndash;&ndash;, 1647, <em>Discours de la
M&eacute;thode</em> (Discourse on Method), Leiden.</li>

<li>Devlin, Keith and Duska Rosenberg, 2008, &ldquo;Information in the
Study of Human Interaction&rdquo;, Adriaans and van Benthem 2008b:
685&ndash;709. doi:10.1016/B978-0-444-51726-5.50021-2</li>

<li>Dictionnaire du Moyen Fran&ccedil;ais (1330&ndash;1500), 2015,
&ldquo;Information&rdquo;, in <em>Dictionnaire du Moyen
Fran&ccedil;ais (1330&ndash;1500)</em>, volume 16, 313&ndash;315.
 [<a href="http://www.atilf.fr/dmf/" target="other">Dictionnaire du Moyen Fran&ccedil;ais available online</a>]</li>
 
<li>Domingos, Pedro, 1998, &ldquo;Occam&rsquo;s Two Razors: The Sharp
and the Blunt&rdquo;, in <em>Proceedings of the Fourth International
Conference on Knowledge Discovery and Data Mining</em> (KDD&ndash;98),
New York: AAAI Press, pp. 37&ndash;43.
 [<a href="https://aaai.org/Library/KDD/1998/kdd98-006.php" target="other">Domingos 1998 available online</a>]</li>
 
<li>Downey, Rodney G. and Denis R. Hirschfeldt, 2010, <em>Algorithmic
Randomness and Complexity</em>, (Theory and Applications of
Computability), New York: Springer New York.
doi:10.1007/978-0-387-68441-3</li>

<li>Dretske, Fred, 1981, <em>Knowledge and the Flow of
Information</em>, Cambridge, MA: The MIT Press.</li>

<li>Dufort, Paul A. and Charles J. Lumsden, 1994, &ldquo;The
Complexity and Entropy of Turing Machines&rdquo;, in <em>Proceedings
Workshop on Physics and Computation</em>. PhysComp &rsquo;94, Dallas,
TX: IEEE Computer Society Press, 227&ndash;232.
doi:10.1109/PHYCMP.1994.363677</li>

<li>Dunn, Jon Michael, 2001, &ldquo;The Concept of Information and the
Development of Modern Logic&rdquo;, in <em>Zwischen traditioneller und
moderner Logik: Nichtklassiche Ansatze</em> (<em>Non-classical
Approaches in the Transition from Traditional to Modern Logic</em>),
Werner Stelzner and Manfred St&ouml;ckler (eds.), Paderborn: Mentis,
423&ndash;447.</li>

<li>&ndash;&ndash;&ndash;, 2008, &ldquo;Information in Computer
Science&rdquo;, in Adriaans and van Benthem 2008b: 581&ndash;608.
doi:10.1016/B978-0-444-51726-5.50019-4</li>

<li>Dijksterhuis, E. J., 1986, <em>The Mechanization of the World
Picture: Pythagoras to Newton</em>, Princeton, NJ: Princeton
University Press.</li>

<li>Duns Scotus, John [1265/66&ndash;1308 CE], <em>Opera Omnia</em>
(The Wadding edition), Luke Wadding (ed.), Lyon, 1639; reprinted
Hildesheim: Georg Olms Verlagsbuchhandlung, 1968.</li>

<li>Durand-Lose, J&eacute;r&ocirc;me, 2002, &ldquo;Computing Inside
the Billiard Ball Model&rdquo;, in <em>Collision-Based Computing</em>,
Andrew Adamatzky (ed.), London: Springer London, 135&ndash;160.
doi:10.1007/978-1-4471-0129-1_6</li>

<li>Edwards, Paul, 1967, <em>The Encyclopedia of Philosophy</em>, 8
volumes, New York: Macmillan Publishing Company.</li>

<li>Fayyad, Usama, Gregory Piatetsky-Shapiro, and Padhraic Smyth,
1996, &ldquo;From Data Mining to Knowledge Discovery in
Databases&rdquo;, <em>AI Magazine</em>, 17(3): 37&ndash;37.</li>

<li>Fisher, R. A., 1925, &ldquo;Theory of Statistical
Estimation&rdquo;, <em>Mathematical Proceedings of the Cambridge
Philosophical Society</em>, 22(05): 700&ndash;725.
doi:10.1017/S0305004100009580</li>

<li>Floridi, Luciano, 1999, &ldquo;Information Ethics: On the
Philosophical Foundation of Computer Ethics&rdquo;, <em>Ethics and
Information Technology</em>, 1(1): 33&ndash;52.
doi:10.1023/A:1010018611096</li>

<li>&ndash;&ndash;&ndash;, 2002, &ldquo;What Is the Philosophy of
Information?&rdquo; <em>Metaphilosophy</em>, 33(1&ndash;2):
123&ndash;145. doi:10.1111/1467-9973.00221</li>

<li>&ndash;&ndash;&ndash; (ed.), 2003, <em>The Blackwell Guide to the
Philosophy of Computing and Information</em>, Oxford: Blackwell.
doi:10.1002/9780470757017</li>

<li>&ndash;&ndash;&ndash;, 2010, &ldquo;The Philosophy of Information
as a Conceptual Framework&rdquo;, <em>Knowledge, Technology &amp;
Policy</em>, 23(1&ndash;2): 253&ndash;281.
doi:10.1007/s12130-010-9112-x</li>

<li>&ndash;&ndash;&ndash;, 2011, <em>The Philosophy of
Information</em>, Oxford: Oxford University Press.
doi:10.1093/acprof:oso/9780199232383.001.0001</li>

<li>Fredkin, Edward and Tommaso Toffoli, 1982, &ldquo;Conservative
Logic&rdquo;, <em>International Journal of Theoretical Physics</em>,
21(3&ndash;4): 219&ndash;253. doi:10.1007/BF01857727</li>

<li>Frege, Gottlob, 1879, <em>Begriffsschrift: eine der arithmetischen
nachgebildete Formelsprache des reinen Denkens</em>, Halle.</li>

<li>&ndash;&ndash;&ndash;, 1892, &ldquo;&Uuml;ber Sinn und
Bedeutung&rdquo;, <em>Zeitschrift f&uuml;r Philosophie und
philosophische Kritik</em>, NF 100.</li>

<li>Furey, C., 2015, &ldquo;Charge Quantization from a Number
Operator&rdquo;, <em>Physics Letters B</em>, 742(March):
195&ndash;199. doi:10.1016/j.physletb.2015.01.023</li>

<li>Galileo Galilei, 1623 [1960], <em>Il Saggiatore</em> (in Italian),
Rome; translated as <em>The Assayer</em>, by Stillman Drake and C. D.
O&rsquo;Malley, in <em>The Controversy on the Comets of 1618</em>,
Philadelphia: University of Pennsylvania Press, 1960,
151&ndash;336.</li>

<li>Garey, Michael R. and David S. Johnson, 1979, <em>Computers and
Intractability: A Guide to the Theory of NP-Completeness</em>, (A
Series of Books in the Mathematical Sciences), San Francisco: W. H.
Freeman.</li>

<li>Gell-Mann, Murray and Seth Lloyd, 2003, &ldquo;Effective
Computing&rdquo;. SFI Working Paper 03-12-068, Santa Fe, NM: Santa
Fe Institute.
 [<a href="https://www.santafe.edu/research/results/working-papers/effective-complexity" target="other">Gell-Mann &amp; Lloyd 2003 available online</a>]</li>
 
<li>Gibbs, J. Willard, 1906, <em>The Scientific Papers of J. Willard
Gibbs in Two Volumes</em>, 1. Longmans, Green, and Co.</li>

<li>Godefroy, Fr&eacute;d&eacute;ric G., 1881, <em>Dictionnaire de
l&rsquo;ancienne langue fran&ccedil;aise et de tous ses dialectes du
9e au 15e si&egrave;cle</em>, Paris: F. Vieweg.</li>

<li>G&ouml;del, Kurt, 1931, &ldquo;&Uuml;ber formal unentscheidbare
S&auml;tze der Principia Mathematica und verwandter Systeme I&rdquo;,
<em>Monatshefte f&uuml;r Mathematik und Physik</em>, 38&ndash;38(1):
173&ndash;198. doi:10.1007/BF01700692</li>

<li>Goodstein, R. L., 1957, &ldquo;The Definition of Number&rdquo;,
<em>The Mathematical Gazette</em>, 41(337): 180&ndash;186.
doi:10.2307/3609188</li>

<li>Gr&uuml;nwald, Peter D., 2007, <em>The Minimum Description Length
Principle</em>, Cambridge, MA: MIT Press.</li>

<li>Gr&uuml;nwald, Peter D. and Paul M.B. Vit&aacute;nyi, 2008,
&ldquo;Algorithmic Information Theory&rdquo;, in Adriaans and van
Benthem 2008b: 281&ndash;317.
doi:10.1016/B978-0-444-51726-5.50013-3</li>

<li>Groot, Adrianus Dingeman de, 1961 [1969], <em>Methodology:
Foundations of Inference and Research in the Behavioral Sciences</em>
(<em>Methodologie: grondslagen van onderzoek en denken in de
gedragswetenschappen</em>), The Hague: Mouton. </li>

<li>Hamkins, J., and Lewis, A., 2000, <em>Infinite time Turing machines. Journal of Symbolic Logic</em>, 65(2), 567-604. doi:10.2307/2586556
</li>

<li>Harremo&euml;s, Peter and Flemming Tops&oslash;e, 2008, &ldquo;The
Quantitative Theory of Information&rdquo;, in Adriaans and van Benthem
2008b: 171&ndash;216. doi:10.1016/B978-0-444-51726-5.50011-X</li>

<li>Hartley, R.V.L., 1928, &ldquo;Transmission of Information&rdquo;,
<em>Bell System Technical Journal</em>, 7(3): 535&ndash;563.
doi:10.1002/j.1538-7305.1928.tb01236.x</li>

<li>Hazard, Paul, 1935, <em>La Crise de La Conscience
Europ&eacute;enne (1680&ndash;1715)</em>, Paris: Boivin.</li>

<li>Hey, Anthony J. G., Stewart Tansley, and Kristin Tolle (eds.),

2009, <em>The Fourth Paradigm: Data-Intensive Scientific
Discovery</em>, Redmond, WA: Microsoft Research.
 [<a href="https://www.microsoft.com/en-us/research/publication/fourth-paradigm-data-intensive-scientific-discovery/" target="other">Hey et al. 2009 available online</a>]</li>
 
<li>Hintikka, Jaakko, 1962, <em>Knowledge and Belief: An Introduction
to the Logic of the Two Notions</em>, (Contemporary Philosophy),
Ithaca, NY: Cornell University Press.</li>

<li>&ndash;&ndash;&ndash;, 1973, <em>Logic, Language Games and
Information: Kantian Themes in the Philosophy of Logic</em>, Oxford:
Clarendon Press.</li>

<li>Hume, David, 1739&ndash;40, <em>A Treatise of Human Nature</em>.
Reprinted, L.A. Selby-Bigge (ed.), Oxford: Clarendon Press, 1896.
 [<a href="http://oll.libertyfund.org/titles/hume-a-treatise-of-human-nature" target="other">Hume 1739&ndash;40 [1896] available online</a>]</li>
 
<li>&ndash;&ndash;&ndash;, 1748, <em>An Enquiry concerning Human
Understanding</em>. Reprinted in <em>Enquiries Concerning the Human
Understanding and Concerning the Principles of Morals</em>, 1777 which
was reprinted, L.A. Selby-Bigge (ed.), Oxford: Clarendon Press, 1888
(second edition 1902).
 [<a href="http://oll.libertyfund.org/titles/hume-enquiries-concerning-the-human-understanding-and-concerning-the-principles-of-morals" target="other">Hume 1748 [1902] available online</a>]</li>
 
<li>Hutter, Marcus, 2005, <em>Universal Artificial Intellegence:
Sequential Decisions Based on Algorithmic Probability</em>, (Texts in
Theoretical Computer Science, an EATCS Series), Berlin, Heidelberg:
Springer Berlin Heidelberg. doi:10.1007/b138233</li>

<li>&ndash;&ndash;&ndash;, 2007a, &ldquo;On Universal Prediction and
Bayesian Confirmation&rdquo;, <em>Theoretical Computer Science</em>,
384(1): 33&ndash;48. doi:10.1016/j.tcs.2007.05.016</li>

<li>&ndash;&ndash;&ndash;, 2007b, &ldquo;Algorithmic Information
Theory: a brief non-technical guide to the field&rdquo;,
<em>Scholarpedia</em>, 2(3): art. 2519.
doi:10.4249/scholarpedia.2519</li>

<li>&ndash;&ndash;&ndash;, 2010, &ldquo;A Complete Theory of
Everything (will be subjective)&rdquo;, <em>Algorithms</em>, 3(4):
329&ndash;350. doi:10.3390/a3040329</li>

<li>Hutter, Marcus, John W. Lloyd, Kee Siong Ng, and William T.B.
Uther, 2013, &ldquo;Probabilities on Sentences in an Expressive
Logic&rdquo;, <em>Journal of Applied Logic</em>, special issue:
<em>Combining Probability and Logic: Papers from Progic 2011</em>,
Jeffrey Helzner (ed.), 11(4): 386&ndash;420.
doi:10.1016/j.jal.2013.03.003. </li>

<li>Ibn Tufail, <em>Hayy ibn Yaqdhan</em>, translated as
<em>Philosophus Autodidactus</em>, published by Edward Pococke the
Younger in 1671.</li>

<li>Kahn, David, 1967, <em>The Code-Breakers, The Comprehensive
History of Secret Communication from Ancient Times to the
Internet</em>, New York: Scribner.</li>

<li>Kant, Immanuel, 1781, <em>Kritik der reinen Vernunft</em>
(Critique of Pure Reason), Germany. </li>

<li>Kervaire, Michel A., 1958, &ldquo;Non-Parallelizability of the
n-Sphere for n &gt; 7&rdquo;, <em>Proceedings of the National Academy
of Sciences of the United States of America</em>, 44(3):
280&ndash;283. doi:10.1073/pnas.44.3.280</li>

<li>al-Khw&#257;rizm&#299;, Mu&#7717;ammad ibn M&#363;s&#257;, ca. 820
CE, <em>Hisab al-jabr w&rsquo;al-muqabala, Kitab al-Jabr
wa-l-Muqabala</em> (<em>The Compendious Book on Calculation by
Completion and Balancing</em>), Translated by Frederic Rosen, London:
Murray, 1831.
 [<a href="http://www.wilbourhall.org/index.html#algebra" target="other">al-Khwarizmi translation available online</a>]</li>
 
<li>Kolmogorov, A.N., 1965, &ldquo;Three Approaches to the
Quantitative Definition of Information&rdquo;, <em>Problems of
Information Transmission</em>, 1(1): 1&ndash;7. Reprinted 1968 in
<em>International Journal of Computer Mathematics</em>, 2(1&ndash;4):
157&ndash;168. doi:10.1080/00207166808803030</li>

<li>Koppel, Moshe, 1987, &ldquo;Complexity, Depth, and
Sophistication&rdquo;, <em>Complex Systems</em>, 1(6):
1087&ndash;1091.
 [<a href="https://www.complex-systems.com/abstracts/v01_i06_a04/" target="other">Koppel 1987 available online</a>]</li>
 
<li>Kripke, Saul A., 1959, &ldquo;A Completeness Theorem in Modal
Logic&rdquo;, <em>The Journal of Symbolic Logic</em>, 24(1):
1&ndash;14. doi:10.2307/2964568</li>

<li>&ndash;&ndash;&ndash;, 1971, &ldquo;Identity and Necessity&rdquo;,
in Milton K. Munitz (ed.), <em>Identity and Individuation</em>, New
York: New York University Press, pp. 135-164.</li>

<li>Kuipers, Theo A.F. (ed.), 2007a, <em>General Philosophy of
Science: Focal Issues</em>, Amsterdam: Elsevier Science
Publishers.</li>

<li>&ndash;&ndash;&ndash;, 2007b, &ldquo;Explanation in Philosophy of
Science&rdquo;, in Kuipers 2007a.</li>

<li>Laplace, Pierre Simon, Marquis de, 1814 [1902], <em>A
Philosophical Essay on Probabilities</em>, F.W. Truscott and F.L.
Emory (trans.), New York: J. Wiley; London: Chapman &amp; Hall.</li>

<li>Langton, Chris G., 1990, &ldquo;Computation at the Edge of Chaos:
Phase Transitions and Emergent Computation&rdquo;, <em>Physica D:
Nonlinear Phenomena</em>, 42(1&ndash;3): 12&ndash;37.
doi:10.1016/0167-2789(90)90064-V</li>

<li>Lenski, Wolfgang, 2010, &ldquo;Information: A Conceptual
Investigation&rdquo;, <em>Information 2010</em>, 1(2): 74&ndash;118.
doi:10.3390/info1020074</li>

<li>Levin, Leonid A., 1973, &ldquo;Universal Sequential Search
Problems&rdquo;, <em>Problems of Information Transmission</em>, 9(3):
265&ndash;266.</li>

<li>&ndash;&ndash;&ndash;,1974, &ldquo;Laws of Information
Conservation (Non-Growth) and Aspects of the Foundation of Probability
Theory&rdquo;, <em>Problems of Information Transmission</em>, 10(3):
206&ndash;210.</li>

<li>&ndash;&ndash;&ndash;, 1984, &ldquo;Randomness Conservation
Inequalities; Information and Independence in Mathematical
Theories&rdquo;, <em>Information and Control</em>, 61(1): 15&ndash;37.
doi:10.1016/S0019-9958(84)80060-1</li>

<li>Li, Ming and Paul Vit&aacute;nyi, 2019, <em>An Introduction to
Kolmogorov Complexity and Its Applications</em>, (Texts in Computer
Science), New York: Springer New York.
doi:10.1007/978-0-387-49820-1</li>

<li>Lloyd, Seth, 2000, &ldquo;Ultimate Physical Limits to
Computation&rdquo;, <em>Nature</em>, 406(6799): 1047&ndash;1054.
doi:10.1038/35023282</li>

<li>Lloyd, Seth and Y. Jack Ng, 2004, &ldquo;Black Hole
Computers&rdquo;, <em>Scientific American</em>, 291(5): 52&ndash;61.
doi:10.1038/scientificamerican1104-52</li>

<li>Locke, John, 1689, <em>An Essay Concerning Human
Understanding</em>, J. W. Yolton (ed.), London: Dent; New York:
Dutton, 1961.
 [<a href="https://oll.libertyfund.org/titles/locke-the-works-of-john-locke-in-nine-volumes" target="other">Locke 1689 available online</a>]</li>

<li>Maat, Jaap, 2004, <em>Philosophical Languages in the Seventeenth Century: Dalgarno, Wilkins, Leibniz</em>, The New Synthese Historical Library (Book 54), Springer.
</li>
 
<li>McAllister, James W., 2003, &ldquo;Effective Complexity as a
Measure of Information Content&rdquo;, <em>Philosophy of Science</em>,
70(2): 302&ndash;307. doi:10.1086/375469</li>

<li>Mill, John Stuart, 1843, <em>A System of Logic</em>, London.</li>

<li>Montague, Richard, 2008, &ldquo;Universal Grammar&rdquo;,
<em>Theoria</em>, 36(3): 373&ndash;398.
doi:10.1111/j.1755-2567.1970.tb00434.x</li>

<li>Mugur-Sch&auml;chter, Mioara, 2003, &ldquo;Quantum Mechanics
Versus a Method of Relativized Conceptualization&rdquo;, in
<em>Quantum Mechanics, Mathematics, Cognition and Action</em>, Mioara
Mugur-Sch&auml;chter and Alwyn van der Merwe (eds.), Dordrecht:
Springer Netherlands, 109&ndash;307. doi:10.1007/0-306-48144-8_7</li>

<li>Napier, John, 1614, <em>Mirifici Logarithmorum Canonis
Descriptio</em> (<em>The Description of the Wonderful Canon of
Logarithms</em>), Edinburgh: Andre Hart. Translated and annotated by
Ian Bruce, www.17centurymaths.com.
 [<a href="http://www.17centurymaths.com/contents/napiercontents.html" target="other">Napier 1614 [Bruce translation] available online</a>].</li>
 
<li>Nielsen, Michael A. and Isaac L. Chuang, 2000, <em>Quantum
Computation and Quantum Information</em>, Cambridge: Cambridge
University Press.</li>

<li>Nies, Andr&eacute;, 2009, <em>Computability and Randomness</em>,
Oxford: Oxford University Press.
doi:10.1093/acprof:oso/9780199230761.001.0001</li>

<li>Nyquist, H., 1924, &ldquo;Certain Factors Affecting Telegraph
Speed&rdquo;, <em>Bell System Technical Journal</em>, 3(2):
324&ndash;346. doi:10.1002/j.1538-7305.1924.tb01361.x</li>

<li>Ong, Walter J., 1958, <em>Ramus, Method, and the Decay of
Dialogue, From the Art of Discourse to the Art of Reason</em>,
Cambridge MA: Harvard University Press.</li>

<li>Parikh, Rohit and Ramaswamy Ramanujam, 2003, &ldquo;A Knowledge
Based Semantics of Messages&rdquo;, <em>Journal of Logic, Language and
Information</em>, 12(4): 453&ndash;467.
doi:10.1023/A:1025007018583</li>

<li>Peirce, Charles S., 1868, &ldquo;Upon Logical Comprehension and
Extension&rdquo;, <em>Proceedings of the American Academy of Arts and
Sciences</em>, 7: 416&ndash;432. doi:10.2307/20179572</li>

<li>&ndash;&ndash;&ndash;, 1886 [1993], &ldquo; Letter Peirce to A.
Marquand&rdquo;, Reprinted in <em>Writings of Charles S. Peirce: A
Chronological Edition, Volume 5: 1884&ndash;1886</em>, Indianapolis:
Indiana University Press, pp. 424&ndash;427. See also Arthur W. Burks,
1978, &ldquo;Book Review: &lsquo;The New Elements of
Mathematics&rsquo; by Charles S. Peirce, Carolyn Eisele
(editor)&rdquo;, <em>Bulletin of the American Mathematical
Society</em>, 84(5): 913&ndash;919.
doi:10.1090/S0002-9904-1978-14533-9</li>

<li>Popper, Karl, 1934, <em>The Logic of Scientific Discovery</em>,
(<em>Logik der Forschung</em>), English translation 1959, London:
Hutchison. Reprinted 1977.</li>

<li>Putnam, Hilary, 1988, <em>Representation and reality</em>,
Cambridge, MA: The MIT Press.</li>

<li>Quine, W.V.O., 1951, &ldquo;Main Trends in Recent
Philosophy: Two Dogmas of Empiricism&rdquo;, <em>The Philosophical
Review</em>, 60(1): 20&ndash;43. Reprinted in his 1953 <em>From a
Logical Point of View</em>, Cambridge, MA: Harvard University Press.
doi:10.2307/2181906</li>

<li>Rathmanner, Samuel and Marcus Hutter, 2011, &ldquo;A Philosophical
Treatise of Universal Induction&rdquo;, <em>Entropy</em>, 13(6):
1076&ndash;1136. doi:10.3390/e13061076</li>

<li>R&eacute;dei, Mikl&oacute;s and Michael St&ouml;ltzner (eds.),
2001, <em>John von Neumann and the Foundations of Quantum
Physics</em>, (Vienna Circle Institute Yearbook, 8), Dordrecht:
Kluwer.</li>

<li>R&eacute;nyi, Alfr&eacute;d, 1961, &ldquo;On Measures of Entropy
and Information&rdquo;, in <em>Proceedings of the Fourth Berkeley
Symposium on Mathematical Statistics and Probability, Volume 1:
Contributions to the Theory of Statistics</em>, Berkeley, CA: The
Regents of the University of California, pp. 547&ndash;561.
 [<a href="https://projecteuclid.org/euclid.bsmsp/1200512181" target="other">R&eacute;nyi 1961 available online</a>]</li>
 
<li>Rissanen, J., 1978, &ldquo;Modeling by Shortest Data
Description&rdquo;, <em>Automatica</em>, 14(5): 465&ndash;471.
doi:10.1016/0005-1098(78)90005-5</li>

<li>&ndash;&ndash;&ndash;, 1989, <em>Stochastic Complexity in
Statistical Inquiry</em>, (World Scientific Series in Computer
Science, 15), Singapore: World Scientific.</li>

<li>Rooy, Robert van, 2004, &ldquo;Signalling Games Select Horn
Strategies&rdquo;, <em>Linguistics and Philosophy</em>, 27(4):
493&ndash;527. doi:10.1023/B:LING.0000024403.88733.3f</li>

<li>Russell, Bertrand, 1905, &ldquo;On Denoting&rdquo;, <em>Mind</em>,
new series, 14(4): 479&ndash;493. doi:10.1093/mind/XIV.4.479</li>

<li>Schmandt-Besserat, Denise, 1992, <em>Before Writing</em> (Volume
I: From Counting to Cuneiform), Austin, TX: University of Texas
Press.</li>

<li>Schmidhuber, J&uuml;urgen, 1997a, &ldquo;Low-Complexity
Art&rdquo;, <em>Leonardo</em>, 30(2): 97&ndash;103.
doi:10.2307/1576418</li>

<li>&ndash;&ndash;&ndash;, 1997b, &ldquo;A Computer Scientist&rsquo;s
View of Life, the Universe, and Everything&rdquo;, in <em>Foundations
of Computer Science</em>, (Lecture Notes in Computer Science, 1337),
Christian Freksa, Matthias Jantzen, and R&uuml;diger Valk (eds.),
Berlin, Heidelberg: Springer Berlin Heidelberg, 201&ndash;208.
doi:10.1007/BFb0052088</li>

<li>Schnelle, H., 1976, &ldquo;Information&rdquo;, in Joachim Ritter
(ed.), <em>Historisches W&ouml;rterbuch der Philosophie</em>, IV
[Historical dictionary of philosophy, IV] (pp. 116&ndash;117).
Stuttgart, Germany: Schwabe.</li>

<li>Searle, John R., 1990, &ldquo;Is the Brain a Digital
Computer?&rdquo;, <em>Proceedings and Addresses of the American
Philosophical Association</em>, 64(3): 21&ndash;37.
doi:10.2307/3130074</li>

<li>Seiffert, Helmut, 1968, <em>Information &uuml;ber die
Information</em> [Information about information] Munich: Beck.</li>

<li>Shannon, Claude E., 1948, &ldquo;A Mathematical Theory of
Communication&rdquo;, <em>Bell System Technical Journal</em>, 27(3):
379&ndash;423 &amp; 27(4): 623&ndash;656.
doi:10.1002/j.1538-7305.1948.tb01338.x &amp;
doi:10.1002/j.1538-7305.1948.tb00917.x</li>

<li>Shannon, Claude E. and Warren Weaver, 1949, <em>The Mathematical
Theory of Communication</em>, Urbana, IL: University of Illinois
Press.</li>

<li>Shor, Peter W., 1997, &ldquo;Polynomial-Time Algorithms for Prime
Factorization and Discrete Logarithms on a Quantum Computer&rdquo;,
<em>SIAM Journal on Computing</em>, 26(5): 1484&ndash;1509.
doi:10.1137/S0097539795293172</li>

<li>Simon, J.C. and Olivier Dubois, 1989, &ldquo;Number of Solutions
of Satisfiability Instances &ndash; Applications to Knowledge Bases&rdquo;,
<em>International Journal of Pattern Recognition and Artificial
Intelligence</em>, 3(1): 53&ndash;65.
doi:10.1142/S0218001489000061</li>

<li>Simondon, Gilbert, 1989, <em>L&rsquo;individuation Psychique et
Collective: &Agrave; La Lumi&egrave;re des Notions de Forme,
Information, Potentiel et M&eacute;tastabilit&eacute;
(L&rsquo;Invention Philosophique)</em>, Paris: Aubier.</li>

<li>Singh, Simon, 1999, <em>The Code Book: The Science of Secrecy from
Ancient Egypt to Quantum Cryptography</em>, New York: Anchor
Books.</li>

<li>Solomonoff, R. J., 1960, &ldquo;A Preliminary Report on a General
Theory of Inductive Inference&rdquo;. Report ZTB-138, Cambridge, MA:
Zator.
 [<a href="http://raysolomonoff.com/publications/pubs.html" target="other">Solomonoff 1960 available online</a>]</li>
 
<li>&ndash;&ndash;&ndash;, 1964a, &ldquo;A Formal Theory of Inductive
Inference. Part I&rdquo;, <em>Information and Control</em>, 7(1):
1&ndash;22. doi:10.1016/S0019-9958(64)90223-2</li>

<li>&ndash;&ndash;&ndash;, 1964b, &ldquo;A Formal Theory of Inductive
Inference. Part II&rdquo;, <em>Information and Control</em>, 7(2):
224&ndash;254. doi:10.1016/S0019-9958(64)90131-7</li>

<li>&ndash;&ndash;&ndash;, 1997, &ldquo;The Discovery of Algorithmic
Probability&rdquo;, <em>Journal of Computer and System Sciences</em>,
55(1): 73&ndash;88. doi:10.1006/jcss.1997.1500</li>

<li>Stalnaker, Richard, 1984, <em>Inquiry</em>, Cambridge, MA: MIT
Press.</li>

<li>Stifel, Michael, 1544, <em>Arithmetica integra</em>, Nuremberg:
Johan Petreium.</li>

<li>Tarski, Alfred, 1944, &ldquo;The Semantic Conception of Truth: And
the Foundations of Semantics&rdquo;, <em>Philosophy and
Phenomenological Research</em>, 4(3): 341&ndash;376.
doi:10.2307/2102968</li>

<li>Tsallis, Constantino, 1988, &ldquo;Possible Generalization of
Boltzmann-Gibbs Statistics&rdquo;, <em>Journal of Statistical
Physics</em>, 52(1&ndash;2): 479&ndash;487.
doi:10.1007/BF01016429</li>

<li>Turing, A. M., 1937, &ldquo;On Computable Numbers, with an
Application to the Entscheidungsproblem&rdquo;, <em>Proceedings of the
London Mathematical Society</em>, s2-42(1): 230&ndash;265.
doi:10.1112/plms/s2-42.1.230</li>

<li>Valiant, Leslie G., 2009, &ldquo;Evolvability&rdquo;, <em>Journal
of the ACM</em>, 56(1): Article 3. doi:10.1145/1462153.1462156</li>

<li>van Benthem, Johan F.A.K., 1990, &ldquo;Kunstmatige Intelligentie:
Een Voortzetting van de Filosofie met Andere Middelen&rdquo;,
<em>Algemeen Nederlands Tijdschrift voor Wijsbegeerte</em>, 82:
83&ndash;100.</li>

<li>&ndash;&ndash;&ndash;, 2006, &ldquo;Epistemic Logic and
Epistemology: The State of Their Affairs&rdquo;, <em>Philosophical
Studies</em>, 128(1): 49&ndash;76. doi:10.1007/s11098-005-4052-0</li>

<li>van Benthem, Johan and Robert van Rooy, 2003, &ldquo;Connecting
the Different Faces of Information&rdquo;, <em>Journal of Logic,
Language and Information</em>, 12(4): 375&ndash;379.
doi:10.1023/A:1025026116766</li>

<li>van Peursen, Cornelis Anthonie, 1987, &ldquo;Christian
Wolff&rsquo;s Philosophy of Contingent Reality&rdquo;, <em>Journal of
the History of Philosophy</em>, 25(1): 69&ndash;82.
doi:10.1353/hph.1987.0005</li>

<li>van Rooij, Robert, 2003, &ldquo;Questioning to resolve decision
problems&rdquo;, <em>Linguistics and Philosophy</em>, 26:
727&ndash;763.</li>

<li>Vereshchagin, Nikolai K. and Paul M.B. Vit&aacute;nyi, 2004,
&ldquo;Kolmogorov&rsquo;s Structure Functions and Model
Selection&rdquo;, <em>IEEE Transactions on Information Theory</em>,
50(12): 3265&ndash;3290. doi:10.1109/TIT.2004.838346</li>

<li>Verlinde, Erik, 2011, &ldquo;On the Origin of Gravity and the Laws
of Newton&rdquo;, <em>Journal of High Energy Physics</em>, 2011(4).
doi:10.1007/JHEP04(2011)029 </li>

<li>&ndash;&ndash;&ndash;, 2017, &ldquo;Emergent Gravity and the Dark
Universe&rdquo;, <em>SciPost Physics</em>, 2(3): 016.
doi:10.21468/SciPostPhys.2.3.016</li>

<li>Vigo, Ronaldo, 2011, &ldquo;Representational Information: A New
General Notion and Measure of Information&rdquo;, <em>Information
Sciences</em>, 181(21): 4847&ndash;4859.
doi:10.1016/j.ins.2011.05.020</li>

<li>&ndash;&ndash;&ndash;, 2012, &ldquo;Complexity over Uncertainty in
Generalized Representational Information Theory (GRIT): A
Structure-Sensitive General Theory of Information&rdquo;,
<em>Information</em>, 4(1): 1&ndash;30. doi:10.3390/info4010001</li>

<li>Vit&aacute;nyi, Paul M., 2006, &ldquo;Meaningful
Information&rdquo;, <em>IEEE Transactions on Information Theory</em>,
52(10): 4617&ndash;4626. doi:10.1109/TIT.2006.881729
 [<a href="http://arxiv.org/abs/cs.CC/0111053" target="other">Vit&aacute;nyi 2006 available online</a>].</li>
 
<li>Vogel, Cornelia Johanna de, 1968, <em>Plato: De filosoof van het
transcendente</em>, Baarn: Het Wereldvenster.</li>

<li>Von Neumann, John, 1932, <em>Mathematische Grundlagen der
Quantenmechanik</em>, Berlin: Springer.</li>

<li>Wallace, C. S., 2005, <em>Statistical and Inductive Inference by
Minimum Message Length</em>, Berlin: Springer.
doi:10.1007/0-387-27656-4</li>

<li>Wheeler, John Archibald, 1990, &ldquo;Information, Physics,
Quantum: The Search for Links&rdquo;, in <em>Complexity, Entropy and
the Physics of Information</em>, Wojciech H. Zurek (ed.), Boulder, CO:
Westview Press, 309&ndash;336.
 [<a href="https://philpapers.org/go.pl?id=WHEIPQ&amp;u=https%3A%2F%2Fphilpapers.org%2Farchive%2FWHEIPQ.pdf" target="other">Wheeler 1990 available online</a>]</li>
 
<li>Whitehead, Alfred and Bertrand Russell, 1910, 1912, 1913,
<em>Principia Mathematica</em>, 3 vols, Cambridge: Cambridge
University Press; 2nd edn, 1925 (Vol. 1), 1927 (Vols 2, 3).</li>

<li>Wilkins, John, 1668, &ldquo;An Essay towards a Real Character, and
a Philosophical Language&rdquo;, London.
 [<a href="https://archive.org/details/AnEssayTowardsARealCharacterAndAPhilosophicalLanguage/page/n7" target="other">Wilkins 1668 available online</a>]</li>
 
<li>Windelband, Wilhelm, 1903, <em>Lehrbuch der Geschichte der
Philosophie</em>, T&uuml;bingen: J.C.B. Mohr. </li>

<li>Wolff, J. Gerard, 2006, <em>Unifying Computing and Cognition</em>,
Menai Bridge: CognitionResearch.org.uk.</li>

<li>Wolfram, Stephen, 2002, <em>A New Kind of Science</em>, Champaign,
IL: Wolfram Media.</li>

<li>Wolpert, David H. and William Macready, 2007, &ldquo;Using
Self-Dissimilarity to Quantify Complexity&rdquo;, <em>Complexity</em>,
12(3): 77&ndash;85. doi:10.1002/cplx.20165</li>

<li>Wu, Kun, 2010, &ldquo;The Basic Theory of the Philosophy of
Information&rdquo;, in <em>Proceedings of the 4th International
Conference on the Foundations of Information Science</em>, Beijing,
China, Pp. 21&ndash;24.</li>

<li>&ndash;&ndash;&ndash;, 2016, &ldquo;The Interaction and
Convergence of the Philosophy and Science of Information&rdquo;,
<em>Philosophies</em>, 1(3): 228&ndash;244.
doi:10.3390/philosophies1030228</li>

<li>Zuse, Konrad, 1969, <em>Rechnender Raum</em>, Braunschweig:
Friedrich Vieweg &amp; Sohn. Translated as <em>Calculating Space</em>,
MIT Technical Translation AZT-70-164-GEMIT, MIT (Proj. MAC),
Cambridge, MA, Feb. 1970. English revised by A. German and H. Zenil
2012.
 [<a href="https://philpapers.org/archive/ZUSRR.pdf" target="other">Zuse 1969 [2012] available online</a>]</li>

</ul>

</div> 


<div id="academic-tools">

<h2 id="Aca">Academic Tools</h2>

<blockquote>
<table class="vert-top">
<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=information" target="other">How to cite this entry</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://leibniz.stanford.edu/friends/preview/information/" target="other">Preview the PDF version of this entry</a> at the
 <a href="https://leibniz.stanford.edu/friends/" target="other">Friends of the SEP Society</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/inpho.png" alt="inpho icon" /></td>
<td><a href="https://www.inphoproject.org/entity?sep=information&amp;redirect=True" target="other">Look up topics and thinkers related to this entry</a>
 at the Internet Philosophy Ontology Project (InPhO).</td>
</tr>

<tr>
<td><img src="../../symbols/pp.gif" alt="phil papers icon" /></td>
<td><a href="http://philpapers.org/sep/information/" target="other">Enhanced bibliography for this entry</a>
at <a href="http://philpapers.org/" target="other">PhilPapers</a>, with links to its database.</td>
</tr>

</table>
</blockquote>

</div>

<div id="other-internet-resources">

<h2 id="Oth">Other Internet Resources</h2>

<ul class="hanging">

<li>Aaronson, Scott, 2006,
 <a href="http://www.scottaaronson.com/blog/?p=122" target="other">Reasons to Believe</a>,
 <em>Shtetl-Optimized</em> blog post, September 4, 2006.</li>

<li>Adriaans, Pieter W., 2016,
 <a href="https://arxiv.org/abs/1611.07829" target="other">&ldquo;A General Theory of Information and Computation&rdquo;</a>,
 unpublished manuscript, November 2016, arXiv:1611.07829. </li>

<li>Bekenstein, Jacob D., 1994,
 &ldquo;<a href="http://arxiv.org/abs/gr-qc/9409015" target="other">Do We Understand Black Hole Entropy?</a>&rdquo;,
 Plenary talk at Seventh Marcel Grossman meeting at Stanford
University., arXiv:gr-qc/9409015.</li>

<li>Churchill, Alex, 2012,
 <a href="https://www.toothycat.net/~hologram/Turing/index.html" target="other">Magic: the Gathering is Turing Complete</a>.</li>
 
<li>Cook, Stephen, 2000,
 <a href="http://www.claymath.org/sites/default/files/pvsnp.pdf" target="other">The P versus NP Problem</a>,
 Clay Mathematical Institute; The Millennium Prize Problem.</li>

<li>Huber, Franz, 2007,
 <a href="http://www.iep.utm.edu/conf-ind/" target="other">Confirmation and Induction</a>,
 entry in the <em>Internet Encyclopedia of Philosophy</em>.</li>

<li>Sajjad, H. Rizvi, 2006,
 &ldquo;<a href="http://www.iep.utm.edu/avicenna/" target="other">Avicenna/Ibn Sina</a>&rdquo;,
 entry in the <em>Internet Encyclopedia of Philosophy</em>.</li>

<li>Goodman, L. and Weisstein, E.W., 2019, 
 &ldquo;<a href="http://mathworld.wolfram.com/RiemannHypothesis.html" target="other">The Riemann Hypothesis</a>&rdquo;,
 <em>From MathWorld--A Wolfram Web Resource</em>.</li>

 <li><a href="http://cstheory.stackexchange.com/questions/88/what-would-it-mean-to-disprove-church-turing-thesis" target="other">Computability &ndash; What would it mean to disprove Church-Turing thesis?</a>,
 discussion on Theoretical Computer Science StackExchange.</li>

 <li><a href="https://www.britannica.com/science/prime-number-theorem" target="other">Prime Number Theorem</a>,
 <em>Encyclopedia Britannica</em>, December 20, 2010.</li>

<li><a href="https://en.wikipedia.org/w/index.php?title=Hardware_random_number_generator&amp;oldid=867411555" target="other">Hardware random number generator</a>,
 <em>Wikipedia</em> entry, November 2018.</li>

</ul>

</div>

<div id="related-entries">

<h2 id="Rel">Related Entries</h2>

<p>

 <a href="../aristotle-causality/index.html">Aristotle, Special Topics: causality</a> |
 <a href="../church-turing/index.html">Church-Turing Thesis</a> |
 <a href="../epistemic-paradoxes/index.html">epistemic paradoxes</a> |
 <a href="../frege-hilbert/index.html">Frege, Gottlob: controversy with Hilbert</a> |
 <a href="../frege-theorem/index.html">Frege, Gottlob: theorem and foundations for arithmetic</a> |
 <a href="../goedel-incompleteness/index.html">G&ouml;del, Kurt: incompleteness theorems</a> |
 <a href="../information-biological/index.html">information: biological</a> |
 <a href="../information-semantic/index.html">information: semantic conceptions of</a> |
 <a href="../information-entropy/index.html">information processing: and thermodynamic entropy</a> |
 <a href="../logic-information/index.html">logic: and information</a> |
 <a href="../logic-substructural/index.html">logic: substructural</a> |
 <a href="../philosophy-mathematics/index.html">mathematics, philosophy of</a> |
 <a href="../ockham/index.html">Ockham [Occam], William</a> |
 <a href="../plato-metaphysics/index.html">Plato: middle period metaphysics and epistemology</a> |
 <a href="../port-royal-logic/index.html">Port Royal Logic</a> |
 <a href="../properties/index.html">properties</a> |
 <a href="../qt-entangle/index.html">quantum theory: quantum entanglement and information</a> |
 <a href="../rationalism-empiricism/index.html">rationalism vs. empiricism</a> |
 <a href="../recursive-functions/index.html">recursive functions</a> |
 <a href="../rigid-designators/index.html">rigid designators</a> |
 <a href="../russell-paradox/index.html">Russell&rsquo;s paradox</a> |
 <a href="../set-theory/index.html">set theory</a> |
 <a href="../settheory-alternative/index.html">set theory: alternative axiomatic theories</a> |
 <a href="../continuum-hypothesis/index.html">set theory: continuum hypothesis</a> |
 <a href="../time-thermo/index.html">time: thermodynamic asymmetry in</a>

</p>

</div>
<script type="text/javascript" src="local.js"></script>
<script type="text/javascript" src="../../MathJax/MathJaxb198.js?config=TeX-MML-AM_CHTML"></script>


</div><!-- #aueditable --><!--DO NOT MODIFY THIS LINE AND BELOW-->

<!-- END ARTICLE HTML -->

</div> <!-- End article-content -->

  <div id="article-copyright">
    <p>
 <a href="../../info.html#c">Copyright &copy; 2020</a> by

<br />
<a href="http://www.uva.nl/en/profile/a/d/p.w.adriaans/p.w.adriaans.html" target="other">Pieter Adriaans</a>
&lt;<a href="m&#97;ilto:P&#37;2eW&#37;2eAdriaans&#37;40uva&#37;2enl"><em>P<abbr title=" dot ">&#46;</abbr>W<abbr title=" dot ">&#46;</abbr>Adriaans<abbr title=" at ">&#64;</abbr>uva<abbr title=" dot ">&#46;</abbr>nl</em></a>&gt;
    </p>
  </div>

</div> <!-- End article -->

<!-- NOTE: article banner is outside of the id="article" div. -->
<div id="article-banner" class="scroll-block">
  <div id="article-banner-content">
    <a href="../../fundraising/index.html">
    Open access to the SEP is made possible by a world-wide funding initiative.<br />
    The Encyclopedia Now Needs Your Support<br />
    Please Read How You Can Help Keep the Encyclopedia Free</a>
  </div>
</div> <!-- End article-banner -->

    </div> <!-- End content -->

    <div id="footer">

      <div id="footer-menu">
        <div class="menu-block">
          <h4><i class="icon-book"></i> Browse</h4>
          <ul role="menu">
            <li><a href="../../contents.html">Table of Contents</a></li>
            <li><a href="../../new.html">What's New</a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
            <li><a href="../../published.html">Chronological</a></li>
            <li><a href="../../archives/index.html">Archives</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-info-sign"></i> About</h4>
          <ul role="menu">
            <li><a href="../../info.html">Editorial Information</a></li>
            <li><a href="../../about.html">About the SEP</a></li>
            <li><a href="../../board.html">Editorial Board</a></li>
            <li><a href="../../cite.html">How to Cite the SEP</a></li>
            <li><a href="../../special-characters.html">Special Characters</a></li>
            <li><a href="../../tools/index.html">Advanced Tools</a></li>
            <li><a href="../../contact.html">Contact</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-leaf"></i> Support SEP</h4>
          <ul role="menu">
            <li><a href="../../support/index.html">Support the SEP</a></li>
            <li><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
            <li><a href="../../support/donate.html">Make a Donation</a></li>
            <li><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
          </ul>
        </div>
      </div> <!-- End footer menu -->

      <div id="mirrors">
        <div id="mirror-info">
          <h4><i class="icon-globe"></i> Mirror Sites</h4>
          <p>View this site from another server:</p>
        </div>
        <div class="btn-group open">
          <a class="btn dropdown-toggle" data-toggle="dropdown" href="https://plato.stanford.edu/">
            <span class="flag flag-usa"></span> USA (Main Site) <span class="caret"></span>
            <span class="mirror-source">Philosophy, Stanford University</span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="../../mirrors.html">Info about mirror sites</a></li>
          </ul>
        </div>
      </div> <!-- End mirrors -->
      
      <div id="site-credits">
        <p>The Stanford Encyclopedia of Philosophy is <a href="../../info.html#c">copyright &copy; 2021</a> by <a href="http://mally.stanford.edu/">The Metaphysics Research Lab</a>, Department of Philosophy, Stanford University</p>
        <p>Library of Congress Catalog Data: ISSN 1095-5054</p>
      </div> <!-- End site credits -->

    </div> <!-- End footer -->

  </div> <!-- End container -->

   <!-- NOTE: Script required for drop-down button to work (mirrors). -->
  <script>
    $('.dropdown-toggle').dropdown();
  </script>

</body>

<!-- Mirrored from seop.illc.uva.nl/entries/information/ by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 22 Jun 2022 19:50:09 GMT -->
</html>
