<!DOCTYPE html>
<!--[if lt IE 7]> <html class="ie6 ie"> <![endif]-->
<!--[if IE 7]>    <html class="ie7 ie"> <![endif]-->
<!--[if IE 8]>    <html class="ie8 ie"> <![endif]-->
<!--[if IE 9]>    <html class="ie9 ie"> <![endif]-->
<!--[if !IE]> --> <html> <!-- <![endif]-->

<!-- Mirrored from seop.illc.uva.nl/entries/common-knowledge/ by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 22 Jun 2022 19:42:29 GMT -->
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>
Common Knowledge (Stanford Encyclopedia of Philosophy)
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="robots" content="noarchive, noodp" />
<meta property="citation_title" content="Common Knowledge" />
<meta property="citation_author" content="Vanderschraaf, Peter" />
<meta property="citation_author" content="Sillari, Giacomo" />
<meta property="citation_publication_date" content="2001/08/28" />
<meta name="DC.title" content="Common Knowledge" />
<meta name="DC.creator" content="Vanderschraaf, Peter" />
<meta name="DC.creator" content="Sillari, Giacomo" />
<meta name="DCTERMS.issued" content="2001-08-28" />
<meta name="DCTERMS.modified" content="2013-07-23" />

<!-- NOTE: Import webfonts using this link: -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300,600,200&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css" />

<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap-responsive.min.css" />
<link rel="stylesheet" type="text/css" href="../../css/font-awesome.min.css" />
<!--[if IE 7]> <link rel="stylesheet" type="text/css" href="../../css/font-awesome-ie7.min.css"> <![endif]-->
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/style.css" />
<link rel="stylesheet" type="text/css" media="print" href="../../css/print.css" />
<link rel="stylesheet" type="text/css" href="../../css/entry.css" />
<!--[if IE]> <link rel="stylesheet" type="text/css" href="../../css/ie.css" /> <![endif]-->
<script type="text/javascript" src="../../js/jquery-1.9.1.min.js"></script>
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>

<!-- NOTE: Javascript for sticky behavior needed on article and ToC pages -->
<script type="text/javascript" src="../../js/jquery-scrolltofixed-min.js"></script>
<script type="text/javascript" src="../../js/entry.js"></script>

<!-- SEP custom script -->
<script type="text/javascript" src="../../js/sep.js"></script>
</head>

<!-- NOTE: The nojs class is removed from the page if javascript is enabled. Otherwise, it drives the display when there is no javascript. -->
<body class="nojs article" id="pagetopright">
<div id="container">
<div id="header-wrapper">
  <div id="header">
    <div id="branding">
      <div id="site-logo"><a href="../../index.html"><img src="../../symbols/sep-man-red.png" alt="SEP logo" /></a></div>
      <div id="site-title"><a href="../../index.html">Stanford Encyclopedia of Philosophy</a></div>
    </div>
    <div id="navigation">
      <div class="navbar">
        <div class="navbar-inner">
          <div class="container">
            <button class="btn btn-navbar collapsed" data-target=".collapse-main-menu" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Menu </button>
            <div class="nav-collapse collapse-main-menu in collapse">
              <ul class="nav">
                <li class="dropdown open"><a id="drop1" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-book"></i> Browse</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop1">
                    <li><a href="../../contents.html">Table of Contents</a></li>
                    <li><a href="../../new.html">What's New</a></li>
                    <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
                    <li><a href="../../published.html">Chronological</a></li>
                    <li><a href="../../archives/index.html">Archives</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop2" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-info-sign"></i> About</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop2">
                    <li><a href="../../info.html">Editorial Information</a></li>
                    <li><a href="../../about.html">About the SEP</a></li>
                    <li><a href="../../board.html">Editorial Board</a></li>
                    <li><a href="../../cite.html">How to Cite the SEP</a></li>
                    <li><a href="../../special-characters.html">Special Characters</a></li>
                    <li><a href="../../tools/index.html">Advanced Tools</a></li>
                    <li><a href="../../contact.html">Contact</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop3" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-leaf"></i> Support SEP</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop3">
                    <li><a href="../../support/index.html">Support the SEP</a></li>
                    <li><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
                    <li><a href="../../support/donate.html">Make a Donation</a></li>
                    <li><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
                  </ul>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End navigation -->
    
    <div id="search">
      <form id="search-form" method="get" action="https://seop.illc.uva.nl/search/searcher.py">
        <input type="search" name="query" placeholder="Search SEP" />
        <div class="search-btn-wrapper"><button class="btn search-btn" type="submit"><i class="icon-search"></i></button></div>
      </form>
    </div>
    <!-- End search --> 
    
  </div>
  <!-- End header --> 
</div>
<!-- End header wrapper -->

<div id="content">

<!-- Begin article sidebar -->
<div id="article-sidebar" class="sticky">
  <div class="navbar">
    <div class="navbar-inner">
      <div class="container">
        <button class="btn btn-navbar" data-target=".collapse-sidebar" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Entry Navigation </button>
        <div id="article-nav" class="nav-collapse collapse-sidebar in collapse">
          <ul class="nav">
            <li><a href="#toc">Entry Contents</a></li>
            <li><a href="#Bib">Bibliography</a></li>
            <li><a href="#Aca">Academic Tools</a></li>
            <li><a href="https://leibniz.stanford.edu/friends/preview/common-knowledge/">Friends PDF Preview <i class="icon-external-link"></i></a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=common-knowledge">Author and Citation Info <i class="icon-external-link"></i></a> </li>
            <li><a href="#pagetopright" class="back-to-top">Back to Top <i class="icon-angle-up icon2x"></i></a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</div>
<!-- End article sidebar --> 

<!-- NOTE: Article content must have two wrapper divs: id="article" and id="article-content" -->
<div id="article">
<div id="article-content">

<!-- BEGIN ARTICLE HTML -->


<div id="aueditable"><!--DO NOT MODIFY THIS LINE AND ABOVE-->

<h1>Common Knowledge</h1><div id="pubinfo"><em>First published Tue Aug 28, 2001; substantive revision Tue Jul 23, 2013</em></div>

<div id="preamble">

<p>

A proposition <em>A</em> is <em>mutual knowledge</em> among a set of
agents if each agent knows that <em>A</em>. Mutual knowledge by itself
implies nothing about what, if any, knowledge anyone attributes to
anyone else. Suppose each student arrives for a class meeting knowing
that the instructor will be late. That the instructor will be late is
mutual knowledge, but each student might think only she knows the
instructor will be late. However, if one of the students says openly
&ldquo;Peter told me he will be late again,&rdquo; then each student knows that
each student knows that the instructor will be late, each student knows
that each student knows that each student knows that the instructor
will be late, and so on, <em>ad infinitum.</em> The announcement made
the mutually known fact <em>common knowledge</em> among the students. 
</p>

<p>

Common knowledge is a phenomenon which underwrites much of social
life. In order to communicate or otherwise coordinate their behavior
successfully, individuals typically require mutual or common
understandings or background knowledge. Indeed, if a particular
interaction results in &ldquo;failure&rdquo;, the usual explanation for this is
that the agents involved did not have the common knowledge that would
have resulted in success. If a married couple are separated in a
department store, they stand a good chance of finding one another
because their common knowledge of each others' tastes and experiences
leads them each to look for the other in a part of the store both know
that both would tend to frequent. Since the spouses both love
cappuccino, each expects the other to go to the coffee bar, and they
find one another. But in a less happy case, if a pedestrian causes a
minor traffic jam by crossing against a red light, she explains her
mistake as the result of her not noticing, and therefore not knowing,
the status of the traffic signal that all the motorists knew. The
spouses coordinate successfully given their common knowledge, while the
pedestrian and the motorists miscoordinate as the result of a breakdown
in common knowledge.</p>

<p>

Given the importance of common knowledge in social interactions, it is
remarkable that only quite recently have philosophers and social
scientists attempted to analyze the concept. David Hume (1740) was
perhaps the first to make explicit reference to the role of mutual
knowledge in coordination. In his account of convention in <em>A
Treatise of Human Nature</em>, Hume argued that a necessary condition
for coordinated activity was that agents all know what behavior to
expect from one another. Without the requisite mutual knowledge, Hume
maintained, mutually beneficial social conventions would disappear.
Much later, J. E. Littlewood (1953) presented some examples of
common-knowledge-type reasoning, and Thomas Schelling (1960) and John
Harsanyi (1967&ndash;1968) argued that something like common knowledge is
needed to explain certain inferences people make about each other. The
philosopher Robert Nozick describes, but does not develop, the notion
in his doctoral dissertation (Nozick 1963), while the first
mathematical analysis and application of the notion of common
knowledge is found in the technical report by Friedell (1967), then
published as (Friedell
 1969).<sup>[<a href="notes.html#1" name="note-1">1</a>]</sup>
The first full-fledged philosophical
analysis of common knowledge was offered by David Lewis (1969) in the
monograph <em>Convention.</em> Stephen Schiffer (1972), Robert Aumann
(1976), and Gilbert Harman (1977) independently gave alternate
definitions of common knowledge. Jon Barwise (1988, 1989) gave a
precise formulation of Harman's intuitive account. Throughout the
1980's a number of epistemic logicians, both from philosophy and from
computer science, studied the logical structure of common knowledge,
and the interested reader should consult the relevant portions of the
two important monographs (Fagin et al. 1995) and (Meyer and Van der
Hoek 1995). Margaret Gilbert (1989) proposed a somewhat different
account of common knowledge which she argues is preferable to the
standard account. Others have developed accounts of mutual
knowledge, <em>approximate common knowledge</em>, and <em>common
belief</em> which require less stringent assumptions than the standard
account, and which serve as more plausible models of what agents know
in cases where strict common knowledge seems impossible (Brandenburger
and Dekel 1987, Stinchcombe 1988, Monderer and Samet 1989, Rubinstein
1992). The analysis and applications of common knowledge and related
multi-agent knowledge concepts has become a lively field of
research.</p>

<p>

The purpose of this essay is to overview of some of the most
important results stemming from this contemporary research. The topics
reviewed in each section of this essay are as follows: Section 1 gives
motivating examples which illustrate a variety of ways in which the
actions of agents depend crucially upon their having, or lacking,
certain common knowledge. Section 2 discusses alternative analyses of
common knowledge. Section 3 reviews applications of multi-agent
knowledge concepts, particularly to <em>game theory</em> (von Neumann
and Morgenstern 1944), in which common knowledge assumptions have been
found to have great importance in justifying <em>solution concepts</em>
for mathematical games. Section 4 discusses skeptical doubts about the
attainability of common knowledge. Finally, Section 5 discusses the
<em>common belief</em> concept which result from weakening the
assumptions of Lewis' account of common knowledge.</p>

</div>

<div id="toc">
<!--Entry Contents-->
<ul>
<li><a href="#1">1. Motivating Examples</a>
 
<ul>
<li><a href="#1.1">1.1 The Clumsy Waiter</a></li>

<li><a href="#1.2">1.2 The Barbeque Problem</a></li>

<li><a href="#1.3">1.3 The Farmers' Dilemma</a></li>

<li><a href="#1.4">1.4 The Centipede</a></li>

<li><a href="#1.5">1.5 The Department Store</a></li>
</ul>
</li>

<li><a href="#2">2. Alternative Accounts of Common Knowledge</a>
 
<ul>
<li><a href="#2.1">2.1 The Hierarchical Account</a></li>

<li><a href="#2.2">2.2 Lewis' Account</a></li>

<li><a href="#2.3">2.3 Aumann's Account</a></li>

<li><a href="#2.4">2.4 Barwise's Account</a></li>

<li><a href="#2.5">2.5 Gilbert's Account</a></li>
</ul>
</li>

<li><a href="#3">3. Applications of Mutual and Common Knowledge</a>
 
<ul>
<li><a href="#3.1">3.1 The &ldquo;No Disagreement&rdquo; Theorem</a></li>

<li><a href="#3.2">3.2 Convention</a></li>

<li><a href="#3.3">3.3 Strategic Form Games</a></li>

<li><a href="#3.4">3.4 Games of Perfect Information</a></li>

<li><a href="#3.5">3.5 Communication Networks</a></li>
</ul>
</li>

<li><a href="#4">4. Is Common Knowledge Attainable?</a></li>

<li><a href="#5">5. Coordination and Common <em>p</em>-Belief</a>
 
<ul>
<li><a href="#5.1">5.1 The Email Coordination Example</a></li>

<li><a href="#5.2">5.2 Common <em>p</em>-Belief</a></li>
</ul>
</li>

<li><a href="#Bib">Bibliography</a></li>

<li><a href="#Aca">Academic Tools</a></li>

<li><a href="#Oth">Other Internet Resources</a></li>

<li><a href="#Rel">Related Entries</a></li>
</ul>
<!--Entry Contents-->

<hr />

</div>

<div id="main-text">

<h2><a name="1">1. Motivating Examples</a></h2>

<p>

Most of the examples in this section are familiar in the common
knowledge literature, although some of the details and interpretations
presented here are new. Readers may want to ask themselves what, if
any, distinctive aspects of mutual and common knowledge reasoning each
example illustrates. </p>

<h3><a name="1.1">1.1. The Clumsy Waiter</a><sup>[<a href="notes.html#2" name="note-2">2</a>]</sup></h3>

<p>

A waiter serving dinner slips, and spills gravy on a guest's white silk
evening gown. The guest glares at the waiter, and the waiter declares
&ldquo;I'm sorry. It was my fault.&rdquo; Why did the waiter say that he was at
fault? He knew that he was at fault, and he knew from the guest's angry
expression that she knew he was at fault. However, the sorry waiter
wanted assurance that the guest <em>knew that he knew</em> he was at
fault. By saying openly that he was at fault, the waiter knew that the
guest knew what he wanted her to know, namely, that he knew he was at
fault. Note that the waiter's declaration established at least three
levels of nested knowledge. </p>

<p>

Certain assumptions are implicit in the preceding story.  In
particular, the waiter must know that the guest knows he has spoken
the truth, and that she can draw the desired conclusion from what he
says in this context. More fundamentally, the waiter must know that if
he announces &ldquo;It was my fault&rdquo; to the guest, she will
interpret his intended meaning correctly and will infer what his
making this announcement ordinarily implies in this context. This in
turn implies that the guest must know that if the waiter announces
&ldquo;It was my fault&rdquo; in this context, then the waiter indeed
knows he is at fault. Then on account of his announcement, the waiter
knows that the guest knows that he knows he was at fault. The waiter's
announcement was meant to generate <em>higher-order</em> levels of
knowledge of a fact each already knew.</p>

<p>

Just a slight strengthening of the stated assumptions results in
even higher levels of nested knowledge. Suppose the waiter and the
guest each know that the other can infer what he infers from the
waiter's announcement. Can the guest now believe that the waiter does
not know that she knows that he knows he is at fault? If the guest
considers this question, she reasons that if the waiter falsely
believes it is possible that she does not know that he knows he is at
fault, then the waiter must believe it to be possible that she cannot
infer that he knows he is at fault from his own declaration. Since she
knows she <em>can</em> infer that the waiter knows he is at fault from
his declaration, she knows that the waiter knows she can infer this, as
well. Hence the waiter's announcement establishes the fourth-order
knowledge claim: The guest knows that the waiter knows that she knows
that he knows he is at fault. By similar, albeit lengthier, arguments,
the agents can verify that corresponding knowledge claims of even
higher-order must also obtain under these assumptions.</p>

<h3><a name="1.2">1.2 The Barbecue Problem</a></h3>

<p>

This is a variation of an example first published by Littlewood
(1953), although he notes that his version of the example was already
well-known at the
 time.<sup>[<a href="notes.html#3" name="note-3">3</a>]</sup>
 <em>N</em> individuals enjoy a picnic supper
together which includes barbecued spareribs. At the end of the meal,
<em>k</em> &ge; 1 of these diners have barbecue sauce on their
faces. Since no one can see her own face, none of the messy diners
knows whether he or she is messy. Then the cook who served the
spareribs returns with a carton of ice cream. Amused by what he sees,
the cook rings the dinner bell and makes the following announcement:
&ldquo;At least one of you has barbecue sauce on her face. I will ring
the dinner bell over and over, until anyone who is messy has wiped her
face. Then I will serve dessert.&rdquo; For the first <em>k</em>
&minus; 1 rings, no one does anything. Then, at
the <em>k</em><sup>th</sup> ring, each of the messy individuals
suddenly reaches for a napkin, and soon afterwards, the diners are all
enjoying their ice cream.</p>

<p>

How did the messy diners finally realize that their faces needed
cleaning? The <em>k</em> = 1 case is easy, since in this case, the lone
messy individual will realize he is messy immediately, since he sees
that everyone else is clean. Consider the <em>k</em> = 2 case next. At
the first ring, messy individual <em>i</em><sub>1</sub> knows that one
other person, <em>i</em><sub>2</sub>, is messy, but does not yet know
about himself. At the second ring, <em>i</em><sub>1</sub> realizes that
he must be messy, since had <em>i</em><sub>2</sub> been the only messy
one, <em>i</em><sub>2</sub> would have known this after the first ring
when the cook made his announcement, and would have cleaned her face
then. By a symmetric argument, messy diner <em>i</em><sub>2</sub> also
concludes that she is messy at the second ring, and both pick up a
napkin at that time.</p>

<p>

The general case follows by induction. Suppose that if <em>k</em> =
<em>j</em>, then each of the <em>j</em> messy diners can determine that
he is messy after <em>j</em> rings. Then if <em>k</em> = <em>j</em> +
1, then at the <em>j</em> + 1<sup>st</sup> ring, each of the <em>j</em>
+ 1 individuals will realize that he is messy. For if he were not
messy, then the other <em>j</em> messy ones would have all realized
their messiness at the <em>j</em><sup>th</sup> ring and cleaned
themselves then. Since no one cleaned herself after the
<em>j</em><sup>th</sup> ring, at the <em>j</em> + 1<sup>st</sup> ring
each messy person will conclude that someone besides the other
<em>j</em> messy people must also be messy, namely, himself.</p>

<p>

The &ldquo;paradox&rdquo; of this argument is that for <em>k</em> &gt; 1, like
the case of the clumsy waiter of Example 1.1, the cook's announcement
told the diners something that each already knew. Yet apparently the
cook's announcement also gave the diners useful information. How could
this be? By announcing a fact already known to every diner, the cook
made this fact <em>common knowledge</em> among them, enabling each of
them to eventually deduce the condition of his own face after
sufficiently many rings of the bell.<sup>[<a href="notes.html#4" name="note-4">4</a>]</sup></p>

<h3><a name="1.3">1.3 The Farmer's Dilemma</a></h3>

<p>

Does meeting one's obligations to others serve one's self-interest?
Plato and his successors recognized that in certain cases, the answer
seems to be &ldquo;No.&rdquo; Hobbes (1651, pp. 101&ndash;102) considers the challenge of
a &ldquo;Foole&rdquo;, who claims that it is irrational to honor an agreement made
with another who has already fulfilled his part of the agreement.
Noting that in this situation one has gained all the benefit of the
other's compliance, the Foole contends that it would now be best for
him to break the agreement, thereby saving himself the costs of
compliance. Of course, if the Foole's analysis of the situation is
correct, then would the other party to the agreement not anticipate the
Foole's response to agreements honored, and act accordingly?</p>

<p>

Hume (1740, pp. 520&ndash;521) takes up this question, using an example:
Two neighboring farmers each expect a bumper crop of corn. Each will
require his neighbor's help in harvesting his corn when it ripens, or
else a substantial portion will rot in the field. Since their corn will
ripen at different times, the two farmers can ensure full harvests for
themselves by helping each other when their crops ripen, and both know
this. Yet the farmers do not help each other. For the farmer whose corn
ripens later reasons that if she were to help the other farmer, then
when her corn ripens he would be in the position of Hobbes' Foole,
having already benefited from her help. He would no longer have
anything to gain from her, so he would not help her, sparing himself
the hard labor of a second harvest. Since she cannot expect the other
farmer to return her aid when the time comes, she will not help when
his corn ripens first, and of course the other farmer does not help her
when her corn ripens later.</p>

<p>

The structure of Hume's <em>Farmers' Dilemma</em> problem can be
summarized using the following tree diagram:</p>

<blockquote><img src="figure1.1a.gif" alt="Figure 1.1a" />
<br />
 <strong>Figure 1.1a</strong></blockquote>

<p>

This tree is an example of a <em>game in extensive form.</em> At each
stage <em>i</em>, the agent who moves can either choose
<em>C</em><sup>i</sup>, which corresponds to helping or
<em>cooperating</em>, or <em>D</em><sup>i</sup>, which corresponds to
not helping or <em>defecting</em>. The relative preferences of the two
agents over the various outcomes are reflected by the ordered pairs of
<em>payoffs</em> each receives at any particular outcome. If, for
instance, Fiona chooses <em>C</em><sup>i</sup> and Alan chooses
<em>D</em><sup>i</sup>, then Fiona's payoff is 0, her worst payoff, and
Alan's is 4, his best payoff. In a game such as the Figure 1.1.a game,
agents are <em>(Bayesian) rational</em> if each chooses an act that
maximizes her expected payoff, given what she knows. </p>

<p>

In the Farmers' Dilemma game, following the
<em>C</em><sup>1</sup>,<em>C</em><sup>2</sup>-path is strictly better
for both farmers than following the
<em>D</em><sup>1</sup>,<em>D</em><sup>2</sup>-path. However, Fiona
chooses <em>D</em><sup>1</sup>, as the result of the following simple
argument: &ldquo;If I were to choose <em>C</em><sup>1</sup>, then Alan, who
is rational and who knows the payoff structure of the game, would
choose <em>D</em><sup>2</sup>. I am also rational and know the payoff
structure of the game. So I should choose <em>D</em><sup>1</sup>.&rdquo;
Since Fiona knows that Alan is rational and knows the game's payoffs,
she concludes that she need only analyze the <em>reduced</em> game in
the following figure:</p>

<blockquote><img src="figure1.1b.gif" alt="Figure 1.1b" />
<br />
 <strong>Figure 1.1b</strong></blockquote>

<p>

In this reduced game, Fiona is certain to gain a strictly higher
payoff by choosing <em>D</em><sup>1</sup> than if she chooses
<em>C</em><sup>1</sup>, so <em>D</em><sup>1</sup> is her unique best
choice. Of course, when Fiona chooses <em>D</em><sup>1</sup>, Alan,
being rational, responds by choosing <em>D</em><sup>2</sup>. If Fiona
and Alan know: (i) that they are both rational, (ii) that they both
know the payoff structure of the game, and (iii) that they both know
(i) and (ii), then they both can predict what the other will do at
every node of the Figure 1.1.a game, and conclude that they can rule
out the <em>D</em><sup>1</sup>,<em>C</em><sup>2</sup>-branch of the
Figure 1.1.b game and analyze just the reduced game of the following
figure:</p>

<blockquote><img src="figure1.1c.gif" alt="Figure 1.1c" />
<br />
 <strong>Figure 1.1c</strong></blockquote>

<p>

On account of this <em>mutual knowledge</em>, both know that Fiona
will choose <em>D</em><sup>1</sup>, and that Alan will respond with
<em>D</em><sup>2</sup>. Hence, the
<em>D</em><sup>1</sup>,<em>D</em><sup>2</sup>-outcome results if the
Farmers' Dilemma game is played by agents having this mutual knowledge,
though it is suboptimal since both agents would fare better at the
 <em>C</em><sup>1</sup>,<em>C</em><sup>2</sup>-branch.<sup>[<a href="notes.html#5" name="note-5">5</a>]</sup>
 This
argument, which in its essentials is Hume's argument, is an example of
a standard technique for solving sequential games known as
<em>backwards
 induction.</em><sup>[<a href="notes.html#6" name="note-6">6</a>]</sup>
 The basic idea behind backwards induction is
that the agents engaged in a sequential game deduce how each will act
throughout the entire game by ruling out the acts that are not
payoff-maximizing for the agents who would move last, then ruling out
the acts that are not payoff-maximizing for the agents who would move
next-to-last, and so on. Clearly, backwards induction arguments rely
crucially upon what, if any, mutual knowledge the agents have regarding
their situation, and they typically require the agents to evaluate the
truth values of certain subjunctive conditionals, such as &ldquo;If I (Fiona)
were to choose <em>C</em><sup>1</sup>, then Alan would choose
<em>D</em><sup>2</sup>&rdquo;.</p>

<h3><a name="1.4">1.4 The Centipede</a></h3>

<p>

The mutual knowledge assumptions required to construct a backwards
induction solution to a game become more complex as the number of
stages in the game increases. To see this, consider the sequential
<em>Centipede</em> game depicted in the following figure:</p>

<blockquote><img src="figure1.2.gif" alt="Figure 1.2" />
<br />
 <strong>Figure 1.2</strong></blockquote>

<p>

At each stage <em>i</em>, the agent who moves can either choose
<em>R</em><sup><em>i</em></sup>, which in the first three stages gives
the other agent an opportunity to move, or
<em>L</em><sup><em>i</em></sup>, which ends the game. </p>

<p>

Like the Farmers' Dilemma, this game is a commitment problem for the
agents. If each agent could trust the other to choose
<em>R</em><sup><em>i</em></sup> at each stage, then they would each
expect to receive a payoff of 3. However, Alan chooses
<em>L</em><sup>1</sup>, leaving each with a payoff of only 1, as the
result of the following backwards induction argument: &ldquo;If node
<em>n</em><sub>4</sub> were to be reached, then Fiona, (being rational)
would choose <em>L</em><sup>4</sup>. I, knowing this, would (being
rational) choose <em>L</em><sup>3</sup> if node <em>n</em><sub>3</sub>
were to be reached. Fiona, knowing <em>this</em>, would (being
rational) choose <em>L</em><sup>2</sup> if node <em>n</em><sub>2</sub>
were to be reached. Hence, I (being rational) should choose
<em>L</em><sup>1</sup>.&rdquo; To carry out this backwards induction
argument, Alan implicitly assumes that: (i) he knows that Fiona knows
he is rational, and (ii) he knows that Fiona knows that he knows she is
rational. Put another way, for Alan to carry out the backwards
induction argument, at node <em>n</em><sub>1</sub> he must know what
Fiona must know at node <em>n</em><sub>2</sub> to make
<em>L</em><sup>2</sup> her best response should <em>n</em><sub>2</sub>
be reached. While in the Farmer's Dilemma Fiona needed only
<em>first-order</em> knowledge of Alan's rationality and
<em>second-order</em> knowledge of Alan's knowledge of the game to
derive the backwards induction solution, in the Figure 1.2 game, for
Alan to be able to derive the backwards induction solution, the agents
must have <em>third-order mutual knowledge</em> of the game and
<em>second-order mutual knowledge</em> of rationality, and Alan must
have <em>fourth-order</em> knowledge of this mutual knowledge of the
game and <em>third-order</em> knowledge of their mutual knowledge of
rationality. This argument also involves several counterfactuals, since
to construct it the agents must be able to evaluate conditionals of the
form, &ldquo;If node <em>n</em><sub><em>i</em></sub> were to be reached, Alan
(Fiona) would choose <em>L</em><sup><em>i</em></sup>
(<em>R</em><sup><em>i</em></sup>)&rdquo;, which for <em>i</em> &gt; 1 are
counterfactual, since third-order mutual knowledge of rationality
implies that nodes <em>n</em><sub>2</sub>, <em>n</em><sub>3</sub>, and
<em>n</em><sub>4</sub> are never reached.</p>

<p>

The method of backwards induction can be applied to any sequential
game of <em>perfect information</em>, in which the agents can observe
each others' moves in turn and can recall the entire history of play.
However, as the number of potential stages of play increases, the
backwards induction argument evidently becomes harder to construct.
This raises certain questions: (1) What precisely are the mutual or
common knowledge assumptions that are required to justify the backwards
induction argument for a particular sequential game? (2) As a
sequential game increases in complexity, would we expect the mutual
knowledge that is required for backwards induction to start to
fail?</p>

<h3><a name="1.5">1.5 The Department Store</a></h3>

<blockquote class="bigindent">When a man loses his wife in a department store without any
prior understanding on where to meet if they get separated, the chances
are good that they will find each other. It is likely that each will
think of some obvious place to meet, so obvious that each will be sure
that it is &ldquo;obvious&rdquo; to both of them. One does not simply predict where
the other will go, which is wherever the first predicts the second to
predict the first to go, and so <em>ad infinitum.</em> Not &ldquo;What would
I do if I were she?&rdquo; but &ldquo;What would I do if I were she wondering what
she would do if she were wondering what I would do if I were she
&hellip; ?&rdquo; 

<span class="blockright">&mdash;Thomas Schelling, <em>The Strategy of
Conflict</em></span>
</blockquote>

<p>

Schelling's department store problem is an example of a <em>pure
coordination problem</em>, that is, an interaction problem in which the
interests of the agents coincide perfectly. Schelling (1960) and Lewis
(1969), who were the first to make explicit the role common knowledge
plays in social coordination, were also among the first to argue that
coordination problems can be modeled using the analytic vocabulary of
game theory. A very simple example of such a coordination problem is
given in the next figure:</p>

<blockquote><img src="figure1.3.gif" alt="Figure 1.3" />
<br />
 <strong>Figure 1.3</strong></blockquote>

<p>

The matrix of Figure 1.3 is an example of a <em>game in strategic
form.</em> At each outcome of the game, which corresponds to a cell in
the matrix, the row (column) agent receives as payoff the first
(second) element of the ordered pair in the corresponding cell.
However, in strategic form games, each agent chooses without first
being able to observe the choices of any other agent, so that all must
choose as if they were choosing simultaneously. The Figure 1.3 game is
a game of <em>pure coordination</em> (Lewis 1969), that is, a game in
which at each outcome, each agent receives exactly the same payoff. One
interpretation of this game is that Schelling's spouses, Liz and
Robert, are searching for each other in the department store with four
floors, and they find each other if they go to the same floor. Four
outcomes at which the spouses coordinate correspond to the strategy
profiles
(<em>s</em><sub><em>j</em></sub>,&nbsp;<em>s</em><sub><em>j</em></sub>),
1 &le; <em>j</em> &le; 4, of the Figure 1.3 game. These four profiles
are strict <em>Nash equilibria</em> (Nash 1950, 1951) of the game, that
is, each agent has a decisive reason to follow her end of one of these
strategy profiles provided that the other also follows this
 profile.<sup>[<a href="notes.html#7" name="note-7">7</a>]</sup></p>

<p>

The difficulty the agents face is trying to select an equilibrium to
follow. For suppose that Robert hopes to coordinate with Liz on a
particular equilibrium of the game, say
(<em>s</em><sub>2</sub>,&nbsp;<em>s</em><sub>2</sub>). Robert reasons
as follows: &ldquo;Since there are several strict equilibria we might follow,
I should follow my end of
(<em>s</em><sub>2</sub>,&nbsp;<em>s</em><sub>2</sub>) if, and only if,
I have sufficiently high expectations that Liz will follow her end of
(<em>s</em><sub>2</sub>,&nbsp;<em>s</em><sub>2</sub>). But I can only
have sufficiently high expectations that Liz will follow
(<em>s</em><sub>2</sub>,&nbsp;<em>s</em><sub>2</sub> ) if she has
sufficiently high expectations that I will follow
(<em>s</em><sub>2</sub>,&nbsp;<em>s</em><sub>2</sub>). For her to have
such expectations, Liz must have sufficiently high (second-order)
expectations that I have sufficiently high expectations that she will
follow (<em>s</em><sub>2</sub>,&nbsp;<em>s</em><sub>2</sub>), for if
Liz doesn't have these (second-order) expectations, then she will
believe I don't have sufficient reason to follow
(<em>s</em><sub>2</sub>,&nbsp;<em>s</em><sub>2</sub>) and may therefore
deviate from (<em>s</em><sub>2</sub>, <em>s</em><sub>2</sub>) herself.
So I need to have sufficiently high (third-order) expectations that Liz
has sufficiently high (second-order) expectations that I have
sufficiently high expectations that she will follow
(<em>s</em><sub>2</sub>,&nbsp;<em>s</em><sub>2</sub> ), which involves
her in fourth-order expectations regarding me, which involves me in
fifth-order expectations regarding Liz, and so on.&rdquo; What would
suffice for Robert, and Liz, to have decisive reason to follow
(<em>s</em><sub>2</sub>,&nbsp;<em>s</em><sub>2</sub>) is that they
each <em>know</em> that the other <em>knows</em> that &hellip; that
the other will follow (<em>s</em><sub>2</sub>,
<em>s</em><sub>2</sub>) for any number of levels of knowledge, which is
to say that between Liz and Robert it is common knowledge that they
will follow (<em>s</em><sub>2</sub>, <em>s</em><sub>2</sub>). If agents
follow a strict equilibrium in a pure coordination game as a
consequence of their having common knowledge of the game, their
rationality and their intentions to follow this equilibrium, and no
other, then the agents are said to be following a
<em>Lewis-convention</em> (Lewis 1969).</p>

<p>

Lewis' theory of convention applies to a more general class of games
than pure coordination games, but pure coordination games already model
a variety of important social interactions. In particular, Lewis models
conventions of language as equilibrium points of a pure coordination
game. The role common knowledge plays in games of pure coordination
sketched above of course raises further questions: (1) Can people ever
attain the common knowledge which characterizes a Lewis-convention? (2)
Would less stringent epistemic assumptions suffice to justify Nash
equilibrium behavior in a coordination problem?</p>

<h2><a name="2">2. Alternative Accounts of Common Knowledge</a></h2>

<ul>
<li><a href="#2.1">2.1 The Hierarchical Account</a></li>

<li><a href="#2.2">2.2 Lewis' Account</a></li>

<li><a href="#2.3">2.3 Aumann's Account</a></li>

<li><a href="#2.4">2.4 Barwise's Account</a></li>

<li><a href="#2.5">2.5 Gilbert's Account</a></li>
</ul>

<p>

Informally, a proposition <em>A</em> is <em>mutually known</em> among a
set of agents if each agent knows that <em>A</em>. Mutual knowledge by
itself implies nothing about what, if any, knowledge anyone attributes
to anyone else. Suppose each student arrives for a class meeting
knowing that the instructor will be late. That the instructor will be
late is mutual knowledge, but each student might think only she knows
the instructor will be late. However, if one of the students says
openly &ldquo;Peter told me he will be late again,&rdquo; then the mutally known
fact is now <em>commonly known.</em> Each student now knows that the
instructor will be late, and so on, <em>ad infinitum.</em> The agents
have common knowledge in the sense articulated informally by Schelling
(1960), and more precisely by Lewis (1969) and Schiffer (1972).
Schiffer uses the formal vocabulary of <a href="../logic-epistemic/index.html">epistemic logic</a>
(Hintikka 1962) to state his definition of common knowledge. Schiffer's
general approach was to augment a system of sentential logic with a set
of knowledge operators corresponding to a set of agents, and then to
define common knowledge as a hierarchy of propositions in the augmented
system. Bacharach (1992) and Bicchieri (1993) adopt this approach, and
develop logical theories of common knowledge which include soundness
and completeness theorems in the style of (Fagin et al. 1995). One can
also develop formal accounts of common knowledge in set-theoretic
terms, as it was done in the early Friedell (1969) and in the economic
literature after Aumann (1976). Such an approach, easily proven to be
equivalent to the ones cast in epistemic logic, is taken also in this
article.<sup>[<a href="notes.html#8" name="note-8">8</a>]</sup></p>
 
<h3><a name="2.1">2.1 The Hierarchical Account</a></h3>

<p>

Monderer and Samet (1988) and Binmore and Brandenburger (1989) give a
particularly elegant set-theoretic definition of common knowledge. I
will review this definition here, and then show that it is logically
equivalent to the &lsquo;<em>i</em> knows that <em>j</em> knows that
&hellip; <em>k</em> knows that A&rsquo; hierarchy that Lewis (1969)
and Schiffer (1972) argue characterizes common
 knowledge.<sup>[<a href="notes.html#9" name="note-9">9</a>]</sup></p>

<p>

Some preliminary notions must be stated first. Following C. I. Lewis
(1943&ndash;1944) and Carnap (1947), propositions are formally subsets of a
set &Omega; of <em>state descriptions</em> or <em>possible
worlds.</em> One can think of the elements of &Omega; as representing
Leibniz's possible worlds or Wittgenstein's possible states of
affairs. Some results in the common knowledge literature presuppose
that &Omega; is of finite cardinality. If this admittedly unrealistic
assumption is needed in any context, this will be explicitly stated in
this essay, and otherwise one may assume that &Omega; may be either a
finite or an infinite set. A distinguished actual world
&omega;<sub>&alpha;</sub> is an element of &Omega;. A proposition
<em>A</em> &sube; &Omega; obtains (or is true) if the actual world
&omega;<sub>&alpha;</sub> &isin; <em>A</em>. In general, we say that
<em>A</em> <em>obtains at</em> a world &omega; &isin; &Omega; if
&omega; &isin; <em>A</em>. What an agent <em>i</em> knows about the
possible worlds is stated formally in terms of a <em>knowledge
operator</em> <strong>K</strong><sub><em>i</em></sub>. Given a
proposition <em>A</em> &sube; &Omega;,
<strong>K</strong><sub><em>i</em></sub>(<em>A</em>) denotes a new
proposition, corresponding to the set of possible worlds at which
agent <em>i</em> knows that A obtains.
<strong>K</strong><sub><em>i</em></sub>(<em>A</em>) is read as
&lsquo;<em>i</em> knows (that) <em>A</em> (is the case)&rsquo;. The
knowledge operator <strong>K</strong><sub><em>i</em></sub> satisfies
certain axioms, including:</p>

<blockquote>
 <p>
 K1: &nbsp; <strong>K</strong><sub><em>i</em></sub>(<em>A</em>) &sube;
<em>A</em> </p>

<p>

K2: &nbsp; &Omega; &sube;
<strong>K</strong><sub><em>i</em></sub>(&Omega;)</p>

<p>

K3: &nbsp; <strong>K</strong><sub><em>i</em></sub>(<font size="+2">&cap;</font><sub><em>k</em></sub>
<em>A</em><sub><em>k</em></sub>) &nbsp; = &nbsp; <font size="+2">&cap;</font><sub><em>k</em></sub>
<strong>K</strong><sub><em>i</em></sub>(<em>A</em><sub><em>k</em></sub>)</p>

<p>

K4: &nbsp; <strong>K</strong><sub><em>i</em></sub>(<em>A</em>)
&sube;
<strong>K</strong><sub><em>i</em></sub><strong>K</strong><sub><em>i</em></sub>(<em>A</em>)<sup>[<a href="notes.html#10" name="note-10">10</a>]</sup></p>

<p>

K5: &nbsp; &minus;<strong>K</strong><sub><em>i</em></sub>(<em>A</em>)
&sube;
<strong>K</strong><sub><em>i</em></sub>&minus;<strong>K</strong><sub><em>i</em></sub>(<em>A</em>)</p>

</blockquote>

<p>
 In words, K1 says that if <em>i</em> knows <em>A</em>, then
<em>A</em> must be the case. K2 says that <em>i</em> knows that some
possible world in &Omega; occurs no matter which possible world
&omega; occurs. K3 says that <em>i</em> knows a conjunction if, and
only if, <em>i</em> knows each conjunct. K4 is a <em>reflection
axiom</em>, sometimes also presented as the <em>axiom of
transparency</em> (or of <em>positive introspection</em>), which says
that if <em>i</em> knows <em>A</em>, then <em>i</em> knows that she
knows <em>A</em>. Finally, K5 says that if the agent does <em>not</em>
know an event, then she knows that she does not know. This axiom is
presented as the axiom of <em>negative introspection</em>, or as
the <em>axiom of wisdom</em> (since the agents possess Socratic
wisdom, knowing that they do not know.) Note that by K3, if <em>A</em>
&sube; <em>B</em> then
<strong>K</strong><sub><em>i</em></sub>(<em>A</em>) &sube;
<strong>K</strong><sub><em>i</em></sub>(<em>B</em>), by K1 and K2,
<strong>K</strong><sub><em>i</em></sub>(&Omega;) = &Omega;, and by K1
and K4, <strong>K</strong><sub><em>i</em></sub>(<em>A</em>) =
<strong>K</strong><sub><em>i</em></sub><strong>K</strong><sub><em>i</em></sub>(<em>A</em>). Any
system of knowledge satisfying K1 &ndash; K5 corresponds to the modal
system S5, while any system satisying K1 &ndash; K4 corresponds to S4
(Kripke 1963). If one drops the K1 axiom and retains the others, the
resulting system would give a formal account of what an agent
<em>believes</em>, but does not necessarily <em>know.</em> </p>

<p>

A useful notion in the formal analysis of knowledge is that of a
<em>possibility set.</em> An agent i's possibility set at a state of
the world &Omega; is the smallest set of possible worlds that
<em>i</em> thinks could be the case if &omega; is the actual world.
More precisely,</p>

<blockquote><strong>Definition 2.1</strong> <br />
 Agent <em>i</em>'s <em>possibility set</em>
 <span class="scriptuc">H</span><sub>i</sub>(&omega;)
 at &omega; &isin; &Omega; is defined as 

<blockquote><span class="scriptuc">H</span><sub><em>i</em></sub>(&omega;)
 &equiv; <font size="+2">&cap;</font>{ <em>E</em> |
&omega; &isin;
<strong>K</strong><sub><em>i</em></sub>(<em>E</em>) }</blockquote>

<p>

The collection of sets </p>

<blockquote><span class="scriptuc">H</span><sub><em>i</em></sub> &nbsp; = &nbsp;
 <font size="+2">&cup;</font><sub>&omega;&isin;&Omega;</sub>
 <span class="scriptuc">H</span><sub><em>i</em></sub>(&omega;)</blockquote>

<p>

is <em>i</em>'s <em>private information system.</em></p></blockquote>

<p>

Since in words,
 <span class="scriptuc">H</span><sub><em>i</em></sub>(&omega;) is the
intersection of all propositions which <em>i</em> knows at
 &omega;,
 <span class="scriptuc">H</span><sub><em>i</em></sub>(&omega;) is the smallest
proposition in &Omega; that <em>i</em> knows at &omega;. Put another
way,
 <span class="scriptuc">H</span><sub><em>i</em></sub>(&omega;) is the most
specific information that <em>i</em> has about the possible world
&omega;. The intuition behind assigning agents private information
systems is that while an agent <em>i</em> may not be able to perceive
or comprehend every last detail of the world in which <em>i</em> lives,
<em>i</em> does know certain facts about that world. The elements of
<em>i</em>'s information system represent what <em>i</em> knows
immediately at a possible world. We also have the following:</p>

<!--pdf include
<br/>
pdf include-->

<blockquote><strong>Proposition 2.2</strong>
<br />
 <strong>K</strong><sub><em>i</em></sub>(<em>A</em>) = { &omega; |
 <span class="scriptuc">H</span><sub><em>i</em></sub>(&omega;)
&sube; <em>A</em> }</blockquote>

<p>

In many formal analyses of knowledge in the literature, possibility
sets are taken as primitive and Proposition 2.2 is given as the
definition of knowledge. If one adopts this viewpoint, then the axioms
K1 &ndash; K5 follow as consequences of the definition of knowledge. In many
applications, the agents' possibility sets are assumed to
 <em>partition</em><sup>[<a href="notes.html#11" name="note-11">11</a>]</sup>
 the set, in which case
 <span class="scriptuc">H</span><sub><em>i</em></sub> is called
i's <em>private information partition</em>. Notice that if axioms K1 &ndash;
K5 hold, then the possibility sets of each agent always partition the
state set, and vice versa.</p>

<p>

To illustrate the idea of possibility sets, let us return to the
Barbecue Problem described in Example 1.2. Suppose there are three
diners: Cathy, Jennifer and Mark. Then there are 8 relevant states of
the world, summarized by Table 2.1:</p>

<blockquote><img src="figure2.1.gif" alt="Table 2.1" />
<br />
&nbsp; &nbsp; &nbsp; <strong>Table 2.1</strong></blockquote>

<p>

Each diner knows the condition of the other diners' faces, but not
her own. Suppose the cook makes no announcement, after all. Then none
of the diners knows the true state of the world whatever &omega; &isin;
&Omega; the actual world turns out to be, but they do know <em>a
priori</em> that certain propositions are true at various states of the
world. For instance, Cathy's information system before any announcement
is made is depicted in Figure 2.1a:</p>

<blockquote><img alt="missing text, please inform" src="figure2.1a.gif" />
<br />
 <strong>Figure 2.1a</strong></blockquote>

<p>

In this case, Cathy's information system is a partition
 <span class="scriptuc">H</span><sub>1</sub> of &Omega; defined
by</p>

<blockquote><span class="scriptuc">H</span><sub>1</sub> =
{<em>H</em><sub><em>CC</em></sub>, <em>H</em><sub><em>CM</em></sub>,
<em>H</em><sub><em>MC</em></sub>,
<em>H</em><sub><em>MM</em></sub>}</blockquote>

<p>

where</p>

<blockquote><em>H</em><sub><em>CC</em></sub> = {&omega;<sub>1</sub>,
&omega;<sub>2</sub>} (i.e., Jennifer and Mark are both clean) 

<p>

<em>H</em><sub><em>CM</em></sub> = {&omega;<sub>4</sub>,
&omega;<sub>6</sub>} (i.e., Jennifer is clean and Mark is messy)</p>

<p>

<em>H</em><sub><em>MC</em></sub> = {&omega;<sub>3</sub>,
&omega;<sub>5</sub>} (i.e., Jennifer is messy and Mark is clean)</p>

<p>

<em>H</em><sub><em>MM</em></sub> = {&omega;<sub>7</sub>,
&omega;<sub>8</sub>} (i.e., Jennifer and Mark are both messy)</p>
</blockquote>

<p>

Cathy knows immediately which cell
 <span class="scriptuc">H</span><sub>1</sub>(&omega;) in her partition is the
case at any state of the world, but does not know which is the true
state at any &omega; &isin; &Omega;.</p>

<p>

If we add in the assumption stated in Example 1.2 that if there is
at least one messy diner, then the cook announces the fact, then
Cathy's information partition is depicted by Figure 2.1b:</p>

<blockquote><img alt="missing text, please inform" src="figure2.1b.gif" />
<br />
 <strong>Figure 2.1b</strong></blockquote>

<p>

In this case, Cathy's information system is a partition
 <span class="scriptuc">H</span><sub>1</sub> of &Omega; defined
by</p>

<blockquote><span class="scriptuc">H</span><sub>1</sub> =
{<em>H</em><sub><em>CCC</em></sub>, <em>H</em><sub><em>MCC</em></sub>,
<em>H</em><sub><em>CM</em></sub>, <em>H</em><sub><em>MC</em></sub>,
<em>H</em><sub><em>MM</em></sub>}</blockquote>

<p>

where</p>

<table width="100%">
<tr>
<td valign="top"><em>H</em><sub><em>CCC</em></sub></td>
<td valign="top">=</td>
<td valign="top">{&omega;<sub>1</sub>}</td>
<td valign="top">(i.e., Jennifer, Mark, and I are all clean)</td>
</tr>

<tr>
<td valign="top"><em>H</em><sub><em>MCC</em></sub></td>
<td valign="top">=</td>
<td valign="top">{&omega;<sub>2</sub>}</td>
<td valign="top">(i.e., Jennifer and Mark are clean and I am messy)</td>
</tr>

<tr>
<td valign="top"><em>H</em><sub><em>CM</em></sub></td>
<td valign="top">=</td>
<td valign="top">{&omega;<sub>4</sub>, &omega;<sub>6</sub>}</td>
<td valign="top">(i.e., Jennifer is clean and Mark is messy)</td>
</tr>

<tr>
<td valign="top"><em>H</em><sub><em>MC</em></sub></td>
<td valign="top">=</td>
<td valign="top">{&omega;<sub>3</sub>, &omega;<sub>5</sub>}</td>
<td valign="top">(i.e., Jennifer is messy and Mark is clean)</td>
</tr>

<tr>
<td valign="top"><em>H</em><sub><em>MM</em></sub></td>
<td valign="top">=</td>
<td valign="top">{&omega;<sub>7</sub>, &omega;<sub>8</sub>}</td>
<td valign="top">(i.e., Jennifer and Mark are both messy)</td>
</tr>
</table>

<p>

In this case, Cathy's information partition is a <em>refinement</em>
of the partition she has when there is no announcement, for in this
case, then Cathy knows <em>a priori</em> that if &omega;<sub>1</sub> is
the case there will be no announcement and will know immediately that
she is clean, and Cathy knows <em>a priori</em> that if
&omega;<sub>2</sub> is the case, then she will know immediately from
the cook's announcement that she is messy.</p>

<p>

Similarly, if the cook makes an announcement only if he sees at least
two messy diners, Cathy's possibility set is the one represented in
fig. 2.1c:</p>

<blockquote><img alt="missing text, please inform" src="fig2.1c.gif" />
<br />
 <strong>Figure 2.1c</strong></blockquote>

<p>

Cathy's information partition is now defined by</p>

<blockquote><span class="scriptuc">H</span><sub>1</sub> =
{<em>H</em><sub><em>CC</em></sub>, <em>H</em><sub><em>CMC</em></sub>,
<em>H</em><sub><em>CCM</em></sub>, <em>H</em><sub><em>MMC</em></sub>, <em>H</em><sub><em>MCM</em></sub>,
<em>H</em><sub><em>MM</em></sub>}</blockquote>

<p>

where</p>

<table width="100%">
<tr>
<td valign="top"><em>H</em><sub><em>CC</em></sub></td>
<td valign="top">=</td>
<td valign="top">{&omega;<sub>1</sub>, &omega;<sub>2</sub>}</td>
<td valign="top">(i.e., Jennifer and Mark are both clean)</td>
</tr>

<tr>
<td valign="top"><em>H</em><sub><em>CMC</em></sub></td>
<td valign="top">=</td>
<td valign="top">{&omega;<sub>3</sub>}</td>
<td valign="top">(i.e., Mark and I are clean, Jennifer is messy)</td>
</tr>

<tr>
<td valign="top"><em>H</em><sub><em>CCM</em></sub></td>
<td valign="top">=</td>
<td valign="top">{&omega;<sub>4</sub>}</td>
<td valign="top">(i.e., Jennifer and I are clean, Mark is messy)</td>
</tr>

<tr>
<td valign="top"><em>H</em><sub><em>CCM</em></sub></td>
<td valign="top">=</td>
<td valign="top">{&omega;<sub>5</sub>}</td>
<td valign="top">(i.e., Jennifer and I are messy, Mark is clean)</td>
</tr>

<tr>
<td valign="top"><em>H</em><sub><em>CCM</em></sub></td>
<td valign="top">=</td>
<td valign="top">{&omega;<sub>6</sub>}</td>
<td valign="top">(i.e., Mark and I are messy, Jennifer is clean)</td>
</tr>

<tr>
<td valign="top"><em>H</em><sub><em>MM</em></sub></td>
<td valign="top">=</td>
<td valign="top">{&omega;<sub>7</sub>, &omega;<sub>8</sub>}</td>
<td valign="top">(i.e., Jennifer and Mark are both messy)</td>
</tr>

</table>

<p>

In this case, Cathy knows <em>a priori</em> that if
&omega;<sub>3</sub> obtains there will be no announcement, and
similarly for &omega;<sub>4</sub>. Thus, she will be able to
distinguish these states from &omega;<sub>5</sub> and
&omega;<sub>6</sub>, respectively. </p>

<p>
 As mentioned earlier in this subsection, the assumption that agents'
possibility sets partition the state space depends on the modeler's
choice of specific axioms for the knowledge operators. For example, if
we drop axiom K5 (preserving the validity of K1 &ndash; K4) the agent's
possibility sets need not partition the space set (follow the link for
an
 <a href="example2.1.html" name="return-2.1">example</a>.
 For more details and applications, cf. Samet 1990.) It was
conjectured (cf. Geanakoplos 1989) that lack of negative introspection
(i.e. systems without K5) would allow to incorporate unforeseen
contingencies in the epistemic model, by representing the agents'
<em>unawareness</em> of certain events (i.e. the case in which the
agent does not know that an event occurs and also does not know that
she does not know that.) It was later shown by Dekel et al. (1998)
that standard models are not suitable to represent agents'
unawareness. An original non-standard model to represent unawareness
is provided in Heifetz <em>et al</em>. (2006). For a comprehensive
bibliography on modeling unawareness and applications of the notion,
cf. the external links at the end on this entry. </p>

<p>

We can now define mutual and common knowledge as follows:</p>

<blockquote><strong>Definition 2.3</strong>
<br />
 Let a set &Omega; of possible worlds together with a set of agents
<em>N</em> be given. 

<p>

1. The proposition that <em>A</em> is <em>(first level</em> or
<em>first order)</em> <em>mutual knowledge for the agents of</em> N,
<strong>K</strong><sup>1</sup><sub><em>N</em></sub>(<em>A</em>), is the
set defined by</p>

<blockquote>
<strong>K</strong><sup>1</sup><sub><em>N</em></sub>(<em>A</em>) &equiv;
<font size="+2">&cap;</font><sub><em>i</em>&isin;<em>N</em></sub>
<strong>K</strong><sub><em>i</em></sub>(<em>A</em>).</blockquote>

<p>

2. The proposition that <em>A</em> is <em>m</em><sup>th</sup>
<em>level</em> (or <em>m</em><sup>th</sup> <em>order</em>) <em>mutual
knowledge among the agents of N</em>,
<strong>K</strong><sup><em>m</em></sup><sub><em>N</em></sub>(<em>A</em>),
is defined recursively as the set</p>

<blockquote>
<strong>K</strong><sup><em>m</em></sup><sub><em>N</em></sub>(<em>A</em>)
&equiv; <font size="+2">&cap;</font><sub><em>i</em>&isin;<em>N</em></sub>
<strong>K</strong><sub><em>i</em></sub>(<strong>K</strong><sup><em>m</em>&minus;1</sup><sub><em>N</em></sub>(<em>A</em>)).</blockquote>

<p>

3. The proposition that <em>A</em> is <em>common knowledge</em>
among the agents of <em>N</em>,
<strong>K</strong><sup>*</sup><sub><em>N</em></sub>(<em>A</em>), is
defined as the set<sup>[<a href="notes.html#12" name="note-12">12</a>]</sup>
</p>

<blockquote>
<table>
<tr>
<td><strong>K</strong><sup>*</sup><sub><em>N</em></sub>(<em>A</em>)
&equiv;</td>
<td align="center">
   <span class="index">&infin;</span><br />
   <font size="+2">&cap;</font><br />
   <span class="index"><em>m</em>=1</span>
</td>

<td><strong>K</strong><sup><em>m</em></sup><sub><em>N</em></sub>(<em>A</em>).</td>
</tr>
</table>
</blockquote>

</blockquote>

<p>

Common knowledge of a proposition <em>E</em> implies common
knowledge of all that <em>E</em> implies, as is shown in the
following:</p>

<blockquote><strong>Proposition 2.4</strong>
<br />
 If &omega; &isin;
<strong>K</strong><sup>*</sup><sub><em>N</em></sub>(<em>E</em>) and
<em>E</em> &sube; <em>F</em>, then &omega; &isin;
<strong>K</strong><sup>*</sup><sub><em>N</em></sub>(<em>F</em>). 

<p>

 <a href="proof2.4.html" name="return-2.4">Proof</a>.</p>
</blockquote>

<p>

Note that
(<strong>K</strong><sup><em>m</em></sup><sub><em>N</em></sub>(<em>E</em>))<sub>
<em>m</em>&ge;1</sub> is a decreasing sequence of events, in the sense
that
<strong>K</strong><sup><em>m</em>+1</sup><sub><em>N</em></sub>(<em>E</em>)
&sube;
<strong>K</strong><sup><em>m</em></sup><sub><em>N</em></sub>(<em>E</em>),
for all <em>m</em> &ge; 1. It is also easy to check that if everyone
knows <em>E</em>, then <em>E</em> must be true, that is,
<strong>K</strong><sup>1</sup><sub><em>N</em></sub>(<em>E</em>) &sube;
<em>E</em>. If &Omega; is assumed to be finite, then if <em>E</em> is
common knowledge at &omega;, this implies that there must be a finite
<em>m</em> such that</p>

<blockquote>
<table>
<tr>
<td><strong>K</strong><sup><em>m</em></sup><sub><em>N</em></sub>(<em>E</em>) = </td>
<td align="center">
    <span class="index">&infin;</span><br />
    <font size="+2">&cap;</font><br />
    <span class="index"><em>n</em> = 1</span>
</td>
<td><strong>K</strong><sup><em>n</em></sup><sub><em>N</em></sub>(<em>E</em>).</td>
</tr>
</table>
</blockquote>

<p>

The following result relates the set-theoretic definition of common
knowledge to the hierarchy of &lsquo;<em>i</em> knows that <em>j</em>
knows that &hellip; knows <em>A</em>&rsquo; statements.</p>

<blockquote>
<p><strong>Proposition 2.5</strong>
<br />
 &omega; &isin;
<strong>K</strong><sup><em>m</em></sup><sub><em>N</em></sub>(<em>A</em>)
iff</p> 

<p>(1) For all agents <em>i</em><sub>1</sub>,
<em>i</em><sub>2</sub>, &hellip; , <em>i</em><sub><em>m</em></sub>
&isin; <em>N</em>, &omega; &isin;
<strong>K</strong><sub><em>i</em><sub>1</sub></sub><strong>K</strong><sub><em>i</em><sub>2</sub></sub>
 &hellip;
 <strong>K</strong><sub><em>i</em><sub><em>m</em></sub></sub>(<em>A</em>)
</p>

<p>Hence, &omega; &isin;
<strong>K</strong><sup>*</sup><sub><em>N</em></sub>(<em>A</em>) iff (1)
is the case for each <em>m</em> &ge; 1. </p>

<p>

 <a href="proof2.5.html" name="return-2.5">Proof</a>.</p>
</blockquote>

<p>

The condition that &omega; &isin;
 <strong>K</strong><sub><em>i</em><sub>1</sub></sub><strong>K</strong><sub><em>i</em><sub>2</sub></sub>
 &hellip;
 <strong>K</strong><sub><em>i</em><sub><em>m</em></sub></sub>(<em>A</em>)
for all <em>m</em> &ge; 1 and all <em>i</em><sub>1</sub>,
<em>i</em><sub>2</sub>, &hellip; , <em>i</em><sub><em>m</em></sub>
&isin; <em>N</em> is Schiffer's definition of common knowledge, and is
often used as the definition of common knowledge in the literature.</p>

<h3><a name="2.2">2.2 Lewis' Account</a></h3>

<p>

Lewis is credited with the idea of characterizing common knowledge as
a hierarchy of &lsquo;<em>i</em> knows that <em>j</em> knows that
&hellip; knows that <em>A</em>&rsquo; propositions. However, Lewis is
aware of the difficulties that such an infinitary definition raises. A
first problem is whether it is possible to reduce the infinity
inherent in the hierarchical account into a workable finite
definition. A second problem is the issue that finite agents cannot
entertain the infinite amount of epistemic states which is necessary
for common knowledge to obtain. Lewis tackles both problems, but his
presentation is informal.  Aumann is often credited with presenting
the first finitary method of generating the common knowledge hierarchy
(Aumann 1976), even though (Friedell 1969) in fact predates both
Aumann's and Lewis's work. Recently, Cubitt and Sugden (2003) have
argued that Aumann's and Lewis' accounts of common knowledge are
radically different and irreconcilable.</p>

<p>

Although Lewis introduced the technical term &lsquo;common
knowledge,&rsquo; his analysis is about belief, rather than
knowledge. Indeed, Lewis offers his solution to the second problem
mentioned above by introducing a distinction between <em>actual
belief</em> and <em>reason to believe</em>. Reasons to believe are
interpreted as potential beliefs of agents, so that the infinite
hierarchy of epistemic states becomes harmless, consisting in an
infinite number of states of potential belief. The solution to the
first problem is given by providing a finite set of conditions that,
if met, generate the infinite series of reasons to believe. Such
conditions taken together represent Lewis' official definition of
common knowledge. Notice that it would be more appropriate to speak of
&lsquo;common reason to believe,&rsquo; or, at least, of &lsquo;common
belief.&rsquo; Lewis himself later acknowledges that &ldquo;[t]hat
term [common knowledge] was unfortunate, since there is no assurance
that it will be knowledge, or even that it will be true.&rdquo;
Cf. (Lewis 1978, p.  44, n.13) Disregarding the distinction between
reasons to believe and actual belief, we follow (Vanderschraaf 1998)
to give the details of a formal account of Lewis's definition here,
and show that Lewis' analysis does result in the common knowledge
hierarchy following from a finite set of axioms. It is however
debatable whether a possible worlds approach can properly render the
subtelties of Lewis' characterization. Cubitt and Sugden (2003), for
example, abandon the possible worlds framework altogether and propose
a different formal interpretation of Lewis in which, among other
elements, the distinction between reasons to believe and actual belief
is taken into account. An attempt to reconcile the two positions can
be found in (Sillari 2005), where Lewis' characterization is
formalized in a richer possible worlds semantic framework where the
distinction between reasons to believe and actual believe is
represented.</p>

<p>

Lewis presents his account of common knowledge on pp. 52&ndash;57 of
<em>Convention</em>. Lewis does not specify what account of knowledge
is needed for common knowledge. As it turns out, Lewis' account is
satisfactory for any formal account of knowledge in which the knowledge
operators <strong>K</strong><sub><em>i</em></sub>, <em>i</em> &isin;
<em>N</em>, satisfy K1, K2, and K3. A crucial assumption in Lewis'
analysis of common knowledge is that agents know they share the same
&ldquo;rationality, inductive standards and background
information&rdquo; (Lewis 1969, p. 53) with respect to a state of
affairs <em>A</em>&prime;, that is, if an agent can draw any
conclusion from <em>A</em>&prime;, she knows that all can do
likewise. This idea is made precise in the following:</p>

<blockquote><strong>Definition 2.6</strong>
<br />
 Given a set of agents <em>N</em> and a proposition <em>A</em>&prime;
&sube; &Omega;, the agents of <em>N</em> are <em>symmetric reasoners
with respect to</em> <em>A</em>&prime; (<em>or</em>
<em>A</em>&prime;<em>-symmetric reasoners</em>) iff, for each
<em>i</em>, <em>j</em> &isin; <em>N</em> and for any proposition
<em>E</em> &sube; &Omega;, if
<strong>K</strong><sub><em>i</em></sub>(<em>A</em>&prime;) &sube;
<strong>K</strong><sub><em>i</em></sub>(<em>E</em>) and
<strong>K</strong><sub><em>i</em></sub>(<em>A</em>&prime;) &sube;
<strong>K</strong><sub><em>i</em></sub><strong>K</strong><sub><em>j</em></sub>(<em>
A</em>&prime;), then
<strong>K</strong><sub><em>i</em></sub>(<em>A</em>&prime;) &sube;
<strong>K</strong><sub><em>i</em></sub><strong>K</strong><sub><em>j</em></sub>(<em>
 E</em>).<sup>[<a href="notes.html#13" name="note-13">13</a>]</sup></blockquote>

<p>

The definiens says that for each agent <em>i</em>, if <em>i</em> can
infer from <em>A</em>&prime; that <em>E</em> is the case and that
everyone knows that <em>A</em>&prime; is the case, then <em>i</em> can
also infer that everyone knows that <em>E</em> is the case.</p>

<blockquote><strong>Definition 2.7</strong>
<br />
 A proposition <em>E</em> is <em>Lewis-common knowledge at</em> &omega;
&isin; &Omega; among the agents of a set <em>N</em> = {1, &hellip; ,
<em>n</em>} iff there is a proposition <em>A</em>* such that &omega;
&isin; <em>A</em>*, the agents of <em>N</em> are <em>A</em>*-symmetric
reasoners, and for every <em>i</em> &isin; <em>N</em>, 

<blockquote>L1: &nbsp; &omega; &isin;
<strong>K</strong><sub><em>i</em></sub>(<em>A</em>*) 

<p>

L2: &nbsp; <strong>K</strong><sub><em>i</em></sub>(<em>A</em>*)
&sube; <strong>K</strong><sub><em>i</em></sub>(<font size="+2">&cap;</font><sub><em>j</em>&isin;<em>N</em></sub>
<strong>K</strong><sub><em>j</em></sub>(<em>A</em>*))</p>

<p>

L3: &nbsp; <strong>K</strong><sub><em>i</em></sub>(<em>A</em>*)
&sube; <strong>K</strong><sub><em>i</em></sub>(<em>E</em>)</p>
</blockquote>

<p>

<em>A</em>* is a <em>basis</em> for the agents' common knowledge.
<strong>L</strong>*<sub><em>N</em></sub> (<em>E</em>) denotes the
proposition defined by L1 &ndash; L3 for a set <em>N</em> of
<em>A</em>*-symmetric reasoners, so we can say that <em>E</em> is
Lewis-common knowledge for the agents of <em>N</em> iff
&omega;&nbsp;&isin;&nbsp;<strong>L</strong>*<sub><em>N</em></sub>(<em>E</em>).</p>
</blockquote>

<p>

In words, L1 says that <em>i</em> knows <em>A</em>* at &omega;. L2
says that if <em>i</em> knows that <em>A</em>* obtains, then <em>i</em>
knows that everyone knows that <em>A</em>* obtains. This axiom is meant
to capture the idea that common knowledge is based upon a proposition
<em>A</em>* that is <em>publicly known</em>, as is the case when agents
hear a public announcement. If the agents' knowledge is represented by
partitions, then a typical basis for the agents' common knowledge would
be an element
 <span class="scriptuc">M</span>(&omega;)
 in
the
 meet<sup>[<a href="notes.html#14" name="note-14">14</a>]</sup>
 of their partitions. L3 says that
<em>i</em> can infer from <em>A</em>* that <em>E</em>. Lewis'
definition implies the entire common knowledge hierarchy, as is shown
in the following result.</p>

<blockquote><strong>Proposition 2.8</strong>
<br />
 <strong>L</strong>*<sub><em>N</em></sub>(<em>E</em>) &sube;
<strong>K</strong>*<sub><em>N</em></sub> (<em>E</em>), that is,
Lewis-common knowledge of <em>E</em> implies common knowledge of
<em>E</em>. 

<p>

 <a href="proof2.8.html" name="return-2.8">Proof</a>.</p>
</blockquote>

<p>

As mentioned above, it has recently come into question whether a
formal rendition of Lewis' definition as the one given above
adeguately represents all facets of Lewis' approach. Cubitt and Sugden
(2003) argue that it does not, their critique hinging on a feature of
Lewis' analysis that is lost in the possible worlds framework, namely
the 3-place relation of <em>indication</em> used by Lewis. The
definition of indication can be found at pp. 52&ndash;53
of <em>Convention</em>:</p>

<blockquote><strong>Definition 2.9</strong>
<br />
 A state of affairs <em>A</em> <em>indicates</em> <em>E</em> to agent
<em>i</em> (<em>A ind<sub>i</sub> E</em>) if and only if, if <em>i</em>
had reason to believe that <em>A</em> held, <em>i</em> would thereby
have reason to believe that <em>E</em></blockquote>

<p>

The wording of Lewis' definition and the use he makes of the
indication relation in the definitory clauses for common knowledge,
suggest that Lewis is careful to distinguish indication and material
implication. Cubitt and Sugden (2003) incorporate such distinction in
their formal reconstruction. Paired with their interpretation of
&ldquo;<em>i</em> has reason to believe <em>x</em>&rdquo; as
&ldquo;<em>x</em> is yielded by some logic of reasoning
that <em>i</em> endorses,&rdquo; we have that, if <em>A
ind<sub>i</sub> x</em>, then <em>i</em>'s reason to believe <em>A</em>
provides <em>i</em> with reason to believe <em>x</em> as well. Given
that Lewis does want to endow agents with deductive reasoning, (Cubitt
and Sugden 2003) list the following axioms, claiming that they capture
the desired properties of indication. For all agents <em>i, j</em>,
with <strong>R</strong><sub><em>i</em></sub>
<em>A</em> standing for &ldquo;agent <em>i</em> has reason to believe A&rdquo;, we have</p>

<blockquote>CS1: &nbsp; (<strong>R</strong><sub><em>i</em></sub>
<em>A</em> &and; <em>A</em> <em>ind</em><sub><em>i</em></sub>
<em>x</em>) &rarr; <strong>R</strong><sub><em>i</em></sub> <em>x</em>. 

<p>

CS2: &nbsp; (<em>A</em> entails <em>B</em>) &rarr; <em>A</em>
<em>ind</em><sub><em>i</em></sub> <em>B</em></p>

<p>

CS3: &nbsp; (<em>A</em> <em>ind</em><sub><em>i</em></sub> <em>x</em>
&and; <em>A</em> <em>ind</em><sub><em>i</em></sub> <em>y</em>) &rarr;
<em>A</em> <em>ind</em><sub><em>i</em></sub> (<em>x</em> &and;
<em>y</em>)</p>

<p>

CS4: &nbsp; (<em>A ind<sub>i</sub> B</em> &and; <em>B
ind<sub>i</sub> x</em>) &rarr; <em>A ind<sub>i</sub> x</em></p>

<p>

CS5: &nbsp; ((<em>A ind<sub>i</sub></em>
<strong>R</strong><em><sub>j</sub> B</em>) &and;
<strong>R</strong><em><sub>i</sub></em>(<em>B ind<sub>j</sub> x</em>))
&rarr; <em>A ind<sub>i</sub></em> <strong>R</strong><em><sub>j</sub>
x</em></p>
</blockquote>

<p>

The first axioms captures the intuition behind indication. It says
that if an agent has reason to believe that <em>A</em> holds, then, if
<em>A</em> indicates <em>x</em> to her, she has reason to believe
<em>x</em> as well. CS2 says that indication extends material
implication. CS3 says that if two propositions <em>x</em> and
<em>y</em> are indicated to an agent by a proposition <em>A</em>, then
<em>A</em> indicates to her also the conjunction of <em>x</em> and
<em>y</em>. The next axiom states that indication is transitive. CS5
says that if a proposition <em>A</em> indicates to <em>i</em> that
agent <em>j</em> has reason to believe <em>B</em>, and <em>i</em> has
reason to believe that <em>B</em> indicates <em>x</em> to <em>j</em>,
then <em>A</em> indicates to <em>i</em> also that <em>j</em> has reason
to believe <em>x</em>.</p>

<p>

Armed with these axioms, it is possible to give the following
definition.</p>

<blockquote><strong>Definition 2.10</strong>
<br />
 In any given population <em>P</em> a proposition <em>A</em> is a
<em>reflexive common indicator that x</em> if and only if, for all
<em>i, j</em> &isin; <em>P</em> and all propositions <em>x, y</em>, the
following four conditions hold: 

<p>

RCI1: &nbsp; <em>A</em> &rarr; <em><strong>R</strong><sub>i</sub>
A</em></p>

<p>

RCI2: &nbsp; <em>A ind<sub>i</sub> <strong>R</strong><sub>j</sub>
A</em></p>

<p>

RCI3: &nbsp; <em>A ind<sub>i</sub> x</em></p>

<p>

RCI4: &nbsp; <em>A ind<sub>j</sub>y</em> &rarr;
<em><strong>R</strong><sub>i</sub></em>(<em>A ind<sub>j</sub>
y</em>)</p>
</blockquote>

<p>

Clauses RCI1-RCI3 above render L1-L3 of definition 2.7 above in the
formal language that underlies axioms CS1-CS5; while RCI4 affirms (cf.
definition 2.6 above) that agents are symmetric reasoners, i.e. that if
a proposition indicates another proposition to a certain agent, then it
does so to all agents in the population.</p>

<p>

The following proposition shows that RCI1-RCI4 are sufficient
conditions for &lsquo;common reason to believe&rsquo; to arise:</p>

<blockquote><strong>Proposition 2.11</strong>
<br />
 If <em>A</em> holds, and if <em>A</em> is a common reflexive indicator
in the population <em>P</em> that <em>x</em>, then there is common
reason to believe in <em>P</em> that <em>x</em>. 

<p>

 <a href="proof2.11.html" name="return-2.11">Proof</a>.</p>
</blockquote>

<p>

A group of (ideal) <em>faultless reasoners</em> who have common reason
to believe that <em>p</em>, will achieve common belief
in <em>p</em>.</p>

<p>

Is it possible to take formally in account the insights of Lewis'
definition of common knowledge without abandoning the possible world
framework? (Sillari 2005) puts forth an attempt to give a postive
answer to that question by articulating in a possible world semantics
the distinction between actual belief and reason to believe. As in
(Cubitt and Sugden 2003), the basic epistemic operator represents
reasons to believe. The idea is then to impose an <em>awareness
structure</em> over possible worlds, adopting the framework first
introduced by Fagin and Halpern (1988). Simply put, an awareness
structure associates to each agent, for every possible world, a set of
events of which the agent is said to be aware. An agent entertains an
actual belief that a certain event occurs if and only if she has
reason to believe that the event occurs <em>and</em> such event is in
her awareness set at the world under consideration.</p>

<h3><a name="2.3">2.3 Aumann's Account</a></h3>

<p>

Aumann (1976) gives a different characterization of common knowledge
which gives another simple algorithm for determining what information
is commonly known. Aumann's original account assumes that the each
agent's possibility set forms a private information partition of the
space &Omega; of possible worlds. Aumann shows that a proposition C is
common knowledge if, and only if, C contains a cell of the meet of the
agents' partitions. One way to compute the meet
 <span class="scriptuc">M</span>
 of the partitions
 <span class="scriptuc">H</span><sub><em>i</em></sub>, <em>i</em> &isin;
<em>N</em> is to use the idea of &ldquo;reachability&rdquo;.</p>

<blockquote><strong>Definition 2.13</strong>
<br />
 A state &omega;&prime; &isin; &Omega; is <em>reachable</em> from
&omega; &isin; &Omega; iff there exists a sequence
&omega;=&omega;<sub>0</sub>, &omega;<sub>1</sub>, &omega;<sub>2</sub>,
&hellip; , &omega;<sub>m</sub>=&omega;&prime; such that for each
<em>k</em> &isin; {0,1, &hellip; , <em>m</em>&minus;1}, there exists an
agent <em>i</em><sub><em>k</em></sub> &isin; <em>N</em> such that
 <span class="scriptuc">H</span><sub><em>i</em><sub><em>k</em></sub></sub>(&omega;<sub><em>k</em></sub>)
=
 <span class="scriptuc">H</span><sub><em>i</em><sub><em>k</em></sub></sub>(&omega;<sub><em>k</em>+1</sub>).</blockquote>

<p>

In words, &omega;&prime; is reachable from &omega; if there exists a
sequence or &ldquo;chain&rdquo; of states from &omega; to &omega;&prime; such that
two consecutive states are in the same cell of some agent's information
partition. To illustrate the idea of reachability, let us return to the
modified Barbecue Problem in which Cathy, Jennifer and Mark receive no
announcement. Their information partitions are all depicted in Figure
2.1d:</p>

<blockquote><img alt="missing text, please inform" src="figure2.1d.gif" />
<br />
 <strong>Figure 2.1d</strong></blockquote>

<p>

One can understand the importance of the notion of reachability in
the following way: If &omega;&prime; is reachable from &omega;, then if
&omega; obtains then some agent can reason that some other agent thinks
that &omega;&prime; is possible. Looking at Figure 2.1d, if &omega; =
&omega;<sub>1</sub> occurs, then Cathy (who knows only that
{&omega;<sub>1</sub>, &omega;<sub>2</sub>} has occurred) knows that
Jennifer thinks that &omega;<sub>5</sub> might have occurred (even
though Cathy knows that &omega;<sub>5</sub> did not occur). So Cathy
cannot rule out the possibility that Jennifer thinks that Mark thinks
that that &omega;<sub>8</sub> might have occurred. And Cathy cannot
rule out the possibility that Jennifer thinks that Mark thinks that
Cathy believes that &omega;<sub>7</sub> is possible. In this sense,
&omega;<sub>7</sub> is reachable from &omega;<sub>1</sub>. The chain of
states which establishes this is &omega;<sub>1</sub>, &omega;
<sub>2</sub>, &omega;<sub>5</sub>, &omega;<sub>8</sub>,
&omega;<sub>7</sub>, since
 <span class="scriptuc">H</span><sub>1</sub>(&omega;<sub>1</sub>) =
 <span class="scriptuc">H</span><sub>1</sub>(&omega;<sub>2</sub>),
 <span class="scriptuc">H</span><sub>2</sub>(&omega;<sub>2</sub>)
=
 <span class="scriptuc">H</span><sub>2</sub>(&omega;<sub>5</sub>),
 <span class="scriptuc">H</span><sub>3</sub>(&omega;<sub>5</sub>)
=
 <span class="scriptuc">H</span><sub>3</sub>(&omega;<sub>8</sub>), and
 <span class="scriptuc">H</span><sub>1</sub>(&omega;<sub>8</sub>)
=
 <span class="scriptuc">H</span><sub>1</sub>(&omega;<sub>7</sub>). Note that one
can show similarly that in this example any state is reachable from any
other state. This example also illustrates the following immediate
result:</p>

<blockquote><strong>Proposition 2.14</strong>
<br />
 &omega;&prime; is reachable from &omega; iff there is a sequence
<em>i</em><sub>1</sub>, <em>i</em><sub>2</sub>, &hellip; ,
<em>i</em><sub><em>m</em></sub> &isin; <em>N</em> such that 

<blockquote>(1) &omega;&prime; &isin;
 <span class="scriptuc">H</span><sub><em>i</em><sub><em>m</em></sub></sub>( &hellip;
 (<span class="scriptuc">H</span><sub><em>i</em><sub>2</sub></sub>(<span class="scriptuc">H</span><sub><em>i</em><sub>1</sub></sub>(&omega;))))</blockquote>
</blockquote>

<p>

One can read (1) as: &lsquo;At &omega;, <em>i</em><sub>1</sub>
thinks that <em>i</em><sub>2</sub> thinks that &hellip; ,
<em>i</em><sub><em>m</em></sub> thinks that &omega;&prime; is
possible.&rsquo;</p>

<p>

We now have:</p>

<blockquote><strong>Lemma 2.15</strong>
<br />
 &omega;&prime; &isin;
 <span class="scriptuc">M</span>(&omega;)
 iff &omega;&prime; is reachable from
&omega;. 

<p>

 <a href="proof2.15.html" name="return-2.15">Proof</a>.</p>
</blockquote>

<p>

and </p>

<blockquote><strong>Lemma 2.16</strong>
<br />
 <span class="scriptuc">M</span>(&omega;)
 is common
knowledge for the agents of <em>N</em> at &omega;. 

<p>

 <a href="proof2.16.html" name="return-2.16">Proof</a>.</p>
</blockquote>

<p>

and </p>

<blockquote><strong>Proposition 2.17</strong> (Aumann 1976)
<br />
 Let
 <span class="scriptuc">M</span>
 be the meet of the
agents' partitions
 <span class="scriptuc">H</span><sub><em>i</em></sub> for each <em>i</em> &isin;
<em>N</em>. A proposition <em>E</em> &sube; &Omega; is common knowledge
for the agents of <em>N</em> at &omega; iff
 <span class="scriptuc">M</span>(&omega;)
 &sube; <em>E</em>. (In Aumann (1976),
<em>E</em> is <em>defined</em> to be common knowledge at &omega;
 iff
 <span class="scriptuc">M</span>(&omega;) &sube;
<em>E</em>.) 

<p>

 <a href="proof2.17.html" name="return-2.17">Proof</a>.</p>
</blockquote>

<p>

If <em>E</em> =
<strong>K</strong><sup>1</sup><sub><em>N</em></sub>(<em>E</em>), then
<em>E</em> is a <em>public event</em> (Milgrom 1981) or a <em>common
truism</em> (Binmore and Brandenburger 1989). Clearly, a common truism
is common knowledge whenever it occurs, since in this case <em>E</em> =
<strong>K</strong><sup>1</sup><sub><em>N</em></sub>(<em>E</em>) =
<strong>K</strong><sup>2</sup><sub><em>N</em></sub>(<em>E</em>) =
&hellip; , so <em>E</em> =
<strong>K</strong><sup>*</sup><sub>N</sub>(<em>E</em>). The proof of
Proposition 2.17 shows that the common truisms are precisely the
elements of
 <span class="scriptuc">M</span>
 and unions of
elements of
 <span class="scriptuc">M</span>,
 so any
commonly known event is the consequence of a common truism.</p>

<h3><a name="2.4">2.4 Barwise's Account</a></h3>

<p>

Barwise (1988) proposes another definition of common knowledge that
avoids explicit reference to the hierarchy of &lsquo;<em>i</em> knows
that <em>j</em> knows that &hellip; knows that <em>A</em>&rsquo;
propositions. Barwise's analysis builds upon an informal proposal by
Harman (1977). Consider the situation of the guest and clumsy waiter in
Example 1 when he announces that he was at fault. They are now in a
setting where they have heard the waiter's announcement and know that
they are in the setting. Harman adopts the circularity in this
characterization of the setting as fundamental, and propses a
definition of common knowledge in terms of this circularity. Barwise's
formal analysis gives a precise formulation of Harman's intuitive
analysis of common knowledge as a <em>fixed point.</em> Given a
function <em>f, A</em> is a fixed point of <em>f</em> if
<em>f(A)=A</em>. Now note that</p>

<table cellpadding="0" cellspacing="0">

<tr>
<td>
<table>
<tr>
<td><strong>K</strong><sup>1</sup><sub><em>N</em></sub>
(<em>E</em> &cap;</td>
<td align="center">
    <span class="index">&infin;</span><br />
    <font size="+2">&cap;</font><br />
    <span class="index"><em>m</em>=1</span>
</td>
<td><strong>K</strong><sup>m</sup><sub><em>N</em></sub>
(<em>E</em>))</td>
</tr>
</table>
</td>

<td align="center">=</td>

<td>
<table>
<tr>
<td><strong>K</strong><sup>1</sup><sub><em>N</em></sub>
(<em>E</em>) &cap;
<strong>K</strong><sup>1</sup><sub><em>N</em></sub> (</td>
<td align="center">
    <span class="index">&infin;</span><br />
    <font size="+2">&cap;</font><br />
    <span class="index"><em>m</em>=1</span>
</td>
<td><strong>K</strong><sup>m</sup><sub><em>N</em></sub>
(<em>E</em>))</td>
</tr>
</table>
</td>

</tr>

<tr>
<td></td>
<td align="center">=</td>
<td>
<table>
<tr>
<td><strong>K</strong><sup>1</sup><sub><em>N</em></sub>
(<em>E</em>) &cap; (</td>
<td align="center">
    <span class="index">&infin;</span><br />
    <font size="+2">&cap;</font><br />
    <span class="index"><em>m</em>=1</span>
</td>
<td><strong>K</strong><sup>1</sup><sub><em>N</em></sub>
(<strong>K</strong><sup>m</sup><sub><em>N</em></sub> (<em>E</em>)))</td>
</tr>
</table>
</td>
</tr>

<tr>
<td></td>
<td align="center">=</td>
<td>
<table>
<tr>
<td><strong>K</strong><sup>1</sup><sub><em>N</em></sub>
(<em>E</em>) &cap; (</td>
<td align="center">
    <span class="index">&infin;</span><br />
    <font size="+2">&cap;</font><br />
    <span class="index"><em>m</em>=1</span>
</td>
<td><strong>K</strong><sup>m</sup><sub><em>N</em></sub>
(<em>E</em>))</td>
</tr>
</table>
</td>
</tr>

<tr>
<td></td>
<td align="center">=</td>
<td>
<table>
<tr>
<td align="center">
    <span class="index">&infin;</span><br />
    <font size="+2">&cap;</font><br />
    <span class="index"><em>m</em>=1</span>
</td>
<td><strong>K</strong><sup>m</sup><sub><em>N</em></sub>
(<em>E</em>)</td>
</tr>
</table>
</td>
</tr>
</table>

<p>

So we have established that
<strong>K</strong><sup>*</sup><sub><em>N</em></sub> (<em>E</em>) is a
fixed point of the function <em>f</em><sub><em>E</em></sub> defined by
<em>f</em><sub><em>E</em></sub>(<em>X</em>) =
<strong>K</strong><sup>1</sup><sub><em>N</em></sub> (<em>E</em> &cap;
<em>X</em>). <em>f</em><sub>E</sub> has other fixed points. For
instance, any contradiction <em>B</em> &cap; <em>B<sup>c</sup></em> =
&oslash; is a fixed point of
 <em>f</em><sub>E</sub>.<sup>[<a href="notes.html#15" name="note-15">15</a>]</sup>
 Note
also that if <em>A</em> &sube; <em>B</em>, then <em>E</em> &cap;
<em>A</em> &sube; <em>E</em> &cap; <em>B</em> and so </p>

<blockquote><em>f</em><sub><em>E</em></sub>(<em>A</em>) =
<strong>K</strong><sup>1</sup><sub><em>N</em></sub> (<em>E</em> &cap;
<em>A</em>) &sube; <strong>K</strong><sup>1</sup><sub><em>N</em></sub>
(<em>E</em> &cap; <em>B</em>) =
<em>f</em><sub><em>E</em></sub>(<em>B</em>)</blockquote>

<p>

that is, <em>f</em><sub><em>E</em></sub> is <em>monotone.</em> (We saw
that <strong>K</strong><sup>1</sup><sub><em>N</em></sub> is also
monotone in the proof of Proposition 2.4.) Barwise's analysis of common
knowledge can be developed using the following result from set theory: </p>

<blockquote><strong>Proposition</strong>
<br />
 A monotone function <em>f</em> has a unique fixed point <em>C</em>
such that if <em>B</em> is a fixed point of <em>f</em>, then
<em>B</em>&sube;<em>C</em>. <em>C</em> is the <em>greatest fixed point
of f.</em></blockquote>

<p>

This proposition establishes that <em>f<sub>E</sub></em> has a greatest
fixed point, which characterizes common knowledge in Barwise's account.
As Barwise himself observes, the fixed point analysis of common
knowledge is closely related to Aumann's partition account. This is
easy to see when one compares the fixed point analysis to the notion of
common truisms that Aumann's account generates. Some authors regard the
fixed point analysis as an alternate formulation of Aumann's analysis.
Barwise's fixed point analysis of common knowledge is favored by those
who are especially interested in the applications of common knowledge
to problems in logic, while the hierarchical and the partition accounts
are favored by those who wish to apply common knowledge in social
philosophy and social science. When knowledge operators satisfy the
axioms (K1)-(K5), the Barwise account of common knowledge is equivalent
to the hierarchical account. </p>

<blockquote><strong>Proposition 2.18</strong>
<br />
 Let <em>C</em><sup>*</sup><sub><em>N</em></sub> be the greatest fixed
point of <em>f</em><sub><em>E</em></sub>. Then
<em>C</em><sup>*</sup><sub><em>N</em></sub>(<em>E</em>) =
<em>K</em><sup>*</sup><sub><em>N</em></sub>(<em>E</em>). ( In Barwise
(1988, 1989), <em>E</em> is <em>defined</em> to be common knowledge at
&omega; iff &omega; &isin;
<em>C</em><sup>*</sup><sub><em>N</em></sub>(<em>E</em>).) 

<p>

 <a href="proof2.18.html" name="return-2.18">Proof</a>.</p>
</blockquote>

<p>

Barwise argues that in fact the fixed point analysis is more flexible
and consequently more general than the hierachical account.  This may
surprise readers in light of Proposition 2.18, which shows that
Barwise's fixed point definition is <em>equivalent</em> to the
hierarchical account. Indeed, while Barwise (1988, 1989) proves a
result showing that the fixed point account implies the hierarchical
account and gives examples that satisfy the common knowledge hierarchy
but fail to be fixed points, a number of authors who have written
after Barwise have given various proofs of the equivalence of the two
definitions, as was shown in Proposition 2.18. In fact, as (Heifetz
1999) shows, the hierarchical and fixed-point accounts are equivalent
for all finite levels of iteration, while fixed-point common knowledge
implies the conjunction of mutual knowledge up to any transfinite
order, but it is never implied by any such conjunction.</p>

<h3><a name="2.5">2.5 Gilbert's Account</a></h3>

<p>

Gilbert (1989, Chapter 3) presents an alternative account of common
knowledge, which is meant to be more intuitively plausible than Lewis'
and Aumann's accounts. Gilbert gives a highly detailed description of
the circumstances under which agents have common knowledge.</p>

<blockquote><strong>Definition 2.19</strong>
<br />
 A set of agents <em>N</em> are in a <em>common knowledge
situation</em>
 <span class="scriptuc">S</span>(<em>A</em>)
with respect to a proposition <em>A</em> if, and only if, &omega;
&isin; <em>A</em> and for each <em>i</em> &isin; <em>N</em>, 

<table>
<tr>
<td valign="top">G<sub>1</sub>:</td>
<td valign="top"><em>i</em> is <em>epistemically normal</em>, in the
sense that <em>i</em> has normal perceptual organs which are
functioning normally and has normal reasoning
 capacity.<sup>[<a href="notes.html#16" name="note-16">16</a>]</sup></td>
</tr>

<tr>
<td>G<sub>2</sub>:</td>
<td><em>i</em> has the concepts needed to fulfill the other
conditions.</td>
</tr>

<tr>
<td>G<sub>3</sub>:</td>
<td><em>i</em> perceives the other agents of <em>N</em>.</td>
</tr>

<tr>
<td>G<sub>4</sub>:</td>
<td><em>i</em> perceives that G<sub>1</sub> and G<sub>2</sub> are the
case.</td>
</tr>

<tr>
<td>G<sub>5</sub>:</td>
<td><em>i</em> perceives that the state of affairs described by
<em>A</em> is the case.</td>
</tr>

<tr>
<td>G<sub>6</sub>:</td>
<td><em>i</em> perceives that all the agents of <em>N</em> perceive
that <em>A</em> is the case.</td>
</tr>
</table>

</blockquote>

<p>

Gilbert's definition appears to contain some redundancy, since
presumably an agent would not perceive A unless A is the case. Gilbert
is evidently trying to give a more explicit account of single agent
knowledge than Lewis and Aumann give. For Gilbert, agent <em>i</em>
knows that a proposition <em>E</em> is the case if, and only if,
&omega; &isin; <em>E</em>, that is, <em>E</em> is true, and either
<em>i</em> perceives that the state of affairs <em>E</em> describes
obtains or <em>i</em> can infer <em>E</em> as a consequence of other
propositions <em>i</em> knows, given sufficient inferential
capacity.</p>

<p>

Like Lewis, Gilbert recognizes that human agents do not in fact have
unlimited inferential capacity. To generate the infinite hierarchy of
mutual knowledge, Gilbert introduces the device of an agent's
<em>smooth-reasoner counterpart.</em> The smooth-reasoner counterpart
<em>i</em>&prime; of an agent <em>i</em> is an agent that draws every
logical conclusion from every fact that <em>i</em> knows. Gilbert
stipulates that <em>i</em>&prime; does not have any of the constrains
on time, memory, or reasoning ability that <em>i</em> might have, so
<em>i</em>&prime; can literally think through the infinitely many
levels of a common knowledge hierarchy.</p>

<blockquote><strong>Definition 2.20</strong>
<br />
 If a set of agents <em>N</em> are in a common knowledge situation
 <span class="scriptuc">S</span><sub><em>N</em></sub>(<em>A</em>)
with respect to <em>A</em>, then the corresponding set
<em>N</em>&prime; of their smooth-reasoner counterparts is in a
<em>parallel situation</em>
 <span class="scriptuc">S</span>&prime;<sub><em>N</em>&prime;</sub>(<em>A</em>)
if, and only if, for each <em>i</em>&prime; &isin; <em>N</em>, 

<table>
<tr>
<td valign="top">G<sub>1</sub>&prime;:</td>
<td><em>i</em>&prime; can perceive anything that the counterpart
<em>i</em> can perceive.</td>
</tr>

<tr>
<td valign="top">G<sub>2</sub>&prime;:</td>
<td>G<sub>2</sub> &ndash; G<sub>6</sub> obtain for <em>i</em>&prime; with
respect to <em>A</em> and <em>N</em>&prime;, same as for the
counterpart <em>i</em> with respect to <em>A</em> and <em>N</em>.</td>
</tr>

<tr>
<td valign="top">G<sub>3</sub>&prime;:</td>
<td><em>i</em>&prime; perceives that all the agents of
<em>N</em>&prime; are smooth-reasoners.</td>
</tr>
</table>
</blockquote>

<p>

From this definition we get the following immediate consequence:</p>

<blockquote><strong>Proposition 2.21</strong>
<br />
 If a set of smooth-reasoner counterparts to a set <em>N</em> of agents
are in a situation
 <span class="scriptuc">S</span>&prime;<sub><em>N</em>&prime;</sub>(<em>A</em>)
parallel to a common knowledge situation
 <span class="scriptuc">S</span><sub>N</sub>(<em>A</em>)
 of <em>N</em>, then 
  <blockquote>
  for all <em>m</em> &isin;
 <img src="Num.gif" alt="the natural numbers" /> and for any
<em>i</em><sub>1</sub>&prime;, &hellip; ,
 <em>i</em><sub>m</sub>&prime;,
 <strong>K</strong><sub><em>i</em><sub>1</sub>&prime;</sub><strong>K</strong><sub><em>i</em><sub>2</sub>&prime;</sub> &hellip;
<strong>K</strong><sub><em>i</em><sub><em>m</em></sub>&prime;</sub>(<em>A</em>).
  </blockquote>

 <p>

Consequently,
<strong>K</strong><sup><em>m</em></sup><sub><em>N</em>&prime;</sub>(<em>A</em>)
 for any <em>m</em> &isin;
 <img src="Num.gif" alt="the natural numbers" />.</p></blockquote>

<p>

Gilbert argues that, given
 <span class="scriptuc">S</span>&prime;<sub><em>N</em>&prime;</sub>(<em>A</em>),
the smooth-reasoner counterparts of the agents of <em>N</em> actually
satisfy a much stronger condition, namely mutual knowledge
<strong>K</strong><sup>&alpha;</sup><sub><em>N</em>&prime;</sub>(<em>A</em>)
to the level of any ordinal number &alpha;, finite or infinite. When
this stronger condition is satisfied, the proposition <em>A</em> is
said to be <em>open* to the agents of</em> <em>N</em>. With the concept
of open*-ness, Gilbert gives her definition of common knowledge.</p>

<blockquote><strong>Definition 2.22</strong>
<br />
 A proposition <em>E</em> &sube; &Omega; is <em>Gilbert-common
knowledge</em> among the agents of a set <em>N</em> = {1, &hellip;
,<em>n</em>}, if and only if, 

<blockquote>
<table>
<tr>
<td>G<sub>1</sub>*:</td>
<td><em>E</em> is open* to the agents of <em>N</em>.</td>
</tr>

<tr>
<td>G<sub>2</sub>*:</td>
<td>For every <em>i</em> &isin; <em>N</em>,
<strong>K</strong><sub><em>i</em></sub>(G<sub>1</sub>*).</td>
</tr>
</table>
</blockquote>

<strong>G</strong><sub><em>N</em></sub>*(<em>E</em>) denotes the
proposition defined by G<sub>1</sub>* and G<sub>2</sub>* for a set
<em>N</em> of <em>A</em>*-symmetric reasoners, so we can say that
<em>E</em> is Lewis-common knowledge for the agents of <em>N</em> iff
&omega; &isin;
<strong>G</strong><sub><em>N</em></sub>*(<em>E</em>).</blockquote>

<p>

One might think that an immediate corollary to Gilbert's definition
is that Gilbert-common knowledge implies the hierarchical common
knowledge of Proposition 2.5. However, this claim follows only on the
assumption that an agent knows all of the propositions that her
smooth-reasoner counterpart reasons through. Gilbert does not
explicitly endorse this position, although she correctly observes that
Lewis and Aumann are committed to something like
 it.<sup>[<a href="notes.html#17" name="note-17">17</a>]</sup>
 Gilbert
maintains that her account of common knowledge expresses our intuitions
with respect to common knowledge better than Lewis' and Aumann's
accounts, since the notion of open*-ness presumably makes explicit that
when a proposition is common knowledge, it is &ldquo;out in the open&rdquo;, so to
speak.</p>

<h2><a name="3">3. Applications of Mutual and Common Knowledge</a></h2>

<p>

Readers primarily interested in philosophical applications of common
knowledge may want to focus on the No Disagreement Theorem and
Convention subsections. Readers interested in applications of common
knowledge in game theory may continue with the Strategic Form Games,
and Games of Perfect Information subsections. </p>

<ul>
<li><a href="#3.1">3.1 The &ldquo;No Disagreement&rdquo; Theorem</a></li>

<li><a href="#3.2">3.2 Convention</a></li>

<li><a href="#3.3">3.3 Strategic Form Games</a></li>

<li><a href="#3.4">3.4 Games of Perfect Information</a></li>

<li><a href="#3.5">3.5 Communication Networks</a></li>
</ul>

<h3><a name="3.1">3.1 The &ldquo;No Disagreement&rdquo; Theorem</a></h3>

<p>

Aumann (1976) originally used his definition of common knowledge to
prove a celebrated result that says that in a certain sense, agents
cannot &ldquo;agree to disagree&rdquo; about their beliefs, formalized as
probability distributions, if they start with common prior beliefs.
Since agents in a community often hold different opinions and know they
do so, one might attribute such differences to the agents' having
different private information. Aumann's surprising result is that even
if agents condition their beliefs on private information, mere common
knowledge of their conditioned beliefs and a common prior probability
distribution implies that their beliefs cannot be different, after
all!</p>

<blockquote><strong>Proposition 3.1</strong>
<br />
 Let &Omega; be a finite set of states of the world. Suppose that 

<ol type="i">
<li>Agents <em>i</em> and <em>j</em> have a common prior probability
distribution &mu;(<font size="+2">&middot;</font>) over the events of
&Omega; such that &mu;(&omega;)&nbsp;&gt;&nbsp;0, for each &omega;
&isin; &Omega;, and</li>

<li>It is common knowledge at &omega; that <em>i</em>'s posterior
probability of event <em>E</em> is
<em>q</em><sub><em>i</em></sub>(<em>E</em>) and that <em>j</em>'s
posterior probability of <em>E</em> is
<em>q</em><sub><em>j</em></sub>(<em>E</em>).</li>
</ol>

<p>

Then <em>q</em><sub><em>i</em></sub>(<em>E</em>) =
<em>q</em><sub><em>j</em></sub>(<em>E</em>). </p>

<p>

 <a href="proof3.1.html" name="return-3.1">Proof</a>.
<br />
[Note that in the proof of this proposition, and in the sequel,
&mu;(<font size="+2">&middot;</font>|<em>B</em>) denotes conditional
probability; that is, given &mu;(<em>B</em>)&gt;0,
&mu;(<em>A</em>|<em>B</em>) =
&mu;(<em>A</em>&cap;<em>B</em>)/&mu;(<em>B</em>).]</p>
</blockquote>

<p>

In a later article, Aumann (1987) argues that the assumptions that
&Omega; is finite and that &mu;(&omega;)&nbsp;&gt;&nbsp;0 for each
&omega; &isin; &Omega; reflect the idea that agents only regard as
&ldquo;really&rdquo; possible a finite collection of salient worlds to which they
assign positive probability, so that one can drop the states with
probability 0 from the description of the state space. Aumann also
notes that this result implicitly assumes that the agents have common
knowledge of their partitions, since a description of each possible
world includes a description of the agents' possibility sets. And of
course, this result depends crucially upon (i), which is known as the
<em>common prior assumption</em> (CPA).</p>

<p>

Aumann's &ldquo;no disagreement&rdquo; theorem has been generalized in
a number of ways in the literature (McKelvey and Page 1986, Monderer
and Samet 1989, Geanakoplos 1994). However, all of these &ldquo;no
disagreement&rdquo; results raise the same philosophical puzzle raised
by Aumann's original result: How are we to explain differences in
belief? Aumann's result leaves us with two options: (1) admit that at
some level, common knowledge of the agents' beliefs or how they form
their beliefs fails, or (2) deny the CPA. Thus, even if agents do
assign precise posterior probabilities to an event, Aumann shows that
if they have merely first-order mutual knowledge of the posteriors,
they can &ldquo;agree to
 disagree&rdquo;.<sup>[<a href="notes.html#18" name="note-18">18</a>]</sup>
Another way Aumann's result might fail is if agents do not have common
knowledge that they update their beliefs by Bayesian
conditionalization. Then clearly, agents can explain divergent
opinions as the result of others having modified their beliefs in the
&ldquo;wrong&rdquo; way. However, there are cases in which neither
explanation will seem convincing and denying the requisite common
knowledge seems a rather <em>ad hoc</em> move. Why should one think
that such failures of common knowledge provide a general explanation
for divergent beliefs?</p>

<p>

What of the second option, that is, denying the 
CPA?<sup>[<a href="notes.html#19" name="note-19">19</a>]</sup>The main
argument put forward in favor of the CPA is that any differences in
agents' probabilities should be the result of their having different
information only, that is, there is no reason to think that the
different beliefs that agents have regarding the same event are the
result of anything other than their having different information.
However, one can reply that this argument amounts simply to a
restatement of the Harsanyi Doctrine.<sup>[<a href="notes.html#20" name="note-20">20</a>]</sup></p>

<h3><a name="3.2">3.2 Convention</a></h3>

<p>

Schelling's Department Store problem of Example 1.5 is a very simple
example in which the agents &ldquo;solve&rdquo; their coordination problem
appropriately by establishing a <em>convention.</em> (see also the entry on
 <a href="../convention/index.html">convention</a> in this encyclopedia.) Using the
vocabulary of game theory, Lewis (1969) defines a convention as a
<em>strict coordination equilibrium</em> of a game which agents follow
on account of their common knowledge that they all prefer to follow
this coordination equilibrium in a recurrent coordination problem. A
coordination equilibrium of a game is a strategy combination such that
no agent is better off if any agent unilaterally deviates from this
combination. As with equilibria in general, a coordination equilibrium
is <em>strict</em> if any agent who deviates unilaterally from the
equilibrium is strictly worse off. The strategic form game of Figure
1.3 summarizes Liz's and Robert's situation. The Department Store game
has four Nash equilibrium outcomes in pure strategies:
(<em>s</em><sub>1</sub>, <em>s</em><sub>1</sub>),
(<em>s</em><sub>2</sub>, <em>s</em><sub>2</sub>),
(<em>s</em><sub>3</sub>, <em>s</em><sub>3</sub>), and
(<em>s</em><sub>4</sub>,
 <em>s</em><sub>4</sub>).<sup>[<a href="notes.html#21" name="note-21">21</a>]</sup> 
These four equilibria are all strict coordination equilibria. If the
agents follow either of these equilibria, then they coordinate
successfully. For agents to be following a Lewis-convention in this
situation, they must follow one of the game's coordination
equilibria. However, for Lewis to follow a coordination equilibrium is
not a sufficient condition for agents to be following a
convention. For suppose that Liz and Robert fail to analyze their
predicament properly at all, but Liz chooses
<em>s</em><sub>2</sub> and Robert chooses <em>s</em><sub>2</sub>, so
that they coordinate at (<em>s</em><sub>2</sub>,
<em>s</em><sub>2</sub>) by sheer luck. Lewis does not count accidental
coordination of this sort as a convention.</p>

<p>

Suppose next that both agents are Bayesian rational, and that part
of what each agent knows is the payoff structure of the Intersection
game. If the agents expect each other to follow
(<em>s</em><sub>2</sub>, <em>s</em><sub>2</sub>) and they consequently
coordinate successfully, are they then following a convention? Not
necessarily, contends Lewis, in a subtle argument on p. 59 of
<em>Convention.</em> For while each knows the game and that she is
rational, she might not attribute like knowledge to the other
agent. If each agent believes that the other agent will follow her end
of the (<em>s</em><sub>2</sub>, <em>s</em><sub>2</sub>) equilibrium
mindlessly, then her best response is to follow her end of
(<em>s</em><sub>2</sub>, <em>s</em><sub>2</sub>). But in this case the
agents coordinated as the result of their each falsely believing that
the other acts like an automaton, and Lewis thinks that any proper
account of convention must require that agents have <em>correct</em>
beliefs about one another. In particular, Lewis requires that each
agent involved in a convention must have mutual expectations that each
is acting with the aim of coordinating with the other. The argument
can be carried further on. What if both agents believe that they will
follow (<em>s</em><sub>2</sub>, <em>s</em><sub>2</sub>), and believe
that each other will do so thinking that the other will
choose <em>s</em><sub>2</sub> rationally and not midlessly? Then, say,
Liz would coordinate as the result of her false second-order belief
that Robert believes that Liz acts mindlessly. Similarly for
third-order beliefs and so on for any higher order of knowledge.</p>

<p>

Lewis concludes that a necessary condition for agents to be following
a convention is that their preferences to follow the corresponding
coordination equilibrium be common knowledge (the issue whether
conventions need to be common knowledge has been debated recently,
cf. Cubitt and Sugden 2003, Binmore 2008, Sillari 2008, and, for an
experimental approach, see Devetag et al. 2013). So on Lewis' account,
a convention for a set of agents is a coordination equilibrium which
the agents follow on account of their common knowledge of their
rationality, the payoff structure of the relevant game and that each
agent follows her part of the equilibrium.</p>

<blockquote>A regularity <em>R</em> in the behavior of members of a
population <em>P</em> when they are agents in a recurrent situation
<em>S</em> is a <em>convention</em> if and only if it is true that, and
it is common knowledge in <em>P</em> that, in any instance of
<em>S</em> among the members of <em>P</em>, 

<ol>
<li>everyone conforms to <em>R</em>;</li>

<li>everyone expects everyone else to conform to <em>R</em>;</li>

<li>everyone has approximately the same preferences regarding all
possible combinations of actions;</li>

<li>everyone prefers that everyone conform to <em>R</em>, on condition
that at least all but one conform to R;</li>

<li>everyone would prefer that everyone conform to <em>R</em>&prime;,
on condition that at least all but one conform to
<em>R</em>&prime;,</li>
</ol>

<p>

where <em>R</em>&prime; is some possible regularity in the behavior of
members of <em>P</em> in <em>S</em>, such that no one in any instance
of <em>S</em> among members of <em>P</em> could conform both to
<em>R</em>&prime; and to <em>R</em>.
<br />
 (Lewis 1969, p.
 76)<sup>[<a href="notes.html#22" name="note-22">22</a>]</sup></p></blockquote>

<p>

Lewis includes the requirement that there be an alternate
coordination equilibrium <em>R</em>&prime; besides the equilibrium
<em>R</em> that all follow in order to capture the fundamental
intuition that how the agents who follow a convention behave depends
crucially upon how they expect the others to behave.</p>

<p>

Sugden (1986) and Vanderschraaf (1998) argue that it is not crucial
to the notion of convention that the corresponding equilibrium be a
coordination equilibrium. Lewis' key insight is that a convention is a
pattern of mutually beneficial behavior which depends on the agents'
common knowledge that all follow <em>this</em> pattern, and no other.
Vanderschraaf gives a more general definition of convention as a
<em>strict</em> equilibrium together with common knowledge that all
follow this equilibrium and that all would have followed a different
equilibrium had their beliefs about each other been different. An
example of this more general kind of convention is given below in the
discussion of the Figure 3.1 example.</p>

<h3><a name="3.3">3.3 Strategic Form Games</a></h3>

<p>

Lewis formulated the notion of common knowledge as part of his
general account of conventions. In the years following the publication
of <em>Convention</em>, game theorists have recognized that any
explanation of a particular pattern of play in a game depends crucially
on mutual and common knowledge assumptions. More specifically,
<em>solution concepts</em> in game theory are both motivated and
justified in large part by the mutual or common knowledge the agents in
the game have regarding their situation.</p>

<p>

To establish the notation that will be used in the discussion that
follows, the usual definitions of a game in strategic form, expected
utility and agents' distributions over their opponents' strategies, are
given here:</p>

<blockquote><a name="def32"><strong>Definition 3.2</strong></a>
<br />
 A <em>game</em> &Gamma; is an ordered triple (<em>N</em>, <em>S</em>,
<strong><em>u</em></strong>) consisting of the following elements: 

<ol type="a">
<li>A finite set <em>N</em> = {1,2, &hellip; , <em>n</em>}, called the
<em>set of agents</em> or <em>players.</em></li>

<li>For each agent <em>k</em> &isin; <em>N</em>, there is a finite set
<em>S</em><sub><em>k</em></sub> =
 {<em>s</em><sub><em>k</em><font size="-1">1</font></sub>,<em>s</em><sub><em>k</em>2</sub>, &hellip; ,
<em>s</em><sub><em>k</em><font size="-1"><em>n</em><sub><em>k</em></sub></font></sub>}, called
the <em>alternative pure strategies</em> for agent <em>k</em>. The
Cartesian product <em>S</em> = <em>S</em><sub>1</sub> &times; &hellip;
&times; <em>S</em><sub><em>n</em></sub> is called the <em>pure strategy
set</em> for the game &Gamma;.</li>

<li>A map <strong><em>u</em></strong> : <em>S</em> &rarr;
 <img src="Re.jpg" alt="the real numbers" /><sup>n</sup>, called the
<em>utility</em> or <em>payoff function</em> on the pure strategy set.
At each strategy combination <strong><em>s</em></strong> =
(<em>s</em><sub>1<em>j</em><sub><font size="-1">1</font></sub></sub>,
 &hellip; ,
<em>s</em><sub><em>n</em><em>j</em><sub><font size="-1"><em>n</em></font></sub></sub>) &isin; <em>S</em>, agent <em>k</em>'s
particular payoff or utility is given by the <em>k</em><sup>th</sup>
component of the value of <strong><em>u</em></strong>, that is, agent
<em>k</em>'s utility <em>u</em><sub><em>k</em></sub> at
<strong><em>s</em></strong> is determined by 

 <blockquote>
 <em>u</em><sub><em>k</em></sub>
(<strong><em>s</em></strong>) =
<em>I</em><sub><em>k</em></sub>(<strong><em>u</em></strong>(<em>s</em><sub>1<em>j</em><sub><font size="-1">1</font></sub></sub>,
 &hellip; ,
<em>s</em><sub><em>n</em><em>j</em><sub><font size="-1"><em>n</em></font></sub></sub>))</blockquote>

<p>

where <em>I</em><sub><em>k</em></sub>(<strong><em>x</em></strong>)
projects <strong><em>x</em></strong> &isin;
 <img src="Re.jpg" alt="the real numbers" /><sup><em>n</em></sup> onto its
<em>k</em><sup>th</sup> component.</p></li>
</ol>

</blockquote>

<p>

The subscript &lsquo;-<em>k</em>&rsquo; indicates the result of
removing the <em>k</em><sup>th</sup> component of an <em>n</em>-tuple
or an <em>n</em>-fold Cartesian product. For instance,</p>

<blockquote><em>S</em><sub>-<em>k</em></sub> = <em>S</em><sub>1</sub>
&times; &hellip; &times; <em>S</em><sub><em>k</em>&minus;1</sub>
&times; <em>S</em><sub><em>k</em>+1</sub> &times; &hellip; &times;
<em>S</em><sub><em>n</em></sub></blockquote>

<p>

denotes the pure strategy combinations that agent <em>k</em>'s
opponents may play.</p>

<p>

Now let us formally introduce a system of the agents' beliefs into
this framework.
&Delta;<sub><em>k</em></sub>(<em>S</em><sub>-<em>k</em></sub>) denotes
the set of probability distributions over the measurable space
(<em>S</em><sub>-<em>k</em></sub>,
 <img src="frakF.jpg" alt="gothic-F" /><sub><em>k</em></sub>), where
 <img src="frakF.jpg" alt="gothic-F" /><sub><em>k</em></sub> denotes the Boolean algebra
generated by the strategy combinations
<em>S</em><sub>-<em>k</em></sub>. Each agent <em>k</em> has a
probability distribution &mu;<sub><em>k</em></sub> &isin;
&Delta;<sub><em>k</em></sub>(<em>S</em><sub>-<em>k</em></sub>), and
this distribution determines the <em>(Savage) expected utilities</em>
for each of <em>k</em>'s possible acts:</p>

<blockquote>
<em>E</em>(<em>u</em><sub><em>k</em></sub>(<em>s</em><sub><em>k</em>
<em>j</em></sub>)) =
<font size="+2">&sum;</font><sub><em>A</em><sub>&minus;<em>k</em></sub> &isin;
<em>S</em><sub>&minus;<em>k</em></sub></sub> 
<em>u</em><sub><em>k</em></sub>(<em>s</em><sub><em>kj</em></sub>,
 <strong><em>s</em></strong><sub>&minus;<em>k</em></sub>)
&mu;<sub><em>k</em></sub>(<strong><em>s</em></strong><sub>&minus;<em>k</em></sub>),
&nbsp; <em>j</em> = 1, 2, &hellip; , <em>n</em><sub><em>k</em></sub>
</blockquote>

<p>

If <em>i</em> is an opponent of <em>k</em>, then <em>i</em>'s
individual strategy <em>s</em><sub><em>i</em>&nbsp;<em>j</em></sub> may
be characterized as a union of strategy combinations
 <font size="+2">&cup;</font>{<strong><em>s</em></strong><sub>&minus;<em>k</em></sub>&thinsp;|&thinsp;<em>s</em><sub><em>ij</em></sub>&thinsp;&isin;&thinsp;<strong><em>s</em></strong><sub>&minus;<em>k</em></sub>}
 &isin;
 <img src="frakF.jpg" alt="gothic-F" /><sub><em>k</em></sub>, and so
<em>k</em>'s marginal probability for <em>i</em>'s strategy
<em>s</em><sub><em>i</em>&nbsp;<em>j</em></sub> may be calculated as
follows:</p>

<blockquote>
&mu;<sub><em>k</em></sub>(<em>s</em><sub><em>ij</em></sub>)
=
<font size="+2">&sum;</font><sub>{<em>s</em><sub>&minus;<em>k</em></sub>|&thinsp;<em>s</em><sub><em>ij</em></sub>&isin;<em>s</em><sub>&minus;<em>k</em></sub>}</sub> &mu;<sub><em>k</em></sub>(<em>s</em><sub>&minus;<em>k</em></sub>)
</blockquote>

<p>

&mu;<sub><em>k</em></sub>(<font size="+2">&middot;</font>&nbsp;|&nbsp;<em>A</em>) denotes <em>k</em>'s
conditional probability distribution given a set <em>A</em>, and
<em>E</em>(<font size="+2">&middot;</font>&nbsp;|&nbsp;<em>A</em>)
denotes <em>k</em>'s conditional expectation given
&mu;<sub><em>k</em></sub>(<font size="+2">&middot;</font>&nbsp;|&nbsp;<em>A</em>). </p>

<p>

Suppose first that the agents have common knowledge of the full
payoff structure of the game they are engaged in and that they are all
rational, and that no other information is common knowledge. In other
words, each agent knows that her opponents are expected utility
maximizers, but does not in general know exactly which strategies they
will choose or what their probabilities for her acts are. These common
knowledge assumptions are the motivational basis for the solution
concept for noncooperative games known as <em>rationalizability</em>,
introduced independently by Bernheim (1984) and Pearce (1984). Roughly
speaking, a <em>rationalizable strategy</em> is any strategy an agent
may choose without violating common knowledge of Bayesian rationality.
Bernheim and Pearce argue that when only the structure of the game and
the agents' Bayesian rationality are common knowledge, the game should
be considered &ldquo;solved&rdquo; if every agent plays a rationalizable strategy.
For instance, in the &ldquo;Chicken&rdquo; game with payoff structure defined by
Figure 3.1,</p>

<blockquote><img alt="missing text, please inform" src="figure3.1.gif" />
<br />
 <strong>Figure 3.1</strong></blockquote>

<p>

if Joanna and Lizzi have common knowledge of all of the payoffs at
every strategy combination, and they have common knowledge that both
are Bayesian rational, then any of the four pure strategy profiles is
rationalizable. For if their beliefs about each other are defined by
the probabilities </p>

<blockquote>&alpha;<sub>1</sub> = &mu;<sub>1</sub> (Joanna plays
<em>s</em><sub>1</sub>), and
<br />
 &alpha;<sub>2</sub> = &mu;<sub>2</sub> (Lizzi plays
<em>s</em><sub>1</sub>)</blockquote>

<p>

then </p>

<blockquote>
<em>E</em>(<em>u</em><sub><em>i</em></sub>(<em>s</em><sub>1</sub>)) =
3&alpha;<sub><em>i</em></sub> + 2(1 &minus;
&alpha;<sub><em>i</em></sub>) = &alpha;<sub><em>i</em></sub> +
2</blockquote>

<p>

and </p>

<blockquote>
<em>E</em>(<em>u</em><sub><em>i</em></sub>(<em>s</em><sub>2</sub>)) =
4&alpha;<sub><em>i</em></sub> + 0(1 &minus;
&alpha;<sub><em>i</em></sub>) = 4&alpha;<sub><em>i</em></sub>, &nbsp;
<em>i</em> = 1, 2</blockquote>

<p>

so each agent maximizes her expected utility by playing
<em>s</em><sub>1</sub> if &alpha;<sub><em>i</em></sub>&nbsp;+ 2
&ge;&nbsp;4&alpha;<sub><em>i</em></sub> or
&alpha;<sub><em>i</em></sub>&nbsp;&le;&nbsp;2/3 and maximizes her
expected utility by playing <em>s</em><sub>2</sub> if
&alpha;<sub><em>i</em></sub>&nbsp;&ge;&nbsp;2/3. If it so happens that
&alpha;<sub><em>i</em></sub>&nbsp;&gt;&nbsp;2/3 for both agents, then
both conform with Bayesian rationality by playing their respective ends
of the strategy combination
(<em>s</em><sub>2</sub>,<em>s</em><sub>2</sub>) <em>given their
beliefs</em>, even though each would want to defect from this strategy
combination were she to discover that the other is in fact going to
play <em>s</em><sub>2</sub>. Note that the game's pure strategy Nash
equilibria, (<em>s</em><sub>1</sub>, <em>s</em><sub>2</sub>) and
(<em>s</em><sub>2</sub>, <em>s</em><sub>1</sub>), are rationalizable,
since it is rational for Lizzi and Joanna to conform with either
equilibrium given appropriate distributions. In general, the set of a
game's rationalizable strategy combinations contains the set of the
game's pure strategy Nash 
equilibria.<sup>[<a href="notes.html#23" name="note-23">23</a>]</sup></p>

<p>

Rationalizability can be defined formally in several ways. A
variation of Bernheim's original (1984) definition is given here.</p>

<blockquote><strong>Definition 3.3</strong>
<br />
 Given that each agent <em>k</em> &isin; <em>N</em> has a probability
distribution &mu;<sub><em>k</em></sub> &isin;
&Delta;<sub><em>k</em></sub>(<em>s</em><sub>-<em>k</em></sub>), the
system of beliefs 

<blockquote><strong>&mu;</strong> &nbsp; = &nbsp; (&mu;<sub>1</sub>,
&hellip; , &mu;<sub>n</sub>) &isin;
&Delta;<sub>1</sub>(<em>S</em><sub>-1</sub>) &times; &hellip; &times;
&Delta;<sub><em>n</em></sub>(<em>S</em><sub>-<em>n</em></sub>)</blockquote>

<p>

is <em>Bayes concordant</em> if and only if, </p>

<blockquote>
<table>
<tr>
<td valign="top">(3.i)</td>
<td>For <em>i</em> &ne; <em>k</em>,
&mu;<sub><em>i</em></sub>(<em>s</em><sub><em>kj</em></sub>)
&gt; 0 &rArr; <em>s</em><sub><em>kj</em></sub> maximizes
<em>k</em>'s expected utility for some &sigma;<sub><em>k</em></sub>
&isin;
&Delta;<sub><em>k</em></sub>(<em>s</em><sub>-<em>k</em></sub>),</td>
</tr>
</table>
</blockquote>

<p>

and (3.i) is common knowledge. A pure strategy combination
<strong><em>s</em></strong> =
 (<em>s</em><sub>1<em>j</em><sub>1</sub></sub>,
&hellip; ,
 <em>s</em><sub><em>nj</em><sub><em>n</em></sub></sub>)
&isin; <em>S</em> is <em>rationalizable</em> if and only if the agents
have a Bayes concordant system <strong>&mu;</strong> of beliefs and,
for each agent <em>k</em> &isin; <em>N</em>, </p>

<blockquote>
<table>
<tr>
<td>(3.ii)</td>
<td>
<em>E</em>(<em>u</em><sub><em>k</em></sub>(<em>s</em><sub><em>kj</em><sub><em>k</em></sub></sub>)) &ge;
<em>E</em>(<em>u</em><sub><em>k</em></sub>(<em>s</em><sub><em>ki</em><sub><em>k</em></sub></sub>)), for <em>i</em><sub><em>k</em></sub> &ne;
 <em>j</em><sub><em>k</em></sub>.<sup>[<a href="notes.html#24" name="note-24">24</a>]</sup></td>
</tr>
</table>
</blockquote>
</blockquote>

<p>

The following result shows that the common knowledge restriction on
the distributions in Definition 3.1 formalizes the assumption that the
agents have common knowledge of Bayesian rationality.</p>

<blockquote><strong>Proposition 3.4</strong>
<br />
 In a game &Gamma;, common knowledge of Bayesian rationality is
satisfied if, and only if, (3.i) is common knowledge. 

<p>

 <a href="proof3.4.html" name="return-3.4">Proof</a>.</p>
</blockquote>

<p>

When agents have common knowledge of the game and their Bayesian
rationality only, one can predict that they will follow a
rationalizable strategy profile. However, rationalizability becomes an
unstable solution concept if the agents come to know more about one
another. For instance, in the Chicken example above with
&alpha;<sub><em>i</em></sub> &gt; 2/3, <em>i</em> = 1, 2, if either
agent were to discover the other agent's beliefs about her, she would
have good reason not to follow the
(<em>s</em><sub>2</sub>,<em>s</em><sub>2</sub>) profile and to revise
her own beliefs regarding the other agent. If, in the other hand, it so
happens that &alpha;<sub>1</sub> = 1 and &alpha;<sub>2</sub> = 0, so
that the agents maximize expected payoff by following the
(<em>s</em><sub>2</sub>, <em>s</em><sub>1</sub>) profile, then should
the agents discover their beliefs about each other, they will still
follow (<em>s</em><sub>2</sub>, <em>s</em><sub>1</sub>). Indeed, if
their beliefs are common knowledge, then one can predict with certainty
that they will follow (<em>s</em><sub>2</sub>,<em>s</em><sub>1</sub>).
The Nash equilibrium (<em>s</em><sub>2</sub>,<em>s</em><sub>1</sub>) is
characterized by the belief distributions defined by
&alpha;<sub>1</sub> = 1 and &alpha;<sub>2</sub> = 0.</p>

<p>

The Nash equilibrium is a special case of <em>correlated equilibrium
concepts</em>, which are defined in terms of the belief distributions
of the agents in a game. In general, a correlated
equilibrium-in-beliefs is a system of agents' probability distributions
which remains stable given common knowledge of the game, rationality
and the <em>beliefs themselves</em>. We will review two alternative
correlated equilibrium concepts (Aumann 1974, 1987; Vanderschraaf
1995, 2001), and show how each generalizes the Nash equilibrium concept.</p>

<blockquote><strong>Definition 3.5</strong>
<br />
 Given that each agent <em>k</em> &isin; <em>N</em> has a probability
distribution &mu;<sub><em>k</em></sub> &isin;
&Delta;<sub><em>k</em></sub> (<em>s</em><sub>-<em>k</em></sub>), the
system of beliefs 

<blockquote>
<table>
<tr>
<td><strong>&mu;</strong>* = (&mu;<sub>1</sub>*,
&hellip; , &mu;<sub><em>n</em></sub>* ) &isin;
&Delta;<sub>1</sub>(<em>s</em><sub>-1</sub>) &times; &hellip; &times;
&Delta;<sub><em>n</em></sub>(<em>s</em><sub>-<em>n</em></sub>)</td>
</tr>
</table>
</blockquote>

<p>

is an <em>endogenous correlated equilibrium</em> if, and only
if,</p>

<blockquote>(3.iii) For <em>i</em> &ne; <em>k</em>,
&mu;<sub><em>i</em></sub>*(<em>s</em><sub><em>kj</em></sub>)
&gt; 0 &rArr; <em>s</em><sub><em>kj</em></sub> maximizes
<em>k</em>'s expected utility given
&mu;<sub><em>k</em></sub>*.</blockquote>

<p>

If <strong>&mu;</strong>* is an endogenous correlated equilibrium a
pure strategy combination <strong><em>s</em></strong>* =
(<em>s</em><sub>1</sub>*, &hellip; ,<em>s</em><sub><em>n</em></sub>* )
&isin; S is an <em>endogenous correlated equilibrium strategy
combination given</em> <strong>&mu;</strong>* if, and only if, for each
agent <em>k</em> &isin; <em>N</em>,</p>

<blockquote>(3.iv)
<em>E</em>(<em>u</em><sub><em>k</em></sub>(<em>s</em><sub><em>k</em></sub>*))
&ge;
<em>E</em>(<em>u</em><sub><em>k</em></sub>(<em>s</em><sub><em>ki</em></sub>))
 for <em>s</em><sub><em>ki</em></sub> &ne;
<em>s</em><sub><em>k</em></sub>*.</blockquote>
</blockquote>

<p>

Hence, the endogenous correlated equilibrium <strong>&mu;</strong>*
restricts the set of strategies that the agents might follow, as do the
Bayes concordant beliefs of rationalizability. However, the endogenous
correlated equilibrium concept is a proper refinement of
rationalizability, because the latter does not presuppose that
condition (3.iii) holds with respect to the beliefs one's opponents
actually have. If exactly one pure strategy combination
<strong><em>s</em></strong>* satisfies (3.iv) given
<strong>&mu;</strong>*, then <strong>&mu;</strong>* is a <em>strict
equilibrium</em>, and in this case one can predict with certainty what
the agents will do given common knowledge of the game, rationality and
their beliefs. Note that Definition 3.5 says nothing about whether or
not the agents regard their opponents' strategy combinations as
probabilistically independent. Also, this definition does not require
that the agents' probabilities are <em>consistent</em>, in the sense
that agents' probabilities for a mutual opponent's acts agree. A simple
refinement of the endogenous correlated equilibrium concept
characterizes the Nash equilibrium concept.</p>

<blockquote><strong>Definition 3.6</strong>
<br />
 A system of agents' beliefs <strong>&mu;</strong>* is a <em>Nash
equilibrium</em> if, and only if, 

<ol type="a">
<li>condition (3.iii) is satisfied,</li>

<li>For each <em>k</em> &isin; <em>N</em>, &mu;<sub><em>k</em></sub>*
satisfies probabilistic independence, and</li>

<li>For each <em>s</em><sub><em>kj</em></sub> &isin;
<em>s</em><sub><em>k</em></sub>, if <em>i</em>, <em>l</em> &ne;
<em>k</em> then &mu;<sub><em>i</em></sub>*(<em>s</em><sub><em>kj</em></sub>) =
&mu;<sub><em>l</em></sub>*(<em>s</em><sub><em>kj</em></sub>).</li>
</ol>
</blockquote>

<p>

In other words, an endogenous correlated equilibrium is a Nash
equilibrium-in-beliefs when each agent regards the moves of his
opponents as probabilistically independent and the agents'
probabilities are consistent. Note that in the 2-agent case, conditions
(b) and (c) of the Definition 3.6 are always satisfied, so for 2-agent
games the endogenous correlated equilibrium concept reduces to the Nash
equilibrium concept. Conditions (b) and (c) are traditionally assumed
in game theory, but Skyrms (1991) and Vanderschraaf (1995, 2001) argue that
there may be good reasons to relax these assumptions in games with 3 or
more agents.</p>

<p>

Brandenburger and Dekel (1988) show that in 2-agent games, if the
beliefs of the agents are common knowledge, condition (3.iii)
characterizes a Nash equilibrium-in-beliefs. As they note, condition
(3.iii) characterizes a Nash equilibrium in beliefs for the
<em>n</em>-agent case if the probability distributions are consistent
and satisfy probabilistic independence. Proposition 3.7 extends
Brandenburger and Dekel's result to the endogenous correlated
equilibrium concept by relaxing the consistency and probabilistic
independence assumptions.</p>

<blockquote><strong>Proposition 3.7</strong>
<br />
Assume that the probabilities 

<blockquote><strong>&mu;</strong> =
(&mu;<sub>1</sub>,&hellip;,&mu;<sub><em>n</em></sub>) &isin;
&Delta;<sub>1</sub>(<em>s</em><sub>-1</sub>) &times; &hellip; &times;
&Delta;<sub><em>n</em></sub>(<em>s</em><sub>-<em>n</em></sub>)</blockquote>

<p>

are common knowledge. Then common knowledge of Bayesian rationality is
satisfied if, and only if, <strong>&mu;</strong> is an endogenous
correlated equilibrium. </p>

<p>

 <a href="proof3.7.html" name="return-3.7">Proof</a>.</p>
</blockquote>

<p>

In addition, we have: </p>

<!--pdf include
<br/>
pdf include-->

<blockquote><strong>Corollary 3.8</strong> (Brandenburger and Dekel,
1988)
<br />
 Assume in a 2-agent game that the probabilities 

<blockquote><strong>&mu;</strong> = (&mu;<sub>1</sub>,&mu;<sub>2</sub>)
&isin; &Delta;<sub>1</sub>(<em>s</em><sub>-1</sub>) &times;
&Delta;<sub>2</sub>(<em>s</em><sub>-2</sub>)</blockquote>

<p>

are common knowledge. Then common knowledge of Bayesian rationality
is satisfied if, and only if, <strong>&mu;</strong> is a Nash
equilibrium.</p>

<p>

<strong>Proof.</strong>
<br />
The endogenous correlated equilibrium concept reduces to the Nash
equilibrium concept in the 2-agent case, so the corollary follows by
Proposition 3.7.</p>
</blockquote>

<p>

If <strong>&mu;</strong>* is a strict equilibrium, then one can
predict which pure strategy profile the agents in a game will follow
given common knowledge of the game, rationality and
<strong>&mu;</strong>*. But if <strong>&mu;</strong>* is such that
several distinct pure strategy profiles satisfy (3.iv) with respect to
<strong>&mu;</strong>*, then one can no longer predict with certainty
what the agents will do. For instance, in the Chicken game of Figure
3.1, the belief distributions defined by &alpha;<sub>1</sub> =
&alpha;<sub>2</sub> = 2/3 together are a Nash equilibrium-in-beliefs.
Given common knowledge of this equilibrium, either pure strategy is a
best reply for each agent, in the sense that either pure strategy
maximizes expected utility. Indeed, if agents can also adopt randomized
or <em>mixed</em> strategies at which they follow one of several pure
strategies according to the outcome of a chance experiment, then any of
the infinitely mixed strategies an agent might adopt in Chicken is a
best reply given
 <strong>&mu;</strong>*.<sup>[<a href="notes.html#25" name="note-25">25</a>]</sup>
 So the endogenous
correlated equilibrium concept does not determine the exact outcome of
a game in all cases, even if one assumes probabilistic consistency and
independence so that the equilibrium is a Nash equilibrium.</p>

<p>

Another correlated equilibrium concept formalized by Aumann (1974,
1987) does give a determinate prediction of what agents will do in a
game given appropriate common knowledge. To illustrate Aumann's
correlated equilibrium concept, let us consider the Figure 3.1 game
once more. If Joanna and Lizzi can tie their strategies to their
knowledge of the possible worlds in a certain way, they can follow a
system of correlated strategies which will yield a payoff vector they
both prefer to that of the mixed Nash equilibrium and which is itself
an equilibrium. One way they can achieve this is to have their friend
Ron play a variation of the familiar shell game by hiding a pea under
one of three walnut shells, numbered 1, 2 and 3. Joanna and Lizzi both
think that each of the three relevant possible worlds corresponding to
&omega;<sub><em>k</em></sub> = {the pea lies under shell <em>k</em>} is
equally likely. Ron then gives Lizzi and Joanna each a private
recommendation, based upon the outcome of the game, which defines a
system of strategy combinations f as follows</p>

<blockquote>

<table>
<tr>
<td>(<img src="star.gif" alt="star" />)</td>
<td><em>f</em>(&omega;) =</td>
<td><sup><img src="cases.gif" alt="cases bracket" /></sup></td>
<td>
<table>
<tr>
<td>(<em>s</em><sub>1</sub>,<em>s</em><sub>1</sub>) if
&omega;<sub><em>k</em></sub> = &omega;<sub>1</sub></td>
</tr>

<tr>
<td>(<em>s</em><sub>1</sub>,<em>s</em><sub>2</sub>) if
&omega;<sub><em>k</em></sub> = &omega;<sub>2</sub></td>
</tr>

<tr>
<td>(<em>s</em><sub>2</sub>,<em>s</em><sub>1</sub>) if
&omega;<sub><em>k</em></sub> = &omega;<sub>3</sub></td>
</tr>
</table>
</td>
</tr>
</table>
</blockquote>

<p>

<em>f</em> is a <em>correlated</em> strategy system because the
agents tie their strategies, by following their recommendations, to the
same set of states of the world &Omega;. <em>f</em> is also a strict
<em>Aumann correlated equilibrium</em>, for if each agent knows how Ron
makes his recommendations, but knows only the recommendation he gives
her, either would do strictly worse were she to deviate from her
 recommendation.<sup>[<a href="notes.html#26" name="note-26">26</a>]</sup>
 Since there are several strict equilibria of
Chicken, <em>f</em> corresponds to a convention as defined in
Vanderschraaf (1998). The overall expected payoff vector of <em>f</em>
is (3,3), which lies outside the convex hull of the payoffs for the
game's Nash equilibria and which Pareto-dominates the expected payoff
vector (4/3, 4/3), of the mixed Nash equilibrium defined by
&alpha;<sub>1</sub> = 2/3, <em>i</em> = 1,
 2.<sup>[<a href="notes.html#27" name="note-27">27</a>]</sup>
 The correlated equilibrium
f is characterized by the probability distribution of the agents' play
over the strategy profiles, given in Figure 3.3:</p>

<blockquote><img alt="missing text, please inform" src="figure3.3.gif" />
<br />
 <strong>Figure 3.3</strong></blockquote>

<p>

Aumann (1987) proves a result relating his correlated equilibrium
concept to common knowledge. To review this result, we must give the
formal definition of Aumann correlated equilibrium.</p>

<blockquote><strong>Definition 3.9</strong>
<br />
 Given a game &Gamma; = (<em>N</em>, <em>S</em>,
<strong><em>u</em></strong>) together with a finite set of possible
worlds &Omega;, the vector valued function <em>f</em>: &Omega; &rarr;
<em>S</em> is a <em>correlated n-tuple.</em> If <em>f</em>(&omega;) =
(<em>f</em><sub>1</sub>(&omega;), &hellip; ,
<em>f</em><sub><em>n</em></sub>(&omega;)) denotes the components of
<em>f</em> for the agents of <em>N</em>, then agent <em>k</em>'s
<em>recommended strategy</em> at &omega; is
<em>f</em><sub><em>k</em></sub>(&omega;). <em>f</em> is an <em>Aumann
correlated equilibrium</em> iff 

<blockquote><em>E</em>(<em>u</em><sub><em>k</em></sub>
 <img src="circ.jpg" alt="circle" />
 <em>f</em>) &ge;
<em>E</em>(<em>u</em><sub><em>k</em></sub>(<em>f</em><sub>-<em>k</em></sub>,
<em>g</em><sub><em>k</em></sub>)),</blockquote>

<p>

for each <em>k</em> &isin; <em>N</em> and for any function
<em>g</em><sub><em>k</em></sub> that is a function of
<em>f</em><sub><em>i</em></sub>.</p></blockquote>

<p>

The agents are at Aumann correlated equilibrium if at each possible
world &omega; &isin; &Omega;, no agent will want to deviate from his
recommended strategy, given that the others follow their recommended
strategies. Hence, Aumann correlated equilibrium uniquely specifies the
strategy of each agent, by explicitly introducing a space of possible
worlds to which agents can correlate their acts. The deviations
<em>g</em><sub><em>i</em></sub> are required to be functions of
<em>f</em><sub><em>i</em></sub>, that is, compositions of some other
function with <em>f</em><sub><em>i</em></sub>, because <em>i</em> is
informed of <em>f</em><sub><em>i</em></sub>(&omega;) only, and so can
only distinguish between the possible worlds of &Omega; that are
distinguished by <em>f</em><sub><em>i</em></sub>. As noted already, the
primary difference between Aumann's notion of correlated equilibrium
and the endogenous correlated equilibrium is that in Aumann's
correlated equilibrium, the agents correlate their strategies to some
event &omega; &isin; &Omega; that is external to the game. One way to
view this difference is that agents who correlate their strategies
exogenously can calculate their expected utilities conditional on their
own strategies.</p>

<p>

In Aumann's model, a description of each possible world &omega;
includes descriptions of the following: the game &Gamma;, the agent's
private information partitions, and the actions chosen by each agent at
&omega;, and each agent's prior probability distribution
&mu;<sub><em>k</em></sub>(<font size="+2">&middot;</font>) over
&Omega;. The basic idea is that conditional on &omega;, everyone knows
everything that can be the object of uncertainty on the part of any
agent, but in general, no agent necessarily knows which world &omega;
is the actual world. The agents can use their priors to calculate the
probabilities that the various act combinations
<strong><em>s</em></strong> &isin; <em>S</em> are played. If the
agents' priors are such that for all <em>i</em>, <em>j</em> &isin;
<em>N</em>, &mu;<sub><em>i</em></sub>(&omega;) = 0 iff
&mu;<sub><em>j</em></sub>(&omega;) = 0, then the agents' priors are
<em>mutually absolutely continuous</em>. If the agents' priors all
agree, that is, &mu;<sub>1</sub>(&omega;) = &hellip; =
&mu;<sub>n</sub>(&omega;) = &mu;(&omega;) for each &omega; &isin;
&Omega;, then it is said that the <em>common prior assumption</em>, or
CPA, is satisfied. If agents are following an Aumann correlated
equilibrium <em>f</em> and the CPA is satisfied, then <em>f</em> is an
<em>objective</em> Aumann correlated equilibrium. An Aumann correlated
equilibrium is a Nash equilibrium if the CPA is satisfied and the
agents' distributions satisfy probabilistic
 independence.<sup>[<a href="notes.html#28" name="note-28">28</a>]</sup></p>

<p>

Let <em>s</em><sub><em>i</em></sub>(&omega;) denote the strategy
chosen by agent <em>i</em> at possible world &omega;. Then
<em>s</em>:&nbsp;&Omega;&nbsp;&rarr; <em>S</em> defined by
<em>s</em>(&omega;) =
(<em>s</em><sub>1</sub>(&omega;),&hellip;,<em>s</em><sub><em>n</em></sub>(&omega;))
is a correlated <em>n</em>-tuple. Given that
 <span class="scriptuc">H</span><sub><em>i</em></sub> is a partition of
 &Omega;,<sup>[<a href="notes.html#29" name="note-29">29</a>]</sup>
the function <em>s</em><sub><em>i</em></sub>:&nbsp;&Omega;&nbsp;&rarr;
<em>s</em><sub><em>i</em></sub> defined by <em>s</em> is
 <span class="scriptuc">H</span><sub><em>i</em></sub>-<em>measurable</em> if for
each
 <span class="scriptuc">H</span><sub><em>ij</em></sub> &isin;
 <span class="scriptuc">H</span><sub><em>i</em></sub>,
<em>s</em><sub><em>i</em></sub>(&omega;&prime;) is constant for each
&omega;&prime; &isin;
 <span class="scriptuc">H</span><sub><em>ij</em></sub>.
 <span class="scriptuc">H</span><sub><em>i</em></sub>-measurability is a formal
way of saying that <em>i</em> knows what she will do at each possible
world, given her information.</p>

<!--pdf include
<br/>
pdf include-->

<blockquote><strong>Definition 3.10</strong>
<br />
 Agent <em>i</em> is <em>Bayes rational</em> with respect to &omega;
&isin; &Omega; (alternatively, &omega;<em>-Bayes rational</em>) iff
<em>s</em><sub><em>i</em></sub> is
 <span class="scriptuc">H</span><sub><em>i</em></sub>-measurable and 

<blockquote><em>E</em>(<em>u</em><sub><em>i</em></sub><img src="circ.jpg" alt="circle" /><em>s</em> |
 <span class="scriptuc">H</span><sub><em>i</em></sub>)(&omega;) &ge;
<em>E</em>(<em>u</em><sub><em>i</em></sub>(<em>v</em><sub><em>i</em></sub>,<em>
s</em><sub>-i</sub>) |
 <span class="scriptuc">H</span><sub><em>i</em></sub>)(&omega;)</blockquote>

<p>

for any
 <span class="scriptuc">H</span><sub><em>i</em></sub>-measurable function
<em>v</em><sub><em>i</em></sub> : &Omega; &rarr;
<em>s</em><sub><em>i</em></sub>.</p></blockquote>

<p>

Note that Aumann's definition of &omega;-Bayesian rationality implies
that
&mu;<sub><em>i</em></sub>(<span class="scriptuc">H</span><sub><em>i</em></sub>(&omega;))
&gt; 0, so that the conditional expectations are defined. Aumann's
main result, given next, implicitly assumes that
&mu;<sub><em>i</em></sub>(<span class="scriptuc">H</span><sub><em>i</em></sub>(&omega;))
&gt; 0 for every agent <em>i</em> &isin; <em>N</em> and every possible
world &omega; &isin; &Omega;. This poses no technical difficulties if
the CPA is satisfied, or even if the priors are only mutually
absolutely continuous, since if this is the case then one can simply
drop any &omega; with zero prior from consideration.</p>

<blockquote><strong>Proposition 3.11</strong> (Aumann 1987)
<br />
 If each agent <em>i</em> &isin; <em>N</em> is &omega;-Bayes rational
at each possible world &omega; &isin; &Omega;, then the agents are
following an Aumann correlated equilibrium. If the CPA is satisfied,
then the correlated equilibrium is objective. 

<p>

 <a href="proof3.11.html" name="return-3.11">Proof</a>.</p>
</blockquote>

<p>

Part of the uncertainty the agents might have about their situation
is whether or not all agents are rational. But if it is assumed that
all agents are &omega;-Bayesian rational at each &omega; &isin;
&Omega;, then a description of this fact forms part of the description
of each possible &omega; and thus lies in the meet of the agents'
partitions. As noted already, descriptions of the agents' priors, their
partitions and the game also form part of the description of each
possible world, so propositions corresponding to these facts also lie
in the meet of the agents' partitions. So another way of stating
Aumann's main result is as follows: <em>Common knowledge of</em>
&omega;-<em>Bayesian rationality at each possible world implies that
the agents follow an Aumann correlated equilibrium</em>.</p>

<p>

Propositions 3.7 and 3.11 are powerful results. They say that common
knowledge of rationality and of agents beliefs about each other,
quantified as their probability distributions over the strategy
profiles they might follow, implies that the agents' beliefs
characterize an equilibrium of the game. Then if the agents' beliefs
are unconditional, Proposition 3.7 says that the agents are rational to
follow a strategy profile consistent with the corresponding endogenous
correlated equilibrium. If their beliefs are conditional on their
private information partitions, then Proposition 3.11 says they are
rational to follow the strategies the corresponding Aumann correlated
equilibrium recommends. However, we must not overestimate the
importance of these results, for they say nothing about the
<em>origins</em> of the common knowledge of rationality and beliefs.
For instance, in the Chicken game of Figure 3.1, we considered an
example of a correlated equilibrium in which it was <em>assumed</em>
that Lizzi and Joanna had common knowledge of the system of recommended
strategies defined by
 (<img src="star.gif" alt="star" />). Given this
common knowledge, Joanna and Lizzi indeed have decisive reason to
follow the Aumann correlated equilibrium f. But where did this common
knowledge come from? How, in general, do agents come to have the common
knowledge which justifies their conforming to an equilibrium?
Philosophers and social scientists have made only limited progress in
addressing this question.</p>

<h3><a name="3.4">3.4 Games of Perfect Information</a></h3>

<p>

In extensive form games, the agents move in sequence. At each stage,
the agent who is to move must base her decisions upon what she knows
about the preceding moves. This part of the agent's knowledge is
characterized by an <em>information set</em>, which is the set of
alternative moves that an agent knows her predecessor might have
chosen. For instance, consider the extensive form game of Figure
3.4:</p>

<blockquote><img alt="missing text, please inform" src="figure3.4.gif" />
<br />
 <strong>Figure 3.4</strong></blockquote>

<p>

When Joanna moves she is at her information set <em>I</em><sup>22</sup>
= {<em>C</em><sup>1</sup>, <em>D</em><sup>1</sup>}, that is, she moves
knowing that Lizzi might have chosen either <em>C</em><sup>1</sup> or
<em>D</em><sup>1</sup>, so this game is an extensive form
representation of the Chicken game of Figure 3.1. </p>

<p>

In a game of perfect information, each information set consists of a
single node in the game tree, since by definition at each state the
agent who is to move knows exactly how her predecessors have moved. In
Example 1.4 it was noted that the method of backwards induction can be
applied to any game of perfect
 information.<sup>[<a href="notes.html#30" name="note-30">30</a>]</sup>
 The backwards induction
solution is the unique Nash equilibrium of a game of perfect
information. The following result gives sufficient conditions to
justify backwards induction play in a game of perfect information:</p>

<blockquote><strong>Proposition 3.12</strong> (Bicchieri 1993)
<br />
 In an extensive form game of perfect information, the agents follow
the backwards induction solution if the following conditions are
satisfied for each agent <em>i</em> at each information set
<em>I</em><sup><em>ik</em></sup>: 

<ol type="a">
<li><em>i</em> is rational, <em>i</em> knows this and <em>i</em> knows
the game, and</li>

<li>At any information set <em>I</em><sup><em>jk</em> + 1</sup> that
immediately follows <em>I</em><sup><em>ik</em></sup>, <em>i</em> knows
at <em>I</em><sup><em>ik</em></sup> what <em>j</em> knows at
<em>I</em><sup><em>jk</em> + 1</sup>.</li>
</ol>

 <a href="proof3.12.html" name="return-3.12">Proof</a>.</blockquote>

<p>

Proposition 3.12 says that far less than common knowledge of the
game and of rationality suffices for the backwards induction solution
to obtain in a game of perfect information. All that is needed is for
each agent at each of her information sets to be rational, to know the
game and to know what the next agent to move knows! For instance, in
the Figure 1.2 game, if <em>R</em><sub>1</sub> (<em>R</em><sub>2</sub>)
stands for &ldquo;Alan (Fiona) is rational&rdquo; and
<strong>K</strong><sub><em>i</em></sub>(&Gamma; ) stands for
&ldquo;<em>i</em> knows the game &Gamma;&rdquo;, then the backwards induction
solution is implied by the following:</p>

<ol type="i">
<li>At <em>I</em><sup>24</sup>, <em>R</em><sub>2</sub> and
<strong>K</strong><sub>2</sub>(&Gamma;).</li>

<li>At <em>I</em><sup>13</sup>, <em>R</em><sub>1</sub>,
<strong>K</strong><sub>1</sub>(&Gamma;),
<strong>K</strong><sub>1</sub>(<em>R</em><sub>2</sub>), and
<strong>K</strong><sub>1</sub><strong>K</strong><sub>2</sub>(&Gamma;).</li>

<li>At <em>I</em><sup>22</sup>,
<strong>K</strong><sub>2</sub>(<em>R</em><sub>1</sub>),
<strong>K</strong><sub>2</sub><strong>K</strong><sub>1</sub>(<em>R</em><sub>
2</sub>), and
<strong>K</strong><sub>2</sub><strong>K</strong><sub>1</sub><strong>K</strong><sub>
2</sub>(&Gamma;).</li>

<li>At <em>I</em> <sup>11</sup>,
<strong>K</strong><sub>1</sub><strong>K</strong><sub>2</sub>(<em>R</em><sub>
1</sub>),
<strong>K</strong><sub>1</sub><strong>K</strong><sub>2</sub><strong>K</strong><sub>
1</sub>(<em>R</em><sub>2</sub>), and
<strong>K</strong><sub>1</sub><strong>K</strong><sub>2</sub><strong>K</strong><sub>
 1</sub><strong>K</strong><sub>2</sub>(&Gamma;).<sup>[<a href="notes.html#31" name="note-31">31</a>]</sup></li>
</ol>

<p>

One might think that a corollary to Proposition 3.11 is that in a game
of perfect information, common knowledge of the game and of rationality
implies the backwards induction solution. This is the <em>classical
argument</em> for the backwards induction solution. Many game theorists
continue to accept the classical argument, but in recent years, the
argument has come under strong challenge, led by the work of Reny
(1987, 1992), Binmore (1987) and Bicchieri (1989, 1993). The basic idea
underlying their criticisms of backwards induction can be illustrated
with the Figure 1.2 game. According to the classical argument, if Alan
and Fiona have common knowledge of rationality and the game, then each
will predict that the other will follow her end of the backwards
induction solution, to which his end of the backwards induction
solution is his unique best response. However, what if Fiona
reconsiders what to do if she finds herself at the information set
<em>I</em><sup>22</sup>? If the information set <em>I</em><sup>22</sup>
is reached, then Alan has of course not followed the backwards
induction solution. If we assume that at <em>I</em><sup>22</sup>, Fiona
knows only what is stated in (iii), then she can explain her being at
<em>I</em><sup>22</sup> as a failure of either
<strong>K</strong><sub>1</sub><strong>K</strong><sub>2</sub><strong>K</strong><sub>
1</sub>(<em>R</em><sub>2</sub>) or
<strong>K</strong><sub>1</sub><strong>K</strong><sub>2</sub><strong>K</strong><sub>
1</sub><strong>K</strong><sub>2</sub>(&Gamma;) at
<em>I</em><sup>11</sup>. In this case, Fiona's thinking that either
&sim;<strong>K</strong><sub>1</sub><strong>K</strong><sub>2</sub><strong>
K</strong><sub>1</sub>(<em>R</em><sub>2</sub>) or
&sim;<strong>K</strong><sub>1</sub><strong>K</strong><sub>2</sub><strong>
K</strong><sub>1</sub><strong>K</strong><sub>2</sub>(&Gamma;) at
<em>I</em><sup>11</sup> is compatible with what Alan in fact does know
at <em>I</em><sup>11</sup>, so Fiona should not necessarily be
surprised to find herself at <em>I</em><sup>22</sup>, and given that
what she knows there is characterized by (iii), following the backwards
induction solution is her best strategy. But if rationality and the
game are common knowledge, or even if Fiona and Alan both have just
have mutual knowledge of the statements characterized by (iii) and
(iv), then at <em>I</em><sup>22</sup>, Fiona knows that
<strong>K</strong><sub>1</sub><strong>K</strong><sub>2</sub><strong>K</strong><sub>
1</sub>(<em>R</em><sub>2</sub>) or
<strong>K</strong><sub>1</sub><strong>K</strong><sub>2</sub><strong>K</strong><sub>
1</sub><strong>K</strong><sub>2</sub>(&Gamma;) at
<em>I</em><sup>11</sup>. Hence given this much mutual knowledge, Fiona
no longer can explain why Alan has deviated from the backwards
induction solution, since this deviation contradicts part of what is
their mutual knowledge. So if she finds herself at
<em>I</em><sup>22</sup>, Fiona does not necessarily have good reason to
think that Alan will follow the backwards induction solution of the
subgame beginning at <em>I</em><sup>22</sup>, and hence she might not
have good reason to follow the backwards induction solution, either.
Bicchieri (1993), who along with Binmore (1987) and Reny (1987, 1992)
extends this argument to games of perfect information with arbitrary
length, draws a startling conclusion: If agents have strictly too few
or <em>strictly too many</em> levels of mutual knowledge of rationality
and the game relative to the number of potential moves, one cannot
predict that they will follow the backwards induction solution. This
would undermine the central role backwards induction has played in the
analysis of extensive form games. For why should the number of levels
of mutual knowledge the agents have depend upon the length of the game?</p>

<p>

The classical argument for backwards induction implicitly assumes
that at each stage of the game, the agents discount the preceding moves
as strategically irrelevant. Defenders of the classical argument can
argue that this assumption makes sense, since by definition at any
agents' decision node, the previous moves that led to this node are now
fixed. Critics of the classical argument question this assumption,
contending that when reasoning about how to move at any of his
information sets, <em>including those not on the backwards induction
equilibrium path</em>, part of what an agent must consider is what
conditions might have led to his being at that information set. In
other words, agents should incorporate reasoning about the reasoning of
the previous movers, or <em>forward induction</em> reasoning, into
their deliberations over how to move at a given information set.
Binmore (1987) and Bicchieri (1993) contend that a backwards induction
solution to a game should be consistent with the solution a
corresponding forward induction argument recommends. As we have seen,
given common knowledge of the game and of rationality, forward
induction reasoning can lead the agents to an apparent contradiction:
The classical argument for backwards induction is predicated on what
agents predict they would do at nodes in the tree that are never
reached. They make these predictions based upon their common knowledge
of the game and of rationality. But forward induction reasoning seems
to imply that if any off-equilibrium node had been reached, common
knowledge of rationality and the game must have failed, so how could
the agents have predicted what would happen at these nodes?</p>

<h3><a name="3.5">3.5 Communication Networks</a></h3>

<p>

Situations in which a member of a population <em>P</em> is willing
to engage in a certain course of action provided that a large enough
portion of <em>P</em> engages in some appropriate behavior are typical
problems of <em>collective action</em>. Consider the case of an agent
who is debating whether to join a revolt. Her decision to join or not
to join will depend on the number of other agents whom she expects to
join the revolt. If such a number is too low, she will prefer not to
revolt, while if the number is sufficiently large, she will prefer to
revolt. Michael Chwe proposes a model where such a situation is modeled
game-theoretically. Players' knowledge about other players' intentions
depends on a <em>social network</em> in which players are located. The
individual &lsquo;thresholds&rsquo; for each player (the number of
other agents that are needed for that specific player to revolt) are
only known by the immediate neighbors in the network. Besides the
intrinsic value of the results obtained by Chwe's analysis regarding
the subject of collective action, his model also provides insights
about both the relation between social networks and common knowledge
and about the role of common knowledge in collective action. For
example, in some situations, first-order knowledge of other agents'
personal thresholds is not sufficient to motivate an agent to take
action, whereas higher-order knowledge or, in the limit, common
knowledge is.</p>

<p>

We present Chwe's model following (Chwe 1999) and (Chwe 2000).
Suppose there is a group <em>P</em> of <em>n</em> people, and each
agent has two strategies: <em>r</em> (revolt, that is participating in
the collective action) and <em>s</em> (stay home and not participate).
Each agent has her own individual <em>threshold</em> &theta; &isin; (1,
2,..., <em>n</em>+1) and she prefers <em>r</em> over <em>s</em> if and
only if the total number of players who revolt is greater that or equal
to her threshold. An agent with threshold 1 always revolts; an agent
with threshold 2 revolts only if another agent does; an agent with
threshold <em>n</em> revolts only if all agents do; an agent with
threshold <em>n</em>+1 never revolts, etc. The agents are located in a
social network, represented by a binary relation &rarr; over
<em>P</em>. The intended meaning of <em>i &rarr; j</em> is that agent
<em>i</em> &lsquo;talks&rsquo; to agent <em>j</em>, that is to say,
agent <em>i knows</em> the threshold of agent <em>j</em>. If we define
<em>B(i)</em> to be the set {<em>j &isin; P : j &rarr; i</em>}, we can
interpret <em>B(i)</em> as <em>i</em>'s &lsquo;neighborhood&rsquo; and
say that, in general, <em>i</em> knows the thresholds of all agents in
her neighborood. A further assumption is that, for all <em>j,k &isin;
B(i)</em>, <em>i</em> knows whetehr <em>j &rarr; k</em> or not, that
is, every agent knows whether her neighbors are communicating with each
other. The relation &rarr; is taken to be reflexive (one knows her own
threshold).</p>

<p>

Players' knowledge is represented as usual in a possible worlds
framework. Consider for example the case in which there are two
agents, both with one of thresholds 1, 2 or 3. There are nine possible
worlds represented by ordered pairs of numbers, representing the first
and second player's individual thresholds respectively: 11, 12,
13,..., 32, 33. If the players do not communicate, each knows her own
threshold only. Player 1's information partition reflects her
ignorance about player's 2 threshold and it consists of the sets {11,
12, 13}, {21, 22, 23}, {31, 32, 33}; whereas, similarly, player 2's
partition consists of the sets {11, 21, 31}, {12, 22, 32}, {13, 23,
33}. If player 1's threshold is 1, she revolts no matter what player
2's threshold is. Hence, player 1 revolts in {11, 12, 13}. If player
1's threshold is 3, she never revolts.  Hence, she plays <em>s</em> in
{31, 32, 33}. If her threshold is 2, she revolts only if the other
player revolts as well. Since in this example we are assuming that
there is no communication between the agents, player 1 cannot be sure
of player's 2 action, and chooses the non-risky
<em>s</em> in {21, 22, 23} as well. Similarly, player 2 plays
<em>r</em> in {11, 21, 31} and <em>s</em> otherwise. Consider now the
case in which 1 &rarr; 2 and 2 &rarr; 1. Both players have now the
finest information partitions. Thresholds of 1 and 3 yield <em>r</em>
and <em>s</em>, respectively, for both players again. However, in
player 1's cells {21} and {22}, she knows that player 2 will revolt,
and, having threshold 2, she revolts as well. Similarly for player 2 in
his cells {12} and {22}. Note, that the case in which both players have
threshold 2, yields both the equilibrium in which both players revolt
and the equilibrium in which each player stays home. It is assumed that
in the case of multiple equilibria, the one which results in the most
revolt will obtain.</p>

<blockquote><img alt="missing text, please inform" src="figure3.5.gif" />
<br />
 <strong>Figure 3.5</strong></blockquote>

<p>

The analysis of the example above applies to general networks with
<em>n</em> agents. Consider for example the three person network 1
&rarr; 2, 2 &rarr; 1, 2 &rarr; 3, represented in figure 3.5a (notice
that symmetric links are represented by a line without arrowheads) and
assume that each player has threshold 2. The network between players 1
and 2 is the same as the one above, hence if they have threshold 2,
they both revolt regardless of the threshold of player 3. Player 3, on
the other hand, knows her own threshold and player 2's. Hence, if they
all have threshold 2, she cannot distinguish between the possibilities
in the set {122, 222, 322, 422}. At 422, in particular, neither player
1 nor player 2 revolt, hence player 3 cannot take the risk and does
not revolt, <em>even if</em>, in fact, she has a neighbor who
revolts. Adding the link 1 &rarr; 3 to the network (cf. figure 3.5b)
we provide player 3 with knowledge about player 1's action, hence in
this case, if they all have threshold 2, they all revolt. Notice that
if we break the link between players 1 and 2 (so that the network is 1
&rarr; 3 and 2 &rarr; 3), player 3 knows that 1 and 2 cannot
communicate and hence do not revolt at 222, therefore she
chooses <em>s</em> as well. Knowledge of what other players know about
other players is crucial.</p>

<blockquote><img alt="missing text, please inform" src="figure3.6.gif" />
<br />
 <strong>Figure 3.6</strong></blockquote>

<p>

The next example reveals that in some cases not even first-order
knowledge is sufficient to trigger action, and higher levels of
knowledge are necessary. Consider four players, each with threshold 3,
in the two different networks represented in figure 3.6
(&lsquo;square&rsquo;, in figure 3.6a, and &lsquo;kite&rsquo;, in
figure 3.6b.) In the <em>square</em> network, player 1 knows that both
2 and 4 have threshold 3. However, she does not know about player 3's
threshold. If player 3 has threshold 5, then player 2 will never
revolt, since he does not know about player 4's threshold and it is
then possible for him that player 4 has threshold 5 as well. Player 1's
uncertainty about player 3 together with player 1's knowledge of player
2's uncertainty about player 4 force her not to revolt, although she
has threshold 3 and two neighbors with threshold 3 as well. Similar
reasoning applies to all other players, hence in the square no one
revolts. Consider now the <em>kite</em> network. Player 4 ignores
player 1's and player 2's thresholds, hence he does not revolt.
However, player 1 knows that players 2 and 3 have threshold 3, that
they know that they do, and that they know that player 1 knows that
they do. This is enough to trigger action <em>r</em> for the three of
them, and indeed if players 1, 2 and 3 all revolt in all states in
{3331, 3332, 3333, 3334, 3335}, this is an equilibrium since in all
states at least three people revolt each with threshold three.</p>

<p>

The difference between the square and the kite networks is that,
although in the square enough agents are willing to revolt for a revolt
to actually take place, and they all individually know this, no agent
knows that others know it. In the kite, on the other hand, agents in
the triangle not only know that there are three agents with threshold
3, but they also know that they all know it, know that they all know
that they all know it, and so on. There is common knwoledge of such
fact among them. It is interesting to notice that in Chwe's model,
common knowledge obtains without there been a <em>publicly known</em>
fact (cf. section 2.2). The proposition &ldquo;players 1, 2 and 3 all have
threshold 3&rdquo; (semantically: the event {3331, 3332, 3333, 3334, 3335})
is known by players 1, 2 and 3 because of the network structure, and
becomes common knolwedge because the network structure is known by the
players. To be sure, the network structure is not just simply known,
but it is actually commonly known by the players. Player 1, for
example, does not only know that players 2 and 3 communicate with each
other. She also knows that players 2 and 3 know that she knows that
they communicate with each other, and so on.</p>

<p>

In <em>complete</em> networks (networks in which all players
communicate with everyone else, as within the triangle in the kite network)
the information partitions of the players coincide, and they are the
finest partitions of the set of possible worlds. Hence, if players have
sufficiently low thresholds, such fact is commonly known and there is
an equilibrium in which all players revolt.</p>

<blockquote><strong>Definition 3.13</strong>
<br />
 We say that &rarr; is a <em>sufficient network</em> if there is an
equilibrium such that all players choose to revolt.</blockquote>

<p>

For a game in which all players have sufficiently low thresholds, the
complete network is clearly sufficient. Is the complete network
necessary to obtain an equilibrium in which all players revolt? It
turns out that it is not. A crucial role is played by structures of
the same kind as the &lsquo;triangle&rsquo; group in the kite network,
called <em>cliques</em>. In such structures, &lsquo;local&rsquo;
common knowledge (that is, limited to the players part of the
structure) arises naturally. In a complete network (that is, a network
in which there is sufficient but not superfluous communication for it
to fully revolt) in which cliques cover the entire population, if one
clique speaks to another then every member of that clique speaks to
every member of the other clique. Moreover, for every two cliques such
that one is talking to the other, there exists a &lsquo;chain&rsquo;
of cliques with a starting element. In other words, every pair of
cliques in the relation are part of a chain (of length at least 2)
with a starting element (a <em>leading</em> clique.) Revolt propagates
in the network moving from &lsquo;leading adopters&rsquo; to
&lsquo;followers&rsquo;, according to the <em>social role
hierarchy</em> defined by the cliques and their relation. Consider the
following example, in which cliques are represented by circles and
numbers represent the thresholds of individual players:</p>

<blockquote><img alt="missing text, please inform" src="figure3.7.gif" />
<br />
 <strong>Figure 3.7</strong></blockquote>

<p>

Here the threshold 3 clique is the leading clique, igniting revolt
in the threshold 5 follower clique. In turn, the clique of a single
threshold 3 element follows. Notice that although she does not need to
know that the leading clique actually revolts to be willing to revolt,
that information is needed to ensure that the threshold 5 clique does
revolt, and hence that it is safe for her to join the revolt. While in
each clique information about thresholds and hence willingness to
revolt is common knowledge, in a chain of cliques information is
&lsquo;linear&rsquo;; each clique knows about the clique of which it is
a follower, but does not know about earlier cliques.</p>

<p>

Analyzing Chwe's models for collective action under the respect of
weak versus strong links (cf. both Chwe 1999 and Chwe 2000) provides
further insights about the interaction between communication networks
and common knolwedge. A strong link, roughly speaking, joins close
friends, whereas a weak link joins acquaintances. Strong links tend to
increase more slowly than weak ones, since people have common close
friends more often than they share acquaintances. In terms of spreading
information and connecting society, then, weak links do a better job
than strong links, since they traverse society more quickly and have
therefore larger reach. What role do strong and weak links play in
collective action? In Chwe's dynamic analysis, strong links fare
better when thresholds are low, whereas weak links are better when
players' thresholds are higher. Intuitively, one sees that strong links
tend to form small cliques right away (because of the symmetry
intrinsic in them: my friends' friends tend to be my friends as well);
common knowledge arises quickly at the local level and, if thresholds
are low, there is a better chance that a group tied by a strong link
becomes a leading clique initiating revolt. If, on the other hand,
thresholds are high, local common knowledge in small cliques is
fruitless, and weak links, reaching further distances more quickly, speed up
communication and building of the large cliques needed to sparkle
collective action. Such considerations shed some light on the relation
between social networks and common knowledge. While it is true that
knowledge spreads faster in networks in which weak links predominate,
higher-order knowledge (and, hence, common knowledge) tends to arise
more slowly in this kind of networks. Networks with a larger number of
strong links, on the other hand, facilitate the formation of common
knowledge at the local level.</p>

<h2><a name="4">4. Is Common Knowledge Attainable?</a></h2>

<p>

Lewis formulated an account of common knowledge which generates the
hierarchy of&lsquo;<em>i</em> knows that <em>j</em> knows that &hellip;
<em>k</em> knows that <em>A</em>&rsquo; propositions in order to ensure
that in his account of convention, agents have correct beliefs about
each other. But since human agents obviously cannot reason their way
through such an infinite hierarchy, it is natural to wonder whether any
group of people can have full common knowledge of any proposition. More
broadly, the analyses of common knowledge reviewed in &sect;3 would be
of little worth to social scientists and philosophers if this common
knowledge lies beyond the reach of human agents.</p>

<p>

Fortunately for Lewis' program, there are strong arguments that common
knowledge is indeed attainable. Lewis (1969) argues that the common
knowledge hierarchy should be viewed as a chain of implications, and
not as steps in anyone's actual reasoning. He gives informal arguments
that the common knowledge hierarchy is generated from a finite set of
axioms. We saw in &sect;2 that it is possible to formulate Lewis'
axioms precisely and to derive the common knowledge hierarchy from
these axioms and a <em>public event</em> functioning as a basis for
common knowledge. Again, the basic idea behind Lewis' argument is that
for a set of agents, if a proposition A is publicly known among them
and each agent knows that everyone can draw the same
conclusion <em>p</em> from A that she can, then <em>p</em> is common
knowledge. These conditions are obviously context dependent, just as
an individual's knowing or not knowing a proposition is context
dependent.  Yet there are many cases where it is natural to assume
that a public event generates common knowledge, because it is properly
broadcast, agents in the group are in ideal conditions to perceive it,
the inference from the public event to the object of common knowledge
is immediate, etc. Common knowledge could fail if some of the people
failed to perceive the public event, or if some of them believed that
some of the others could not understand the announcement, or hear it,
or could not draw the necessary inferences, and so on. Skeptical doubt
about common knowledge is certainly possible, but such doubt relies
upon <em>ad hoc</em> assumptions similar to those that are needed to
explain failure of <em>individual</em> knowledge, not with the
attainability of common knowledge in principle. Nevertheless, care
must be taken in ascribing common knowledge to a group of human
agents. Common knowledge is a phenomenon highly sensitive to the
agents' circumstances. The following section gives an example that
shows that in order for <em>A</em> to be a common truism for a set of
agents, they ordinarily must perceive an event which
implies <em>A</em> <em>simultaneously</em> and <em>publicly.</em></p>

<h2><a name="5">5. Coordination and Common <em>p</em>-Belief</a></h2>

<p>

In certain contexts, agents might not be able to achieve common
knowledge. Might they achieve something &ldquo;close&rdquo;? One weakening of
common knowledge is of course <em>m</em><sup>th</sup> level mutual
knowledge. For a high value of <em>m</em>,
<strong>K</strong><sup><em>m</em></sup><sub><em>N</em></sub>(<em>A</em>)
might seem a good approximation of
<strong>K</strong><sup>*</sup><sub><em>N</em></sub>(<em>A</em>).
However, the following example, due to Rubinstein (1989, 1992), shows
that simply truncating the common knowledge hierarchy at any finite
level can lead agents to behave as if they had no mutual knowledge at
 all.<sup>[<a href="notes.html#32" name="note-32">32</a>]</sup></p>

<h3><a name="5.1">5.1 The E-mail Coordination Example</a></h3>

<p>

Lizzi and Joanna are faced with the coordination problem summarized in
the following figure: </p>

<blockquote>
  <img src="figure5.1.gif" alt="Figure 5.1" />
  <br />
  <strong>Figure 5.1</strong>
</blockquote>

<p>

In Figure 5.1, the payoffs are dependent upon a pair of possible
worlds. World &omega;<sub>1</sub> occurs with probability
&mu;(&omega;<sub>1</sub>) = .51, while &omega;<sub>2</sub> occurs with
probability &mu;(&omega;<sub>2</sub>) = .49. Hence, they coordinate
with complete success by both choosing <em>A</em> (<em>B</em>) only if
the state of the world is &omega;<sub>1</sub> (&omega;<sub>2</sub>). </p>

<p>

Suppose that Lizzi can observe the state of the world, but Joanna
cannot. We can interpret this game as follows: Joanna and Lizzi would
like to have a dinner together prepared by Aldo, their favorite chef.
Aldo alternates between <em>A</em> and <em>B</em>, the two branches of
Sorriso, their favorite restaurant. State &omega;<sub><em>i</em></sub>
is Aldo's location that day. At state &omega;<sub>1</sub>
(&omega;<sub>2</sub>), Aldo is at <em>A</em> (<em>B</em>). Lizzi, who
is on Sorriso's special mailing list, receives notice of
&omega;<sub><em>i</em></sub>. Lizzi's and Joanna's best outcome occurs
when they meet where Aldo is working, so they can have their planned
dinner. If they meet but miss Aldo, they are disappointed and do not
have dinner after all. If either goes to <em>A</em> and finds herself
alone, then she is again disappointed and does not have dinner. But
what each really wants to avoid is going to <em>B</em> if the other
goes to <em>A</em>. If either of them arrives at <em>B</em> alone, she
not only misses dinner but must pay the exorbitant parking fee of the
hotel which houses <em>B</em>, since the headwaiter of <em>B</em>
refuses to validate the parking ticket of anyone who asks for a table
for two and then sits alone. This is what Harsanyi (1967) terms a game
of <em>incomplete information</em>, since the game's payoffs depend
upon states which not all the agents know.</p>

<p>

<em>A</em> is a &ldquo;play-it-safe&rdquo; strategy for both Joanna and
 Lizzi.<sup>[<a href="notes.html#33" name="note-33">33</a>]</sup>
 By choosing <em>A</em> whatever the state
of the world happens to be, the agents run the risk that they will fail
to get the positive payoff of meeting where Aldo is, but each is also
sure to avoid the really bad consequence of choosing <em>B</em> if the
other chooses <em>A</em>. And since only Lizzi knows the state of the
world, neither can use information regarding the state of the world to
improve their prospects for coordination. For Joanna has no such
information, and since Lizzi knows this, she knows that Joanna has to
choose accordingly, so Lizzi must choose her best response to the move
she anticipates Joanna to make regardless of the state of the world
Lizzi observes. Apparently Lizzi and Joanna cannot achieve expected
payoffs greater than 1.02 for each, their expected payoffs if they
choose (<em>A</em>, <em>A</em>) at either state of the world.</p>

<p>

If the state &omega; were common knowledge, then the conditional
strategy profile (<em>A</em>, <em>A</em>) if &omega; =
&omega;<sub>1</sub> and (<em>B</em>, <em>B</em>), if &omega; =
&omega;<sub>2</sub> would be a strict Nash equilibrium at which each
would achieve a payoff of 2. So the obvious remedy to their predicament
would be for Lizzi to tell Joanna Aldo's location in a face-to-face or
telephone conversation and for them to agree to go where Aldo is, which
would make the state &omega; and their intentions to coordinate on the
best outcome given &omega; common knowledge between them. Suppose for
some reason they cannot talk to each other, but they prearrange that
Lizzi will send Joanna an e-mail message if, and only if,
&omega;<sub>2</sub> occurs. Suppose further that Joanna's and Lizzi's
e-mail systems are set up to send a reply message automatically to the
sender of any message received and viewed, and that due to technical
problems there is a small probability, &epsilon; &gt; 0, that any
message can fail to arrive at its destination. Then if Lizzi sends
Joanna a message, and receives an automatic confirmation, then Lizzi
knows that Joanna knows that &omega;<sub>2</sub> has occurred. If
Joanna receives an automatic confirmation of Lizzi's automatic
confirmation, then Joanna knows that Lizzi knows that Joanna knows that
&omega;<sub>2</sub> occurred, and so on. That &omega;<sub>2</sub> has
occurred would become common knowledge if each agent received
infinitely many automatic confirmations, assuming that all the
confirmations could be sent and received in a finite amount of
 time.<sup>[<a href="notes.html#34" name="note-34">34</a>]</sup>
 However, because of the probability
&epsilon; of transmission failure at every stage of communication, the
sequence of confirmations stops after finitely many stages with
probability one. With probability one, therefore, the agents fail to
achieve full common knowledge. But they do at least achieve something
&ldquo;close&rdquo; to common knowledge. Does this imply that they have good
prospects of settling upon (<em>B</em>, <em>B</em>)?</p>

<p>

Rubinstein shows by induction that if the number of automatically
exchanged confirmation messages is finite, then <em>A</em> is the only
choice that maximizes expected utility for each agent, given what she
knows about what they both know.</p>

<blockquote><a href="rubinsteinsproof.html" name="return-1">Rubinstein's Proof</a>
<br />
</blockquote>

<p>

So even if agents have &ldquo;almost&rdquo; common knowledge, in the sense that
the number of levels of knowledge in &ldquo;Joanna knows that Lizzi knows
that &hellip; that Joanna knows that &omega;<sub>2</sub> occurred&rdquo; is
very large, their behavior is quite different from their behavior given
common knowledge that &omega;<sub>2</sub> has occurred. Indeed, as
Rubinstein points out, given merely &ldquo;almost&rdquo; common knowledge, the
agents choose as if no communication had occurred at all! Rubinstein
also notes that this result violates our intuitions about what we would
expect the agents to do in this case. (See Rubinstein 1992, p. 324.) If
<em>T</em><sub><em>i</em></sub> = 17, wouldn't we expect agent
<em>i</em> to choose <em>B</em>? Indeed, in many actual situations we
might think it plausible that the agents would each expect the other to
choose <em>B</em> even if <em>T</em><sub>1</sub> =
<em>T</em><sub>2</sub> = 2, which is all that is needed for Lizzi to
know that Joanna has received her original message and for Joanna to
know that Lizzi knows this! Binmore and Samelson (2001) in fact show
that if Joanna and Lizzi incur a cost when paying attention to the
messages they exchange, or if sending a message is costly, then longer
streams of messages are not paid attention to or do not occur,
respectively.</p>

<h3><a name="5.2">5.2 Common <em>p</em>-Belief</a></h3>

<p>

The example in Section 5.1 hints that mutual knowledge is not the only
weakening of common knowledge that is relevant to coordination.
Brandenburger and Dekel (1987), Stinchcombe (1988) and Monderer and
Samet (1989) explore another option, which is to weaken the properties
of the <strong>K</strong><sup>*</sup><sub><em>N</em></sub> operator.
Monderer and Samet motivate this approach by noting that even if a
mutual knowledge hierarchy stops at a certain level, agents might still
have higher level mutual <em>beliefs</em> about the proposition in
question. So they replace the knowledge operator
<strong>K</strong><sub><em>i</em></sub> with a <em>belief operator</em>
<strong>B</strong><sup><em>p</em></sup><sub><em>i</em></sub>: </p>

<blockquote><strong>Definition 5.1</strong>
<br />
 If &mu;<sub><em>i</em></sub>(<font size="+2">&middot;</font>) is agent
<em>i</em>'s probability distribution over &Omega;, then 

<blockquote>
<strong>B</strong><sup><em>p</em></sup><sub><em>i</em></sub>(<em>A</em>)
= { &omega; |
&mu;<sub><em>i</em></sub>(<em>A</em> |
 <span class="scriptuc">H</span><sub><em>i</em></sub>(&omega;))
&ge; <em>p</em> }</blockquote>
</blockquote>

<p>
<strong>B</strong><sup><em>p</em></sup><sub><em>i</em></sub>(<em>A</em>)
is to be read &lsquo;<em>i</em> believes <em>A</em> (given <em>i</em>'s
private information) with probability at least <em>p</em> at
&omega;&rsquo;, or &lsquo;<em>i</em> &nbsp; <em>p</em>-believes
<em>A</em>&rsquo;. The belief operator
<strong>B</strong><sup><em>p</em></sup><sub><em>i</em></sub> satisfies
axioms K2, K3, and K4 of the knowledge operator.
<strong>B</strong><sup><em>p</em></sup><sub><em>i</em></sub> does not
satisfy K1, but does satisfy the weaker property</p>

<blockquote>&mu;<sub><em>i</em></sub>(<em>A</em> |
<strong>B</strong><sup><em>p</em></sup><sub><em>i</em></sub>(<em>A</em>))
&ge; <em>p</em></blockquote>

<p>

that is, if one believes <em>A</em> with probability at least
<em>p</em>, then the probability of <em>A</em> is indeed at least
<em>p</em>.</p>

<p>

One can define <em>mutual</em> and <em>common p-beliefs</em>
recursively in a manner similar to the definition of mutual and common
knowledge:</p>

<blockquote><strong>Definition 5.2</strong>
<br />
 Let a set &Omega; of possible worlds together with a set of agents
<em>N</em> be given. 

<p>

(1) The proposition that <em>A</em> is <em>(first level or first
order) mutual</em> p-belief for the agents of <em>N</em>,
<strong>B</strong><sup><em>p</em></sup><sub><em>N</em><sup>1</sup></sub>(<em>A</em>),
is the set defined by</p>

<blockquote>
<strong>B</strong><sup><em>p</em></sup><sub><em>N</em><sup>1</sup></sub>(<em>A</em>)
&equiv; <font size="+2">&cap;</font><sub><em>i</em>&isin;<em>N</em></sub>
<strong>B</strong><sup><em>p</em></sup><sub><em>i</em></sub>(<em>A</em>).</blockquote>

<p>

(2) The proposition that <em>A</em> is <em>m</em><sup>th</sup>
<em>level</em> (or <em>m</em><sup>th</sup> <em>order</em>)
<em>mutual</em> p-belief among the agents of <em>N</em>,
<strong>B</strong><sup><em>p</em></sup><sub><em>N</em><sup><em>m</em></sup></sub>(<em>A</em>),
 is defined recursively as the set </p>

<blockquote>
<strong>B</strong><sup><em>p</em></sup><sub><em>N</em><sup><em>m</em></sup></sub>(<em>A</em>)
 &equiv; <font size="+2">&cap;</font><sub><em>i</em>&isin;<em>N</em></sub>
<strong>B</strong><sup><em>p</em></sup><sub><em>i</em></sub>(<strong>B</strong><sup><em>p</em></sup><sub>N<sup><em>m</em>&minus;1</sup></sub>(<em>A</em>))</blockquote>

<p>

(3) The proposition that <em>A</em> is <em>common p-belief</em> among
the agents of <em>N</em>,
<strong>B</strong><sup><em>p</em></sup><sub><em>N</em><sup>*</sup></sub>(<em>A</em>),
is defined as the set </p>

<blockquote>
<table cellspacing="0" cellpadding="0">
<tr>
<td><strong>B</strong><sup><em>p</em></sup><sub><em>N</em><sup>*</sup></sub>(<em>A</em>)
&equiv;</td>
<td align="center">
   <span class="index">&infin;</span><br />
   <font size="+2">&cap;</font><br />
   <span class="index"><em>m</em>=1</span>
</td>
<td>
<strong>B</strong><sup><em>p</em></sup><sub><em>N</em><sup><em>m</em></sup></sub>(<em>A</em>).</td>
</tr>
</table>
</blockquote>
</blockquote>

<p>

If <em>A</em> is common (or <em>m</em><sup>th</sup> level mutual)
knowledge at world &omega;, then <em>A</em> is common
(<em>m</em><sup>th</sup> level) <em>p</em>-belief at &omega; for every
value of <em>p</em>. So mutual and common <em>p</em>-beliefs formally
generalize the mutual and common knowledge concepts. However, note that
<strong>B</strong><sup>1</sup><sub><em>N</em><sup>*</sup></sub>(<em>A</em>) is not
necessarily the same proposition as
<strong>K</strong><sup>*</sup><sub><em>N</em></sub>(<em>A</em>), that
is, even if <em>A</em> is common 1-belief, <em>A</em> can fail to be
common knowledge.</p>

<p>

Common <em>p</em>-belief forms a hierarchy similar to a common
knowledge hierarchy:</p>

<blockquote>
 <p><strong>Proposition 5.3</strong>
<br />
 &omega; &isin;
<strong>B</strong><sup><em>p</em></sup><sub><em>N</em><sup><em>m</em></sup></sub>(<em>A</em>) iff</p>

<p>(1) For all agents <em>i</em><sub>1</sub>,
<em>i</em><sub>2</sub>, &hellip; , <em>i</em><sub><em>m</em></sub>
&isin; <em>N</em>, &omega; &isin;
<strong>B</strong><sup><em>p</em></sup><sub><em>i</em><sub>1</sub></sub><strong>B</strong><sup><em>p</em></sup><sub><em>i</em><sub>2</sub></sub> &hellip;
<strong>B</strong><sup><em>p</em></sup><sub><em>i</em><sub><em>m</em></sub></sub>(<em>A</em>)</p>

<p>Hence, &omega; &isin;
<strong>B</strong><sup><em>p</em></sup><sub><em>N</em><sup>*</sup></sub>(<em>A</em>)
iff (1) is the case for each <em>m</em> &ge; 1. </p>

<p>

<strong>Proof</strong>. Similar to the
 <a href="proof2.5.html">Proof of Proposition 2.5</a>.</p>
</blockquote>

<p>

One can draw several morals from the e-mail game of Example 5.1.
Rubinstein (1987) argues that his conclusion seems paradoxical for the
same reason the backwards induction solution of Alan's and Fiona's
perfect information game might seem paradoxical: Mathematical induction
does not appear to be part of our &ldquo;everyday&rdquo; reasoning. This game also
shows that in order for A to be a common truism for a set of agents,
they ordinarily must perceive an event which implies A
<em>simultaneously</em> in each others' presence. A third moral is that
in some cases, it may make sense for the agents to employ some solution
concept weaker than Nash or correlated equilibrium. In their analysis
of the e-mail game, Monderer and Samet (1989) introduce the notions of
<em>ex ante</em> and <em>ex post &epsilon;-equilibrium.</em> An <em>ex
ante</em> equilibrium <em>h</em> is a system of strategy profiles such
that no agent <em>i</em> expects to gain more than &epsilon;-utiles if
<em>i</em> deviates from <em>h</em>. An <em>ex post</em> equilibrium
<em>h</em>&prime; is a system of strategy profiles such that no agent
<em>i</em> expects to gain more than &epsilon;-utiles by deviating from
<em>h</em>&prime; given <em>i</em>'s private information. When
&epsilon; = 0, these concepts coincide, and <em>h</em> is a Nash
equilibrium. Monderer and Samet show that, while the agents in the
e-mail game can never achieve common knowledge of the world &omega;, if
they have common <em>p</em>-belief of &omega; for sufficiently high
<em>p</em>, then there is an <em>ex ante</em> equilibrium at which they
follow (<em>A</em>,<em>A</em>) if &omega; = &omega;<sub>1</sub> and
(<em>B</em>,<em>B</em>), if &omega; = &omega;<sub>2</sub>. This
equilibrium turns out not to be <em>ex post.</em> However, if the
situation is changed so that there are no replies, then Lizzi and
Joanna could have at most first order mutual knowledge that &omega; =
&omega;<sub>2</sub>. Monderer and Samet show that in this situation,
given sufficiently high common <em>p</em>-belief that &omega; =
&omega;<sub>2</sub>, there is an <em>ex post</em> equilibrium at which
Joanna and Lizzi choose (<em>B</em>,<em>B</em>) if &omega; =
&omega;<sub>2</sub>! So another way one might view this third moral of
the e-mail game is that agents' prospects for coordination can
sometimes improve dramatically if they rely on their common beliefs as
well as their mutual knowledge.</p>

</div>

<div id="bibliography">

<h2><a name="Bib">Bibliography</a></h2>

<h3>Annotations</h3>

<p>

Lewis (1969) is the classic pioneering study of common knowledge and
its potential applications to conventions and game theory. As Lewis
acknowledges, parts of his work are foreshadowed in Hume (1740) and
Schelling (1960). </p>

<p>

Aumann (1976) gives the first mathematically rigorous formulation of
common knowledge using set theory. Schiffer (1972) uses the formal
vocabulary of <em>epistemic logic</em> (Hintikka 1962) to state his
definition of common knowledge. Schiffer's general approach is to
augment a system of sentential logic with a set of knowledge operators
corresponding to a set of agents, and then to define common knowledge
as a hierarchy of propositions in the augmented system. Bacharach
(1992), Bicchieri (1993) and Fagin, <em>et al</em>. (1995) adopt this
approach, and develop logical theories of common knowledge which
include soundness and completeness theorems. Fagin, et al. show that
the syntactic and set-theoretic approaches to developing common
knowledge are logically equivalent.</p>

<p>

Aumann (1995) gives a recent defense of the classical view of
backwards induction in games of imperfect information. For criticisms
of the classical view, see Binmore (1987), Reny (1992), Bicchieri
(1989) and especially Bicchieri (1993). Brandenburger (1992) surveys
the known results connecting mutual and common knowledge to solution
concepts in game theory. For more in-depth survey articles on common
knowledge and its applications to game theory, see Binmore and
Brandenburger (1989), Geanakoplos (1994) and Dekel and Gul (1996). For
her alternate account of common knowledge along with an account of
conventions which opposes Lewis' account, see Gilbert (1989).</p>

<p>

Monderer and Samet (1989) remains one of the best resources for the
study of common p-belief.</p>

<h3>References</h3>

<ul class="hanging">

<li>Alberucci, Luca and Jaeger, Gerhard, 2005, &ldquo;About cut elimination
for logics of common knowledge&rdquo;, <em>Annals of Pure and Applied
Logic</em>, 133(1&ndash;3): 73&ndash;99.</li>

<li>Aumann, Robert, 1974, &ldquo;Subjectivity and Correlation in Randomized
Strategies&rdquo;, <em>Journal of Mathematical Economics</em>, 1: 67&ndash;96.</li>

<li>&ndash;&ndash;&ndash;, 1976, &ldquo;Agreeing to Disagree&rdquo;, <em>Annals of
Statistics</em>, 4: 1236&ndash;9.</li>

<li>&ndash;&ndash;&ndash;, 1987, &ldquo;Correlated Equilibrium as an Expression of
Bayesian Rationality&rdquo;, <em>Econometrica</em>, 55: 1&ndash;18.</li>

<li>&ndash;&ndash;&ndash;, 1995, &ldquo;Backward Induction and Common Knowledge of
Rationality&rdquo;, <em>Games and Economic Behavior</em> 8: 6&ndash;19.</li>

<li>Bacharach, Michael, 1989, &ldquo;Mutual Knowledge and Human Reason&rdquo;,
mimeo.</li>

<li>Barwise, Jon, 1988, &ldquo;Three Views of Common Knowledge&rdquo;, in
<em>Proceedings of the Second Conference on Theoretical Aspects of
Reasoning About Knowledge</em>, M.Y. Vardi (ed.), San Francisco: Morgan
Kaufman, pp. 365&ndash;379.</li>

<li>&ndash;&ndash;&ndash;, 1989, <em>The Situation in Logic</em>, Stanford:
Center for the Study of Language and Information.</li>

<li>Bernheim, B. Douglas, 1984, &ldquo;Rationalizable Strategic Behavior&rdquo;,
<em>Econometrica</em>, 52: 1007&ndash;1028.</li>

<li>Bicchieri, Cristina, 1989, &ldquo;Self Refuting Theories of Strategic
Interaction: A Paradox of Common Knowledge&rdquo;, <em>Erkenntnis</em>, 30:
69&ndash;85.</li>

<li>&ndash;&ndash;&ndash;, 1993, <em>Rationality and Coordination</em>, Cambridge:
Cambridge University Press.</li>

<li>&ndash;&ndash;&ndash;, 2006, <em>The Grammar of Society</em>, Cambridge:
Cambridge University Press.</li>

<li>Binmore, Ken, 1987, &ldquo;Modelling Rational Players I&rdquo;, <em>Economics
and Philosophy</em>, 3: 179&ndash;241.</li>

<li>&ndash;&ndash;&ndash;, 1992, <em>Fun and Games</em>, Lexington, MA:
D. C. Heath.</li>

<li>&ndash;&ndash;&ndash;, 2008, &ldquo;Do conventions need to be common
knowledge?&rdquo;, <em>Topoi</em>, 27: 17&ndash;27.</li>

<li>Binmore, Ken and Brandenburger, Adam, 1988, &ldquo;Common knowledge and
Game theory&rdquo; ST/ICERD Discussion Paper 88/167, London School of
Economics.</li>

<li>Binmore, Ken and Samuelson, Larry, 2001, &ldquo;Coordinated action
in the electronic mail game&rdquo; <em>Games and Economic
Behavior</em>, 35(1): 6&ndash;30.</li>

<li>Bonanno, Giacomo and Battigalli, Pierpaolo, 1999, &ldquo;Recent results
on belief, knowledge and the epistemic foundations of game theory&rdquo;,
<em>Research in Economics</em>, 53(2): 149&ndash;225.</li>

<li>Bonnay, D. and Egr&eacute;, Paul, 2009, &ldquo;Inexact knowledge with introspection&rdquo;,
<em>Journal of Philosophical Logic</em>, 38: 179&ndash;227.</li>

<li>Brandenburger, Adam, 1992, &ldquo;Knowledge and Equilibrium in Games&rdquo;,
<em>Journal of Economic Perspectives</em>, 6: 83&ndash;101.</li>

<li>Brandenburger, Adam, and Dekel, Eddie, 1987, &ldquo;Common knowledge with
Probability 1&rdquo;, <em>Journal of Mathematical Economics</em>, 16:
237&ndash;245.</li>

<li>&ndash;&ndash;&ndash;, 1988, &ldquo;The Role of Common
Knowledge Assumptions in Game Theory&rdquo;, in <em>The Economics of Missing
Markets, Information and Games</em>, Frank Hahn (ed.), Oxford: 
Clarendon Press, 46&ndash;61.</li>

<li>Carnap, Rudolf, 1947, <em>Meaning and Necessity: A Study in
Semantics and Modal Logic</em>, Chicago, University of Chicago
Press.</li>

<li>Chwe, Michael, 1999, &ldquo;Structure and Strategy in Collective
Action&rdquo;, <em>American Journal of Sociology</em> 105: 128&ndash;56.</li>

<li>&ndash;&ndash;&ndash;, 2000, &ldquo;Communcation and Coordination in Social
Networks&rdquo;, <em>Review of Economic Studies</em>, 67: 1&ndash;16.</li>

<li>&ndash;&ndash;&ndash;, 2001, <em>Rational Ritual</em>, Princeton, NJ:
Princeton University Press</li>

<li>Cubitt, Robin and Sugden, Robert, 2003, &ldquo;Common Knowledge, Salience
and Convention: A Reconstruction of David Lewis' Game Theory&rdquo;,
<em>Economics and Philosophy</em>, 19: 175&ndash;210.</li>

<li>Dekel, Eddie and Gul, Faruk, 1996, &ldquo;Rationality and
Knowledge in Game Theory&rdquo;, working paper, Northwestern and
Princeton Universities.</li>

<li>Dekel, Eddie, Lipman, Bart and Rustichini,
Aldo, 1998, &ldquo;Standard state-space models preclude
unawareness,&rdquo; <i>Econometrica</i>, 66: 159&ndash;173.</li>

<li>Devetag, Giovanna, Hosni, Hykel and Sillari,
Giacomo, 2013, &ldquo;Play 7: mutual versus common knowledge of advice
in a weak-link game,&rdquo; <i>Synthese</i>, 190(8): 1351-1381</li>

<li>Fagin, Ronald and Halpern, Joseph Y., 1988, &ldquo;Awareness and
Limited Reasoning,&rdquo; <em>Artificial Intelligence</em>,
34: 39&ndash;76.</li>

<li>Fagin, Ronald, Halpern, Joseph Y., Moses, Yoram and Vardi, Moshe
Y.,  1995, <em>Reasoning About Knowledge</em>, Cambridge, MA:
MIT Press.</li>

<li>Friedell, Morris, 1967, &ldquo;On the Structure of Shared
Awareness,&rdquo; <em>Working papers of the Center for Research on
Social Organizations</em>, Ann Arbor: University of Michigan, paper 27.</li>

<li>&ndash;&ndash;&ndash;, 1969, &ldquo;On the Structure of Shared
Awareness,&rdquo; <i>Behavioral Science</i>, 14(1): 28&ndash;39.</li>

<li>Geanakoplos, John, 1989, &ldquo;Games theory without partitions,
and applications to speculation and consensus,&rdquo; Cowles
Foundation Discussion Paper No. 914.</li>

<li>&ndash;&ndash;&ndash;, 1994, &ldquo;Common Knowledge&rdquo;,
in <em>Handbook of Game Theory</em> (Volume 2), Robert Aumann and
Sergiu Hart (eds.), Amsterdam: Elsevier Science B.V., 1438&ndash;1496.</li>

<li>Gilbert, Margaret, 1989, <em>On Social Facts</em>, Princeton: Princeton
University Press.</li>

<li>Halpern, Jospeh, 2001, &ldquo;Alternative Semantics for
Unawareness&rdquo;, <em>Games and Economic Behavior</em>, 37(2): 321&ndash;339</li>

<li>Harman, Gilbert, 1977, &ldquo;Review of <em>Linguistic Behavior</em> by
Jonathan Bennett&rdquo;, <em>Language</em>, 53: 417&ndash;424.</li>

<li>Harsanyi, J., 1967, &ldquo;Games with incomplete information played
by &rdquo;Bayesian&ldquo; players, I: The basic
model&rdquo;, <em>Management Science</em>, 14: 159&ndash;82.</li>

<li>&ndash;&ndash;&ndash;, 1968a, &ldquo;Games with incomplete information
played by &rdquo;Bayesian&ldquo; players, II: Bayesian equilibrium
points&rdquo;, <em>Management Science</em>, 14: 320&ndash;324.</li>

<li>&ndash;&ndash;&ndash;, 1968b, &ldquo;Games with incomplete information played by
&rdquo;Bayesian&ldquo; players, III: The basic probability distribution of the
game&rdquo;, <em>Management Science</em>, 14: 486&ndash;502.</li>

<li>Heifetz, Aviad, 1999, &ldquo;Iterative and Fixed Point Common
Belief&rdquo;, <em>Journal of Philosophical Logic</em>, 28(1):
61&ndash;79.</li>

<li>Heifetz, Aviad, Meier, Martin and Schipper, Burkhard, 2006,
&ldquo;Interactive unawareness&rdquo;, <em>Journal of Economic Theory</em>,
130: 78&ndash;94.</li>

<li>Hintikka, Jaakko, 1962, <em>Knowledge and Belief</em>, Ithaca, New
York: Cornell University Press.</li>

<li>Hume, David, 1740 [1888 1976], <em>A Treatise of Human
Nature</em>, L. A. Selby-Bigge (ed.), rev. 2nd. edition P. H. Nidditch
(ed.), Oxford: Clarendon Press.</li>

<li>Lewis, C. I., 1943, &ldquo;The Modes of Meaning&rdquo;, <em>Philosophy and
Phenomenological Research</em>, 4: 236&ndash;250.</li>

<li>Lewis, David, 1969, <em>Convention: A Philosophical Study</em>,
Cambridge, MA: Harvard University Press.</li>

<li>&ndash;&ndash;&ndash;, 1978, &ldquo;Truth in
Fiction&rdquo;, <em>American Philosophical Quarterly</em>, 15:
37&ndash;46.</li>

<li>Littlewood, J. E., 1953, <em>A Mathematical Miscellany</em>,
London: Methuen; reprinted as <em>Littlewood's Miscellany</em>,
B. Bollobas (ed.), Cambridge: Cambridge University Press, 1986.</li>

<li>McKelvey, Richard and Page, Talbot, 1986, &ldquo;Common knowledge,
consensus and aggregate information&rdquo;, <em>Econometrica</em>, 54:
109&ndash;127.</li>

<li>Meyer, J.-J.Ch. and van der Hoek, Wiebe, 1995, <em>Epistemic Logic
for Computer Science and Artificial Intelligence</em> (Cambridge
Tracts in Theoretical Computer Science 41), Cambridge: Cambridge
University Press.</li>

<li>Milgrom, Paul, 1981, &ldquo;An axiomatic characterization of common
knowledge&rdquo;, <em>Econometrica</em>, 49: 219&ndash;222.</li>

<li>Monderer, Dov and Samet, Dov, 1989, &ldquo;Approximating Common Knowledge
with Common Beliefs&rdquo;, <em>Games and Economic Behavior</em>, 1:
170&ndash;190.</li>

<li>Nash, John, 1950, &ldquo;Equilibrium points in n-person games&rdquo;.
<em>Proceedings of the National Academy of Sciences of the United
States</em>, 36: 48&ndash;49.</li>

<li>&ndash;&ndash;&ndash;, 1951, &ldquo;Non-Cooperative Games&rdquo;. <em>Annals of
Mathematics</em>, 54: 286&ndash;295.</li>

<li>Nozick, Robert, 1963, <em>The Normative Theory of Individual
Choice</em>, Ph.D. dissertation, Princeton University</li>

<li>Pearce, David, 1984, &ldquo;Rationalizable Strategic Behavior and the
Problem of Perfection&rdquo;. <em>Econometrica</em>, 52: 1029&ndash;1050.</li>

<li>Reny, Philip, 1987, &ldquo;Rationality, Common Knowledge, and the Theory
of Games&rdquo;, working paper, Department of Economics, University of
Western Ontario.</li>

<li>&ndash;&ndash;&ndash;, 1992, &ldquo;Rationality in Extensive Form Games&rdquo;,
<em>Journal of Economic Perspectives</em>, 6: 103&ndash;118.</li>

<li>Rubinstein, Ariel, 1987, &ldquo;A Game with &rdquo;Almost Common
Knowledge&ldquo;: An Example&rdquo;, in <em>Theoretical
Economics</em>, D. P. 87/165. London School of Economics.</li>

<li>Samet, Dov, 1990, &ldquo;Ignoring Ignorance and Agreeing to Disagree&rdquo;,
<em>Journal of Economic Theory</em>, 52: 190&ndash;207.</li>

<li>Schelling, Thomas, 1960, <em>The Strategy of Conflict</em>,
Cambridge, MA: Harvard University Press.</li>

<li>Schiffer, Stephen, 1972, <em>Meaning</em>, Oxford: Oxford
University Press.</li>

<li>Sillari, Giacomo, 2005, &ldquo;A Logical Framework for Convention&rdquo;,
<em>Synthese</em>, 147(2): 379&ndash;400.</li>

<li>&ndash;&ndash;&ndash;, 2008, &ldquo;Common Knowledge and Convention&rdquo;,
<em>Topoi</em>, 27(1): 29&ndash;39.</li>

<li>Skyrms, Brian, 1984, <em>Pragmatics and Empiricism</em>, New
Haven: Yale University Press.</li>

<li>Stinchcombe, Max, 1988, &ldquo;Approximate Common
Knowledge&rdquo;, mimeo, San Diego: University of California.</li>

<li>Sugden, Robert, 1986, <em>The Economics of Rights, Cooperation and
Welfare</em>, New York: Basil Blackwell.</li>

<li>Vanderschraaf, Peter, 1995, &ldquo;Endogenous Correlated Equilibria in
Noncooperative Games&rdquo;, <em>Theory and Decision</em>, 38: 61&ndash;84.</li>

<li>Vanderschraaf, Peter, 1998, &ldquo;Knowledge, Equilibrium and
Convention&rdquo;, <em>Erkenntnis</em>, 49: 337&ndash;369.</li>

<li>&ndash;&ndash;&ndash;, 2001. <em>A Study in Inductive
Deliberation</em>, New York: Routledge.</li>

<li>von Neumann, John and Morgenstern, Oskar, 1944, <em>Theory of Games
and Economic Behavior</em>, Princeton: Princeton University Press.</li>

</ul>

</div>

<div id="academic-tools">

<h2><a name="Aca">Academic Tools</a></h2>

<blockquote>
<table>
<tr>
<td valign="top"><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=common-knowledge" target="other">How to cite this entry</a>.</td>
</tr>

<tr>
<td valign="top"><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://leibniz.stanford.edu/friends/preview/common-knowledge/" target="other">Preview the PDF version of this entry</a> at the
 <a href="https://leibniz.stanford.edu/friends/" target="other">Friends of the SEP Society</a>.</td>
</tr>

<tr>
<td valign="top"><img src="../../symbols/inpho.png" alt="inpho icon" /></td>
<td><a href="https://www.inphoproject.org/entity?sep=common-knowledge&amp;redirect=True" target="other">Look up topics and thinkers related to this entry</a>
 at the Internet Philosophy Ontology Project (InPhO).</td>
</tr>

<tr>
<td valign="top"><img src="../../symbols/pp.gif" alt="phil papers icon" /></td>
<td><a href="http://philpapers.org/sep/common-knowledge/" target="other">Enhanced bibliography for this entry</a>
at <a href="http://philpapers.org/" target="other">PhilPapers</a>, with links to its database.</td>
</tr>

</table>
</blockquote>

</div>

<div id="other-internet-resources">

<h2><a name="Oth">Other Internet Resources</a></h2>

<ul>
<li><a href="http://www-formal.stanford.edu/jmc/applications/applications.html" target="other">Applications of Circumscription to Formalizing Common Sense Knowledge</a>
 
<li><a href="http://www.econ.ucdavis.edu/faculty/schipper/unaw.htm" target="other">Burkhard C. Schipper's Unawareness Bibliography</a></li>

</ul>

</div>

<div id="related-entries">

<h2><a name="Rel">Related Entries</a></h2>

<p>

 <a href="../convention/index.html">convention</a> |
 <a href="../game-theory/index.html">game theory</a> |
 <a href="../logic-epistemic/index.html">logic: epistemic</a> |
 <a href="../prisoner-dilemma/index.html">prisoner&rsquo;s dilemma</a>

</p>

</div>

</div><!-- #aueditable --><!--DO NOT MODIFY THIS LINE AND BELOW-->

<!-- END ARTICLE HTML -->

</div> <!-- End article-content -->

  <div id="article-copyright">
    <p>
 <a href="../../info.html#c">Copyright &copy; 2013</a> by

<br />
Peter Vanderschraaf
<br />
<a href="https://scienzepolitiche.luiss.it/en/docenti/cv/020229" target="other">Giacomo Sillari</a>
&lt;<a href="m&#97;ilto:gsillari&#37;40luiss&#37;2eit"><em>gsillari<abbr title=" at ">&#64;</abbr>luiss<abbr title=" dot ">&#46;</abbr>it</em></a>&gt;
    </p>
  </div>

</div> <!-- End article -->

<!-- NOTE: article banner is outside of the id="article" div. -->
<div id="article-banner" class="scroll-block">
  <div id="article-banner-content">
    <a href="../../fundraising/index.html">
    Open access to the SEP is made possible by a world-wide funding initiative.<br />
    The Encyclopedia Now Needs Your Support<br />
    Please Read How You Can Help Keep the Encyclopedia Free</a>
  </div>
</div> <!-- End article-banner -->

    </div> <!-- End content -->

    <div id="footer">

      <div id="footer-menu">
        <div class="menu-block">
          <h4><i class="icon-book"></i> Browse</h4>
          <ul role="menu">
            <li><a href="../../contents.html">Table of Contents</a></li>
            <li><a href="../../new.html">What's New</a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
            <li><a href="../../published.html">Chronological</a></li>
            <li><a href="../../archives/index.html">Archives</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-info-sign"></i> About</h4>
          <ul role="menu">
            <li><a href="../../info.html">Editorial Information</a></li>
            <li><a href="../../about.html">About the SEP</a></li>
            <li><a href="../../board.html">Editorial Board</a></li>
            <li><a href="../../cite.html">How to Cite the SEP</a></li>
            <li><a href="../../special-characters.html">Special Characters</a></li>
            <li><a href="../../tools/index.html">Advanced Tools</a></li>
            <li><a href="../../contact.html">Contact</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-leaf"></i> Support SEP</h4>
          <ul role="menu">
            <li><a href="../../support/index.html">Support the SEP</a></li>
            <li><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
            <li><a href="../../support/donate.html">Make a Donation</a></li>
            <li><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
          </ul>
        </div>
      </div> <!-- End footer menu -->

      <div id="mirrors">
        <div id="mirror-info">
          <h4><i class="icon-globe"></i> Mirror Sites</h4>
          <p>View this site from another server:</p>
        </div>
        <div class="btn-group open">
          <a class="btn dropdown-toggle" data-toggle="dropdown" href="https://plato.stanford.edu/">
            <span class="flag flag-usa"></span> USA (Main Site) <span class="caret"></span>
            <span class="mirror-source">Philosophy, Stanford University</span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="../../mirrors.html">Info about mirror sites</a></li>
          </ul>
        </div>
      </div> <!-- End mirrors -->
      
      <div id="site-credits">
        <p>The Stanford Encyclopedia of Philosophy is <a href="../../info.html#c">copyright &copy; 2021</a> by <a href="http://mally.stanford.edu/">The Metaphysics Research Lab</a>, Department of Philosophy, Stanford University</p>
        <p>Library of Congress Catalog Data: ISSN 1095-5054</p>
      </div> <!-- End site credits -->

    </div> <!-- End footer -->

  </div> <!-- End container -->

   <!-- NOTE: Script required for drop-down button to work (mirrors). -->
  <script>
    $('.dropdown-toggle').dropdown();
  </script>

</body>

<!-- Mirrored from seop.illc.uva.nl/entries/common-knowledge/ by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 22 Jun 2022 19:42:38 GMT -->
</html>
