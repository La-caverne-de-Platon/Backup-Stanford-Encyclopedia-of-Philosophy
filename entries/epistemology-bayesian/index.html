<!DOCTYPE html>
<!--[if lt IE 7]> <html class="ie6 ie"> <![endif]-->
<!--[if IE 7]>    <html class="ie7 ie"> <![endif]-->
<!--[if IE 8]>    <html class="ie8 ie"> <![endif]-->
<!--[if IE 9]>    <html class="ie9 ie"> <![endif]-->
<!--[if !IE]> --> <html> <!-- <![endif]-->

<!-- Mirrored from seop.illc.uva.nl/entries/epistemology-bayesian/ by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 22 Jun 2022 19:46:27 GMT -->
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>
Bayesian Epistemology (Stanford Encyclopedia of Philosophy)
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="robots" content="noarchive, noodp" />
<meta property="citation_title" content="Bayesian Epistemology" />
<meta property="citation_author" content="Lin, Hanti" />
<meta property="citation_publication_date" content="2022/06/13" />
<meta name="DC.title" content="Bayesian Epistemology" />
<meta name="DC.creator" content="Lin, Hanti" />
<meta name="DCTERMS.issued" content="2022-06-13" />
<meta name="DCTERMS.modified" content="2022-06-13" />

<!-- NOTE: Import webfonts using this link: -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300,600,200&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css" />

<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap-responsive.min.css" />
<link rel="stylesheet" type="text/css" href="../../css/font-awesome.min.css" />
<!--[if IE 7]> <link rel="stylesheet" type="text/css" href="../../css/font-awesome-ie7.min.css"> <![endif]-->
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/style.css" />
<link rel="stylesheet" type="text/css" media="print" href="../../css/print.css" />
<link rel="stylesheet" type="text/css" href="../../css/entry.css" />
<!--[if IE]> <link rel="stylesheet" type="text/css" href="../../css/ie.css" /> <![endif]-->
<script type="text/javascript" src="../../js/jquery-1.9.1.min.js"></script>
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>

<!-- NOTE: Javascript for sticky behavior needed on article and ToC pages -->
<script type="text/javascript" src="../../js/jquery-scrolltofixed-min.js"></script>
<script type="text/javascript" src="../../js/entry.js"></script>

<!-- SEP custom script -->
<script type="text/javascript" src="../../js/sep.js"></script>
</head>

<!-- NOTE: The nojs class is removed from the page if javascript is enabled. Otherwise, it drives the display when there is no javascript. -->
<body class="nojs article" id="pagetopright">
<div id="container">
<div id="header-wrapper">
  <div id="header">
    <div id="branding">
      <div id="site-logo"><a href="../../index.html"><img src="../../symbols/sep-man-red.png" alt="SEP home page" /></a></div>
      <div id="site-title"><a href="../../index.html">Stanford Encyclopedia of Philosophy</a></div>
    </div>
    <div id="navigation">
      <div class="navbar">
        <div class="navbar-inner">
          <div class="container">
            <button class="btn btn-navbar collapsed" data-target=".collapse-main-menu" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Menu </button>
            <div class="nav-collapse collapse-main-menu in collapse">
              <ul class="nav">
                <li class="dropdown open"><a id="drop1" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-book"></i> Browse</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop1">
                    <li><a href="../../contents.html">Table of Contents</a></li>
                    <li><a href="../../new.html">What's New</a></li>
                    <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
                    <li><a href="../../published.html">Chronological</a></li>
                    <li><a href="../../archives/index.html">Archives</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop2" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-info-sign"></i> About</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop2">
                    <li><a href="../../info.html">Editorial Information</a></li>
                    <li><a href="../../about.html">About the SEP</a></li>
                    <li><a href="../../board.html">Editorial Board</a></li>
                    <li><a href="../../cite.html">How to Cite the SEP</a></li>
                    <li><a href="../../special-characters.html">Special Characters</a></li>
                    <li><a href="../../tools/index.html">Advanced Tools</a></li>
                    <li><a href="../../contact.html">Contact</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop3" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-leaf"></i> Support SEP</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop3">
                    <li><a href="../../support/index.html">Support the SEP</a></li>
                    <li><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
                    <li><a href="../../support/donate.html">Make a Donation</a></li>
                    <li><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
                  </ul>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End navigation -->
    
    <div id="search">
      <form id="search-form" method="get" action="https://seop.illc.uva.nl/search/searcher.py">
        <input type="search" name="query" placeholder="Search SEP" />
        <div class="search-btn-wrapper"><button class="btn search-btn" type="submit"><i class="icon-search"></i></button></div>
      </form>
    </div>
    <!-- End search --> 
    
  </div>
  <!-- End header --> 
</div>
<!-- End header wrapper -->

<div id="content">

<!-- Begin article sidebar -->
<div id="article-sidebar" class="sticky">
  <div class="navbar">
    <div class="navbar-inner">
      <div class="container">
        <button class="btn btn-navbar" data-target=".collapse-sidebar" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Entry Navigation </button>
        <div id="article-nav" class="nav-collapse collapse-sidebar in collapse">
          <ul class="nav">
            <li><a href="#toc">Entry Contents</a></li>
            <li><a href="#Bib">Bibliography</a></li>
            <li><a href="#Aca">Academic Tools</a></li>
            <li><a href="https://leibniz.stanford.edu/friends/preview/epistemology-bayesian/">Friends PDF Preview <i class="icon-external-link"></i></a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=epistemology-bayesian">Author and Citation Info <i class="icon-external-link"></i></a> </li>
            <li><a href="#pagetopright" class="back-to-top">Back to Top <i class="icon-angle-up icon2x"></i></a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</div>
<!-- End article sidebar --> 

<!-- NOTE: Article content must have two wrapper divs: id="article" and id="article-content" -->
<div id="article">
<div id="article-content">

<!-- BEGIN ARTICLE HTML -->


<div id="aueditable"><!--DO NOT MODIFY THIS LINE AND ABOVE-->

<h1>Bayesian Epistemology</h1><div id="pubinfo"><em>First published Mon Jun 13, 2022</em></div>

<!--pdf exclude begin-->
<p class="smaller">
[<em>Editor&rsquo;s Note: The following new entry by Hanti Lin replaces the
 <a href="https://plato.stanford.edu/archives/spr2022/entries/epistemology-bayesian/">former entry</a>
on this topic by the previous author.</em>]
</p>
<!--pdf exclude end-->

<div id="preamble">

<p>
We can think of belief as an all-or-nothing affair. For example, I
believe that I am alive, and I don&rsquo;t believe that I am a
historian of the Mongol Empire. However, often we want to make
distinctions between <em>how strongly</em> we believe or disbelieve
something. I strongly believe that I am alive, am fairly confident that I
will stay alive until my next conference presentation, less confident
that the presentation will go well, and strongly disbelieve that its
topic will concern the rise and fall of the Mongol Empire. The idea
that beliefs can come in different strengths is a central idea behind
Bayesian epistemology. Such strengths are called <em>degrees of
belief</em>, or <em>credences</em>. Bayesian epistemologists study
norms governing degrees of beliefs, including how one&rsquo;s degrees of
belief ought to change in response to a varying body of evidence.
Bayesian epistemology has a long history. Some of its core ideas can
be identified in Bayes&rsquo; (1763) seminal paper in statistics
(Earman 1992: ch. 1), with applications that are now very influential
in many areas of philosophy and of science.</p>

<p>
The present entry focuses on the more traditional, general issues
about Bayesian epistemology, and, along the way, interested readers
will be referred to entries that discuss the more specific topics.
A tutorial on Bayesian epistemology will be provided in the first
section for beginners and those who want a quick overview.</p>
</div>

<div id="toc">
<!--Entry Contents-->
<ul>
<li><a href="#TutoBayeEpis">1. A Tutorial on Bayesian Epistemology</a>
   <ul>
   <li><a href="#CaseStud">1.1 A Case Study</a></li>
   <li><a href="#TwoCoreNorm">1.2 Two Core Norms</a></li>
   <li><a href="#Appl">1.3 Applications</a></li>
   <li><a href="#BayeDiviWhatDoesCoheRequ">1.4 Bayesians Divided: What Does Coherence Require?</a></li>
   <li><a href="#BayeDiviProbPrio">1.5 Bayesians Divided: The Problem of the Priors</a></li>
   <li><a href="#AtteFounDutcBookArgu">1.6 An Attempted Foundation: Dutch Book Arguments</a></li>
   <li><a href="#AlteFoun">1.7 Alternative Foundations</a></li>
   <li><a href="#ObjeCond">1.8 Objections to Conditionalization</a></li>
   <li><a href="#ObjeAbouIdea">1.9 Objections about Idealization</a></li>
   <li><a href="#ConcEncoNonBaye">1.10 Concerns, or Encouragements, from Non-Bayesians</a></li>
   </ul></li>
<li><a href="#BitMathForm">2. A Bit of Mathematical Formalism</a></li>
<li><a href="#SyncNormIRequCohe">3. Synchronic Norms (I): Requirements of Coherence</a>
   <ul>
   <li><a href="#VersProb">3.1 Versions of Probabilism</a></li>
   <li><a href="#CounAddi">3.2 Countable Additivity</a></li>
   <li><a href="#Regu">3.3 Regularity</a></li>
   <li><a href="#NormCondCred">3.4 Norms of Conditional Credences</a></li>
   <li><a href="#ChanCredPrin">3.5 Chance-Credence Principles</a></li>
   <li><a href="#ReflOtheDefePrin">3.6 Reflection and Other Deference Principles</a></li>
   </ul></li>
<li><a href="#SyncNormIIProbPrio">4. Synchronic Norms (II): The Problem of the Priors</a>
   <ul>
   <li><a href="#SubjBaye">4.1 Subjective Bayesianism</a></li>
   <li><a href="#ObjeBaye">4.2 Objective Bayesianism</a></li>
   <li><a href="#ForwLookBaye">4.3 Forward-Looking Bayesianism</a></li>
   <li><a href="#ConnUniqDeba">4.4 Connection to the Uniqueness Debate</a></li>
   </ul></li>
<li><a href="#IssuAbouDiacNorm">5. Issues about Diachronic Norms</a>
   <ul>
   <li><a href="#OldEvid">5.1 Old Evidence</a></li>
   <li><a href="#NewTheo">5.2 New Theory</a></li>
   <li><a href="#UnceLear">5.3 Uncertain Learning</a></li>
   <li><a href="#MemoLoss">5.4 Memory Loss</a></li>
   <li><a href="#SelfLocaCred">5.5 Self-Locating Credences</a></li>
   <li><a href="#BayeWithKine">5.6 Bayesianism without Kinematics</a></li>
   </ul></li>
<li><a href="#ProbIdea">6. The Problem of Idealization</a>
   <ul>
   <li><a href="#DeIdeaUnde">6.1 De-idealization and Understanding</a></li>
   <li><a href="#StriForIdea">6.2 Striving for Ideals</a></li>
   <li><a href="#ApplEmpoIdea">6.3 Applications Empowered by Idealization</a></li>
   </ul></li>
<li><a href="#ClosExpaTerrBaye">7. Closing: The Expanding Territory of Bayesianism</a></li>
<li><a href="#Bib">Bibliography</a></li>
<li><a href="#Aca">Academic Tools</a></li>
<li><a href="#Oth">Other Internet Resources</a></li>
<li><a href="#Rel">Related Entries</a></li>
</ul>

<!--Entry Contents-->
<hr />

</div>

<div id="main-text">

<h2 id="TutoBayeEpis">1. A Tutorial on Bayesian Epistemology</h2>

<p>
This section provides an introductory tutorial on Bayesian
epistemology, with references to subsequent sections or related
entries for details.</p>

<h3 id="CaseStud">1.1 A Case Study</h3>

<p>
For a glimpse of what Bayesian epistemology is, let&rsquo;s see what
Bayesians have to say about this episode in scientific inquiry:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Eddington&rsquo;s Observation).</em>
Einstein&rsquo;s theory of General Relativity entails that light can be
deflected by a massive body such as the Sun. This physical effect,
predicted by Einstein in a 1911 paper, was observed during a solar eclipse on May 29, 1919, from locations chosen from    
Eddington&rsquo;s two expeditions. 
This result surprised the physics community and was deemed a
significant confirmation of Einstein&rsquo;s theory.</li>
</ul>
</div>

<p>
The above case makes a general point:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">The Principle of Hypothetico-Deductive
Confirmation.</em> Suppose that a scientist is testing a hypothesis
<i>H</i>. She deduces from it an empirical consequence <i>E</i>, and
does an experiment, being not sure whether <i>E</i> is true. It turns
out that she obtains <i>E</i> as new evidence as a result of the
experiment. Then she ought to become more confident in <i>H</i>.
Moreover, the more surprising the evidence <i>E</i> is, the higher the
credence in <i>H</i> ought to be raised.</li>
</ul>
</div>

<p>
This intuition about how credences ought to change can be
vindicated in Bayesian epistemology by appeal to two norms. But before turning to them, we need
a setting. Divide the space of possibilities into four, according to whether
hypothesis <i>H</i> is true or false and whether evidence <i>E</i> is
true or false. Since <i>H</i> logically implies <i>E</i>, there are
only three distinct possibilities on the table, which are depicted as the three
dots in
 <a href="#fig-poss">figure 1</a>.</p>
 
<div class="figure" id="fig-poss">
<img src="poss.png" style="width:150pt" alt="a diagram: link to extended description below" />

<p class="center">
<span class="figlabel">Figure 1:</span> A Space of Three
Possibilities. [An
 <a href="figdesc.html#fig-poss">extended description of figure 1</a>
 is in the supplement.]</p>
</div>

<p>
Those possibilities are <em>mutually exclusive</em> in the sense that
no two of them can hold together; and they are <em>jointly
exhaustive</em> in the sense that at least one of them must hold. A
person can be more or less confident that a given possibility
holds. Suppose that it makes sense to say of a person that she
is, say, 80% confident that a certain possibility holds. In this case,
say that this person&rsquo;s degree of belief, or credence, in that
possibility is equal to 0.8. A credence might be any other real
number. (How to make sense of real-valued credences is a major topic
for Bayesians, to be discussed in
 <a href="#sec-foundation">&sect;1.6</a>
 and
 <a href="#sec-foundation-2">&sect;1.7</a>
 below.)</p>

<p>
Now I can sketch the two core norms in Bayesian epistemology.
According to the first norm, called <em>Probabilism</em>, one&rsquo;s
credences in the three possibilities in
 <a href="#fig-poss">figure 1</a>
 ought to fit together so nicely that they are non-negative and sum to
1. Such a distribution of credences can be represented by a bar chart,
as depicted on the left of
 <a href="#fig-condi-1">figure 2</a>.</p>
 
<div class="figure" id="fig-condi-1">
<img src="condi-1.png" style="width:300pt" alt="a diagram: link to extended description below" />

<p class="center">
<span class="figlabel">Figure 2:</span> Conditionalization on
Evidence. [An
 <a href="figdesc.html#fig-condi-1">extended description of figure 2</a>
 is in the supplement.]</p>
</div>

<p>
Now, suppose that a person with this credence distribution receives <i>E</i> as new evidence. It seems that as a result, there should be some change in credences. But how should they change? According to the second norm, called the 									<em>Principle of
Conditionalization</em>, the possibility incompatible with <i>E</i>
(i.e., the rightmost possibility) should have its credence dropped
down to 0, and to satisfy Probabilism, the remaining credences should be scaled
up&mdash;rescaled to sum to 1. So this person&rsquo;s credence in
hypothesis <i>H</i> has to rise in a way such as that depicted in
 <a href="#fig-condi-1">figure 2</a>. </p>
 
<p>
Moreover, suppose that new evidence <i>E</i> is very surprising. It means
that the person starts out being highly confident in the falsity of
<i>E</i>, as depicted on the left of
 <a href="#fig-condi-2">figure 3</a>.</p>
 
<div class="figure" id="fig-condi-2">
<img src="condi-2.png" style="width:300pt" alt="a diagram: link to extended description below" />

<p class="center">
<span class="figlabel">Figure 3:</span> Conditionalization on
Surprising Evidence. [An
 <a href="figdesc.html#fig-condi-2">extended description of figure 3</a>
 is in the supplement.]</p>
</div>

<p>
Then conditionalization on <i>E</i> requires a total credence collapse 
followed by a dramatic scaling-up of the other credences. In
particular, the credence in <i>H</i> is raised significantly, unless
it is zero to begin with. This vindicates the intuition reported in
the case of Eddington&rsquo;s Observation.</p>

<div id="sec-core">

<h3 id="TwoCoreNorm">1.2 Two Core Norms</h3>

<p>
The two Bayesian norms sketched above can be stated a bit more
generally as follows. (A formal statement will be provided after this
tutorial, in
 <a href="#sec-math">section 2</a>.)
 Suppose that there are some possibilities under consideration, which
are mutually exclusive and jointly exhaustive. A proposition under consideration is one that is true or false in each of those possibilities, so it can be identified with the set of the possibilities in which it is true. When those possibilities are finite in number, and when you have credences in all of them,
Probabilism takes a simple form, saying that your credences
ought to be probabilistic in this sense:</p>

<div class="indent">

<ul class="hanging">

<li>(<em class="sc">Non-Negativity</em>) The credences assigned to the
possibilities under consideration are non-negative real numbers.</li>

<li>(<em class="sc">Sum-to-One</em>) The credences assigned to the
possibilities under consideration sum to 1.</li>

<li>(<em class="sc">Additivity</em>) The credence assigned to a
proposition under consideration is equal to the sum of the credences assigned to the
possibilities in that proposition.</li>
</ul>

</div>

<p>
While this norm is <em>synchronic</em> in that it constrains your
credences at each time, the next norm is <em>diachronic</em>. Suppose
that you just received a piece of evidence <i>E</i>, which is true in
at least some possibilities under consideration. Suppose further that
<i>E</i> exhausts all the evidence you just received. Then the
Principle of Conditionalization says that your credences ought to
change as if you followed the procedure below (although it is possible
to design other procedures to the same effect):</p>

<div class="indent">

<ul class="hanging">

<li>(<em class="sc">Zeroing</em>) For each possibility incompatible
with evidence <i>E</i>, drop its credence down to zero.</li>

<li>(<em class="sc">Rescaling</em>) For the possibilities compatible
with evidence <i>E</i>, rescale their credences by a common factor to
make them sum to 1.</li>

<li>(<em class="sc">Resetting</em>) Now that there is a new credence
distribution over the individual possibilities, reset the credences in
propositions according to the Additivity rule in Probabilism.</li>
</ul>
</div>

<p>
The second step, rescaling, deserves attention. It is designed to
ensure compliance with Probabilism, but it also has an independent,
intuitive appeal. Consider any two possibilities in which new evidence
<i>E</i> is true. Thus the new evidence alone cannot distinguish those
two possibilities and, hence, it seems to favor the two equally. So it
seems that, if a person starts out being twice as confident in one of
those two possibilities as in the other, she should remain so after
the credence change in light of <i>E</i>, as required by the rescaling
step. The essence of conditionalization is preservation of certain
ratios of credences, which is a feature inherited by generalizations
of conditionalization (see
 <a href="#sec-condi">section 5</a>
 for details).</p>

<p>
So there you have it: Probabilism and the Principle of
Conditionalization, which are held by most Bayesians to be the two
core norms in Bayesian epistemology.</p>
</div>

<div id="sec-app">

<h3 id="Appl">1.3 Applications</h3>

<p>
Bayesian epistemology features an ambition: to develop a simple
normative framework that consists of little or nothing more than the
two core Bayesian norms, with the goal of explaining or justifying a wide
range of intuitively good epistemic practices and perhaps also of
guiding our inquiries, all done with a focus on credence change. That
sounds quite ambitious, given the narrow focus on credence
change. But many Bayesians maintain that credence change is a unifying
theme that underlies many different aspects of our epistemic
endeavors. Let me mention some examples below.</p>

<p>
First of all, it seems that a hypothesis <i>H</i> is
<em>confirmed</em> by new evidence <i>E</i> exactly when one&rsquo;s
credence in <i>H</i> ought to increase in response to the acquisition
of <i>E</i>. Extending that idea, it also seems that <em>how much</em>
<i>H</i> is confirmed correlates with how much its credence ought to
be raised. With those ideas in mind, Bayesians have developed several
accounts of confirmation; see
 <a href="../confirmation/index.html#BayConThe">section 3 of the entry on confirmation</a>.
 Through the concept of confirmation, some Bayesians have also
developed accounts of closely related concepts. For example, being
<em>supported by evidence</em> seems to be the same as or similar to
being confirmed by evidence, which is ultimately explained by
Bayesians in terms of credence change. So there are some Bayesian
accounts of evidential support; see
 <a href="../bayes-theorem/index.html#3">section 3 of the entry on Bayes&rsquo; theorem</a>
 and
 <a href="../imprecise-probabilities/index.html#WeiEviBalEvi">sections 2.3&ndash;2.5 of the entry on imprecise probabilities</a>.
 Here is another example: <em>how well</em> a theory <em>explains</em>
a body of evidence seems to be closely related to how well the theory
is confirmed by the evidence, which is ultimately explained by
Bayesians in terms of credence change. So there are some Bayesian
accounts of explanatory power; see
 <a href="../abduction/index.html#ExpAbd">section 2 of the entry on abduction</a>.</p>

<p>
The focus on credence change also sheds light on another aspect of our
epistemic practices: inductive inference. An inductive inference is
often understood as a process that results in the formation of an
all-or-nothing attitude: believing or accepting the truth of a
hypothesis <i>H</i> on the basis of one&rsquo;s evidence <i>E</i>.
That does not appear to fit the Bayesian picture well. But to
Bayesians, what really matters is how new evidence <i>E</i> ought to
change one&rsquo;s credence in <i>H</i>&mdash;whether one&rsquo;s
credence ought to be <em>raised</em> or <em>lowered</em>, and by
<em>how much</em>. To be sure, there is the issue of whether the
resulting credence would be high enough to warrant the formation of
the attitude of believing or accepting. But to many Bayesians, that
issue seems only secondary, or better forgone as argued by Jeffrey
(1970). If so, the fundamental issue about inductive inference is
ultimately how credences ought to change in light of new evidence. So
Bayesians have had much to say about various kinds of inductive
inferences and related classic problems in philosophy of science. See
the following footnote for a long list of relevant survey articles (or
research papers, in cases where survey articles are not yet
 available).<sup>[<a href="notes.html#note-1" id="ref-1">1</a>]</sup></p>
 
<p>
For monographs on applications in epistemology and philosophy of
science, see Earman (1992), Bovens &amp; Hartmann (2004), Howson &amp;
Urbach (2006), and Sprenger &amp; Hartmann (2019). In fact, there are
also applications to natural language semantics and pragmatics: for
indicative conditionals, see the survey by Briggs (2019: sec. 6 and 7)
and sections 3 and 4.2 of the entry on
 <a href="../conditionals/index.html">indicative conditionals</a>;
 for epistemic modals, see Yalcin (2012).</p>

<p>
The applications mentioned above rely on the assumption of some or
other norms for credences. Although the correct norms are held by most
Bayesians to include at least Probabilism and the Principle of
Conditionalization, it is debated whether there are more and, if so,
what they are. It is to this issue that I now turn.</p>
</div>

<div id="sec-intro-coh">

<h3 id="BayeDiviWhatDoesCoheRequ">1.4 Bayesians Divided: What Does Coherence Require?</h3>

<p>
Probabilism is often regarded as a <em>coherence norm</em>, which says
how one&rsquo;s opinions ought to fit together on pain of incoherence.
So, if Probabilism matters, the reason seems to be that coherence
matters. This raises a question that divides Bayesians: <em>What does
the coherence of credences require?</em> A typical Bayesian thinks
that coherence requires at least that one&rsquo;s credences follow
Probabilism. But there are actually different versions of Probabilism
and Bayesians disagree about which one is correct. Bayesians also
disagree about whether the coherence of credences requires more than Probabilism and, if
so, to what extent. For example, does coherence require that
one&rsquo;s credence in a <em>contingent</em> proposition lie strictly
between 0 and 1? Another issue is what coherence requires of
conditional credences, i.e., the credences that one has on the
supposition of the truth of one or another proposition. Those and
other related questions have far-reaching impacts on applications of
Bayesian epistemology. For more on the issue of what coherence
requires, see
 <a href="#sec-sync-coh">section 3</a>.</p>
 </div>

<div id="sec-intro-prior">

<h3 id="BayeDiviProbPrio">1.5 Bayesians Divided: The Problem of the Priors</h3>

<p>
There is another issue that divides Bayesians. The package of
Probabilism and the Principle of Conditionalization seems to explain
well why one&rsquo;s credence in General Relativity ought to rise in
Eddington&rsquo;s Observation Case. But that particular Bayesian
explanation relies on a crucial feature of the case: the evidence
<i>E</i> is <em>entailed</em> by the hypothesis <i>H</i> in question.
But such an entailment is missing in many interesting cases, such as
this one:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Enumerative Induction).</em> After a day
of field research, we observed one hundred black ravens without a
counterexample. So the newly acquired evidence is <i>E</i> = &ldquo;we
have observed one hundred ravens and they all were black&rdquo;. We
are interested in this hypothesis <i>H</i> = &ldquo;the next raven to
be observed will be black&rdquo;.</li>
</ul>
</div>

<p>
Now, should the credence in the hypothesis be increased or lowered,
according to the two core Bayesian norms? Well, it depends. Note that
in the present case <i>H</i> entails neither <i>E</i> nor its
negation, so the possibilities in <i>H</i> can be categorized into two
groups: those compatible with <i>E</i>, and those incompatible with <i>E</i>. As a
result of conditionalization, the possibilities incompatible with
<i>E</i> will have their credences be dropped down to zero; those
compatible, scaled up. If the scaling up outweighs the dropping down
for the possibilities inside <i>H</i>, the credence in <i>H</i> will
rise and thus behave inductively; otherwise, it will stay constant or
even go down and thus behave counter-inductively. So it all depends on
the specific details of the <em>prior</em>, which is shorthand for the
assignment of credences that one has before one acquires the new
evidence in question. To sum up: Probabilism and the Principle of Conditionalization, alone, are too weak to entitle us to say whether
one&rsquo;s credence ought to change inductively or
counter-inductively in the above example.</p>

<p>
This point just made generalizes to
most applications of Bayesian epistemology. For example, some coherent
priors lead to enumerative induction and some don&rsquo;t (Carnap
1955), and some coherent priors lead to Ockham&rsquo;s razor and some
don&rsquo;t (Forster 1995: sec. 3). So, besides the coherence norms
(such as Probabilism), are there any other norms that govern
one&rsquo;s prior? This is known as <em>the problem of the
priors</em>.</p>

<p>
This issue divides Bayesians. First of all, there is the party of
<em>subjective Bayesians</em>, who hold that every prior is permitted
unless it fails to be coherent. So, to those Bayesians, the correct
norms for priors are exhausted by Probabilism and the other coherence
norms if any. Second, there is the party of <em>objective
Bayesians</em>, who propose that the correct norms for priors include
not just the coherence norms but also a norm that codifies the
epistemic virtue of freedom from bias. Those Bayesians think that
freedom from bias requires at least that, roughly speaking,
one&rsquo;s credences be evenly distributed to certain possibilities
unless there is a reason not to. This norm, known as <em>the Principle
of Indifference</em>, has long been a source of controversy. Last but
not the least, some Bayesians even propose to take seriously certain
epistemic virtues that have been extensively studied in other
epistemological traditions, and argue that those virtues need to be
codified into norms for priors. For more on those attempted solutions
to the problem of the priors, see
 <a href="#sec-prior">section 4</a>
 below. Also see
 <a href="../probability-interpret/index.html#SubPro">section 3.3 of the entry on interpretations of probability</a>.</p>
 

<p>
So far I have been mostly taking for granted the package of
Probabilism and the Principle of Conditionalization. But is there any
good reason to accept those two norms? This is the next topic.</p>
</div>

<div id="sec-foundation">

<h3 id="AtteFounDutcBookArgu">1.6 An Attempted Foundation: Dutch Book Arguments</h3>

<p>
There have been a number of arguments advanced in support of the two core Bayesian norms.
Perhaps the most influential is of the kind
called <em>Dutch Book arguments</em>. Dutch Book arguments are motivated by a simple, intuitive idea: Belief
guides action. So, the more strongly you believe that it will rain
tomorrow, the more inclined you are, or ought to be, to bet on bad
weather. This idea, which connects degrees of belief to betting
dispositions, can be captured at least partially by the following:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">A Credence-Betting Bridge Principle (Toy
Version).</em> If one&rsquo;s credence in a proposition <i>A</i> is
equal to a real number <i>a</i>, then it is acceptable for one to buy
the bet &ldquo;Win $100 if <i>A</i> is true&rdquo; at the price
\(\$100 \cdot a\) (and at any lower price).</li>
</ul>

</div>

<p>
This bridge principle might be construed as part of a definition or as a 
necessary truth that captures the nature of credences, or understood
as a norm that jointly constrains credences and betting dispositions
(Christensen 1996; Pettigrew 2020a: sec. 3.1). The hope is that, through this bridge
principle or perhaps a refined one, bad credences generate bad symptoms in
betting dispositions. If so, a close look at betting dispositions
might help us sort out bad credences from good ones. This is the
strategy that underlies Dutch Book arguments.</p>

<p>
To illustrate, consider an agent who has a .75 credence in proposition
<i>A</i> and a .30 credence in its negation \(\neg A\) (which violates
Probabilism). Assuming the bridge principle stated above, the agent is
willing to bet as follows:</p>

<ul>

<li>Buy &ldquo;win $100 if <i>A</i> is true&rdquo; at \(\$75\).</li>

<li>Buy &ldquo;win $100 if \(\neg A\) is true&rdquo; at \(\$30\).</li>
</ul>

<p>
So the agent is willing to accept <em>each</em> of those two offers.
But it is actually very bad to accept <em>both</em> at the same time,
for that leads to a sure loss (of $5):</p>

<div>

<table class="centered cellpad-med-dense hrulesTH">
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: left;"><i>A</i> is true</th>
<th style="text-align: left;"><i>A</i> is false</th> </tr> </thead>
<tbody>
<tr class="odd">
<th style="text-align: right;">buy &ldquo;win $100 if <i>A</i> is
true&rdquo; at $75</th>
  <td style="text-align: left;">\(-\$75 + \$100\)</td>
  <td style="text-align: left;">\(-\$75\)</td> </tr>
<tr class="even">
<th style="text-align: right;">buy &ldquo;win $100 if \(\neg A\) is
true&rdquo; at $30</th>
  <td style="text-align: left;">\(-\$30\)</td>
  <td style="text-align: left;">\(-\$30 + \$100\)</td> </tr>
<tr class="odd">
<th style="text-align: right;">net payoff</th>
  <td style="text-align: left;">\(-\$5\)</td>
  <td style="text-align: left;">\(-\$5\)</td> </tr> </tbody>
</table>

</div>

<p>
So this agent&rsquo;s betting dispositions make her susceptible to a
set of bets that are individually acceptable but jointly inflict a
sure loss. Such a set of bets is called a <em>Dutch Book</em>. The
above agent is susceptible to a Dutch Book, which sounds bad for the
agent. So what has gone wrong? The problem seems to be this: Belief
guides action, and in this case, bad beliefs result in bad actions: garbage in, garbage out. Therefore, the agent should
not have had the combination of credence .75 in \(A\) and .30 in
\(\neg A\) to begin with&mdash;or so a Dutch Book argument would
conclude.</p>

<p>
The above line of thought can be generalized and turned into a
template for Dutch Book arguments:</p>

<div class="indent">

<p>
<em class="sc">A Template for Dutch Book Arguments</em></p>

<ul class="hanging">

<li>Premise 1. You should follow such and such a credence-betting
bridge principle (or, due to the nature of credences, you do so
necessarily).</li>

<li>Premise 2. If you do, and if your credences violate constraint
<i>C</i>, then provably you are susceptible to a Dutch Book.</li>

<li>Premise 3. But you should not be so susceptible.</li>

<li>Conclusion. So your credences should satisfy constraint
<i>C</i>.</li>
</ul>

</div>

<p>
There is a Dutch Book argument for Probabilism (Ramsey 1926, de
Finetti 1937). The idea can be extended to develop an argument for the
Principle of Conditionalization (Lewis 1999, Teller 1973). Dutch Book
arguments have also been developed for other norms for credences, but
they require modifying the concept of a Dutch Book in one way or
another. See
 <a href="#sec-sync-coh">section 3</a>
 for references.</p>

<p>
An immediate worry about Dutch Book arguments is that a higher
credence might not be correlated with a stronger disposition to bet.
Consider a person who loathes very much the anxiety caused by placing
a bet. So, though she is very confident in a proposition, she might
still refuse to buy a bet on its truth even at a low price&mdash;and
rightly so. This seems to be a counterexample to premise 1 in the
above. For more on Dutch Book arguments, including objections to them as well as refinements of them, see the survey by H&aacute;jek (2009) and the entry on
 <a href="../dutch-book/index.html">Dutch Book arguments</a>.</p>
 
<p>
There is a notable worry that applies even if we have a Dutch Book
argument that is logically valid and only has true premises. A Dutch
Book argument seems to give only a <em>practical</em> reason for
accepting an <em>epistemic</em> norm: &ldquo;Don&rsquo;t have such and
such combinations of credences, for otherwise there would be something
bad pragmatically&rdquo;. Such a reason seems unsatisfactory for those
who wish to explain the correctness of the Bayesian norms with a
reason that is distinctively epistemic or at least non-pragmatic.
Some Bayesians still think that Dutch Book arguments are good, and address the present worry by trying to give 
a non-pragmatic reformulation of Dutch Book arguments (Christensen
1996; Christensen 2004: sec. 5.3). Some other Bayesians abandon Dutch Book arguments and pursue alternative
foundations of Bayesian epistemology, to which I turn now.</p>
</div>

<div id="sec-foundation-2">

<h3 id="AlteFoun">1.7 Alternative Foundations</h3>

<p>
A second proposed type of foundation for Bayesian epistemology is based on the
idea of <em>accurate estimation</em>. This idea has two parts:
estimation, and its accuracy. On this approach, one&rsquo;s credence
in a proposition <i>A</i> is one&rsquo;s <em>estimate</em> of the
truth value of <i>A</i>, where <i>A</i>&rsquo;s truth value is
identified with 1 if it is true and 0 if it is false (Jeffrey 1986).
The closer one&rsquo;s credence in <i>A</i> is to the truth value of
<i>A</i>, the <em>more accurate</em> one&rsquo;s estimate is. Then a
Bayesian may argue that one&rsquo;s credences ought to be
probabilistic, for otherwise the overall accuracy of one&rsquo;s
credence assignment would be <em>dominated</em>&mdash;namely, it
would, come what may, be lower than the overall accuracy of another
credence assignment that one could have adopted. To some Bayesians,
this gives a distinctively epistemic reason or explanation why
one&rsquo;s credences ought to be probabilistic. The result is the
so-called <em>accuracy-dominance</em> argument for Probabilism (Joyce
1998). This approach has also been extended to argue for the Principle
of Conditionalization (Briggs &amp; Pettigrew 2020). For more on this
approach, see the entry on
 <a href="../epistemic-utility/index.html">epistemic utility arguments for probabilism</a>
 as well as Pettigrew (2016).</p>

<p>
There is a third proposed type of foundation for Bayesian epistemology. It
appeals to a kind of doxastic state called <em>comparative
probability</em>, which concerns a person&rsquo;s taking one
proposition to be <em>more probable than</em>, or <em>as probable
as</em>, or <em>less probable than</em> another proposition. On this
approach, we postulate some bridge principles that connect one&rsquo;s
credences to one&rsquo;s comparative probabilities. Here
is an example of such a bridge principle: for any propositions
<i>X</i> and <i>Y</i>, if <i>X</i> is equivalent to the disjunction of
<em>two</em> incompatible propositions, each of which one takes to be
more probable than <i>Y</i>, then one&rsquo;s credence in <i>X</i>
should be more than <em>twice</em> of that in <i>Y</i>. With such
bridge principles, a Bayesian may argue from norms for comparative
probabilities to norms for credences, such as Probabilism. See Fishburn
(1986) for the historical development of this approach. See
Stef&aacute;nsson (2017) for a recent defense and development. For a
general survey of this approach, see Konek (2019). This approach has
been extended by Joyce (2003: sec. 4) to justify the Principle of
Conditionalization.</p>

<p>
The above are just some of the attempts to provide foundations for Bayesian
epistemology. For more, see the surveys by Weisberg (2011: sec. 4) and
Easwaran (2011).</p>

<p>
There is a distinctive class of worries for all the three proposed
foundations presented above, due to the fact that they rely on one or
another account of the nature of credences. This is where
Bayesian epistemology meets philosophy of mind. Recall that they try to
understand credences in relation to some other mental states: (i)
betting dispositions, (ii) estimates of truth values, or (iii)
comparative probabilities. But those accounts of credences are
apparently vulnerable to counterexamples. (An example was mentioned
above: a person who dislikes the anxiety caused by betting seems to be
a counterexample to the betting account of credences).  For more on such
worries, see Eriksson and H&aacute;jek (2007). For more on accounts of
credences, see
 <a href="../probability-interpret/index.html#SubPro">section 3.3 of the entry on interpretations of probability</a>
 and
 <a href="../imprecise-probabilities/index.html#IntIP">section 3.4 of the entry on imprecise probabilities</a>.</p>
 

<p>
There is a fourth, <em>application-driven</em> style of argument for
norms for credences that seems to be explicit or implicit in the minds
of many Bayesians. The idea is that a good argument for the two core
Bayesian norms can be obtained by appealing to applications. The
goal is to account for a <em>comprehensive</em> range of intuitively
good epistemic practices, all done with a <em>simple</em> set of
general norms consisting of little or nothing more than the two core
Bayesian norms. If this Bayesian normative system is so good that, of
the known competitors, it strikes the best balance of those two
virtues just mentioned&mdash;comprehensiveness and
simplicity&mdash;then <em>that</em> is a good reason for accepting the
two core Bayesian norms. In fact, the method just described is
applicable to any norm, for credences or for actions, in epistemology or
in ethics. Some philosophers argue that this method in its full
generality, called <em>Reflective Equilibrium</em>, is the ultimate
method for finding a good reason for or against norms (Goodman 1955;
Rawls 1971). For more on this method and its controversies, see the
entry on
 <a href="../reflective-equilibrium/index.html">reflective equilibrium</a>.</p>
 

<p>
The above are some ways to argue for Bayesian norms. The rest of this
introductory tutorial is meant to sketch some general objections,
leaving detailed discussions to subsequent sections.</p>
</div>

<div id="sec-intro-old">

<h3 id="ObjeCond">1.8 Objections to Conditionalization</h3>

<p>
The Principle of Conditionalization requires one to react to new
evidence by conditionalizing on it. So this principle, when construed
literally, appears to be silent on the case in which one receives
<em>no</em> new evidence. That is, it seems to be too weak to require
that one shouldn&rsquo;t arbitrarily change credences when there is no
new evidence. To remedy this, the Principle of Conditionalization is
usually understood such that the case of no new evidence is identified
with the limiting case in which one acquires a logical truth as 
trivial new evidence, which rules out no possibilities. In that case,
conditionalization on the trivial new evidence lowers no credences,
and thus rescales credences only by a factor of 1&mdash;no credence
change at all&mdash;as desired. Once the Principle of Conditionalization is
construed that way, it is no longer too weak, but then the worry is
that it becomes too strong. Consider the following case, which Earman
(1992) adapts from Glymour (1980):</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Mercury).</em> It is 1915. Einstein has
just developed a new theory, General Relativity. He assesses the new
theory with respect to some old data that have been known for at
least fifty years: the anomalous rate of the advance of
Mercury&rsquo;s perihelion (which is the point on Mercury&rsquo;s
orbit that is closest to the Sun). After some derivations and
calculations, Einstein soon recognizes that his new theory entails the
old data about the advance of Mercury&rsquo;s perihelion, while the
Newtonian theory does not. Now, Einstein increases his credence in his
new theory, and rightly so.</li>
</ul>

</div>

<p>
Note that, during his derivation and calculation, Einstein does not
perform any experiment or collect any new astronomical data, so the body
of his evidence seems to remain unchanged, only consisting of the old
data. Despite gaining no new evidence, Einstein changes (in fact,
raises) his credence in the new theory, and rightly
so&mdash;against the usual construal of the Principle of
Conditionalization. Therefore, there is a dilemma for that principle:
when construed literally, it is too weak to prohibit arbitrary
credence change; when construed in the usual way, it is too strong to
accommodate Einstein&rsquo;s credence change in the Mercury Case. This
problem is <em>Earman&rsquo;s problem of old evidence</em>.</p>

<p>
The problem of old evidence is sometimes presented in a different
way&mdash;in Glymour&rsquo;s (1980) way&mdash;whose target of attack
is not the Principle of Conditionalization but this:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Bayesian Confirmation Theory (A Simple
Version).</em> Evidence <i>E</i> confirms hypothesis <i>H</i> for a
person at a time if and only if, at that time, her credence in <i>H</i> would be
raised if she were to conditionalize on <i>E</i> (whether or not she
actually does that).</li>
</ul>

</div>

<p>
If <i>E</i> is an old piece of evidence that a person had received before,
this person&rsquo;s credence in <i>E</i> is currently 1. So,
conditionalization on <i>E</i> at the present time would involve
dropping no credence, followed by rescaling credences with a factor of
1&mdash;so there is no credence change at all. Then, by the Bayesian account of
confirmation stated above, old evidence <i>E</i> must fail to confirm
new theory <i>H</i>. But that result seems to be wrong because the old
data about the advance of Mercury&rsquo;s perihelion confirmed
Einstein&rsquo;s new theory; this is <em>Glymour&rsquo;s problem of
old evidence</em>, construed as a challenge to a Bayesian account of confirmation. But, if Earman (1992) is right, the Mercury
Case challenges not just Bayesian confirmation theory, but actually cuts deeper, all the way to
one of the two core Bayesian norms&mdash;namely, the Principle of
Conditionalization&mdash;as suggested by Earman&rsquo;s problem of old
evidence. For attempted solutions to Earman&rsquo;s old evidence
problem (about conditionalization), see
 <a href="#sec-old-evi">section 5.1</a>
 below. For more on Glymour&rsquo;s old evidence problem (about
confirmation), see
 <a href="../confirmation/index.html#NewEviOldEviTotEvi">section 3.5 of the entry on confirmation</a>.</p>

<p>
The above is just the beginning of a series of problems for the
Principle of Conditionalization, which will be discussed after this
tutorial, in
 <a href="#sec-condi">section 5</a>.
 But here is a rough sketch: The problem of old evidence arises when a
new theory is developed to accommodate some old evidence. When the
focus is shifted from old evidence to new theory, we shall discover 
another problem, no less thorny. Also note that the problem of old
evidence results from a kind of inflexibility in conditionalization:
no credence change is permitted without new evidence. Additional problems have been
directed at other kinds of inflexibility in conditionalization, such
as the preservation of fully certain credences. In response, some
Bayesians defend the Principle of Conditionalization by trying to
develop it into better versions, as you will see in
 <a href="#sec-condi">section 5</a>.</p>
 </div>

<div id="sec-intro-ideal">

<h3 id="ObjeAbouIdea">1.9 Objections about Idealization</h3>

<p>
Another worry is that the two core Bayesian norms are not the kind of
norms that we ought to follow, in that they are too demanding to be
actually followed by ordinary human beings&mdash;after all,
<em>ought</em> implies <em>can</em>. More specifically, those Bayesian
norms are often thought to be too demanding for at least three
reasons:</p>

<ol>

<li>(<em class="sc">Sharpness</em>) Probabilism demands that one&rsquo;s
credence in a proposition be extremely sharp, as sharp as an
individual real number, precise to potentially infinitely many digits.</li>

<li>(<em class="sc">Perfect Fit</em>) Probabilism demands that
one&rsquo;s credences fit together nicely; for example, some credences
are required to sum to exactly 1, no more and no less&mdash;a perfect
fit. The Principle of Conditionalization also demands a perfect fit
among three things: prior credences, posterior credences, and new
evidence.</li>

<li>(<em class="sc">Logical Omniscience</em>) Probabilism is often
thought to demand that one be <em>logically omniscient</em>, having
credence 1 in every logical truth and credence 0 in every logical
falsehood.</li>
</ol>

<p>
The last point, logical omniscience, might not be immediately clear
from the preceding presentation, but it can be seen from this
observation: A logical truth is true in all possibilities, so it has
to be assigned credence 1 by Sum-to-One and Additivity in
Probabilism.</p>

<p>
So the worry is that, although Bayesians have a simple normative
framework, they seem to enjoy the simplicity because they idealize
away from the complications in humans&rsquo; epistemic endeavors and
turn instead to normative standards that can be met only by highly
idealized agents. If so, there are pervasive counterexamples to the
two core Bayesian norms: all human beings. Call this <em>the problem
of idealization</em>. For different ways of presenting this problem,
see Harman (1986: ch. 3), Foley (1992: sec. 4.4), Pollock (2006: ch.
6), and Horgan (2017).</p>

<p>
In reply, Bayesians have developed at least three strategies, which
might complement each other. The first strategy is to
<em>remove</em> idealization gradually, one step at a time, and
explain why this is a good way of doing epistemology&mdash;just like
this has long been taken as a good way of doing science. The second
strategy is to explain why it makes sense for we human beings to
<em>strive for</em> some ideals, including the ideals that the two
core Bayesian norms point to, even though human beings cannot attain
those ideals. The third strategy is to explain how the kind of
idealization in question actually <em>empowers and facilitates</em>
the applications of Bayesian epistemology in science (including
especially scientists&rsquo; use of Bayesian statistics). For more on
those replies to the problem of idealization, see
 <a href="#sec-ideal">section 6</a>.</p>
 </div>

<h3 id="ConcEncoNonBaye">1.10 Concerns, or Encouragements, from Non-Bayesians</h3>

<p>
In the eyes of those immersed in the epistemology of all-or-nothing
opinions such as believing or accepting propositions, Bayesians seem
to say and care too little about many important and traditional
issues. Let me give some examples below.</p>

<p>
First of all, the more traditional epistemologists would like to see
Bayesians engage with varieties of skepticism. For example, there is
<em>Cartesian</em> skepticism, which is the view that we cannot know
whether an external world, as we understand it through our
perceptions, exists. There is also the <em>Pyrrhonian</em> skeptical worry
that no belief can ever be justified because, once a belief is to be
justified with a reason, the adduced reason is in need of
justification as well, which kickstarts an infinite regress of
justifications that can never be finished. Note that the above
skeptical views are expressed in terms of knowledge and justification.
So, the more traditional epistemologists would also like to hear what
Bayesians have to say about <em>knowledge</em> and
<em>justification</em>, rather than just norms for credences.</p>

<p>
Second, the more traditional philosophers of science would like to see
Bayesians contribute to some classic debates, such as the one between
scientific realism and anti-realism. <em>Scientific realism</em> is,
roughly, the view that we have good reason to believe that our best
scientific theories are true, literally or approximately. But the
anti-realists disagree. Some of them, such as the
<em>instrumentalists</em>, think that we only have good reason to
believe that our best scientific theories are good tools for certain
purposes. Bayesians often compare the credences assigned to competing
scientific theories, but one might like to see a comparison between,
on the one hand, the credence that a certain theory <i>T</i> is true
and, on the other hand, the credence that <i>T</i> is a good tool for
such and such purposes.</p>

<p>
Last but not least, frequentists about statistical inference would
urge that Bayesians also think about a certain epistemic virtue,
<em>reliability</em>, rather than focus exclusively on coherence.
Namely, they would like to see Bayesians take seriously the analysis
and design of reliable inference methods&mdash;reliable in the sense
of having a low objective, physical chance of making errors.</p>

<p>
To be sure, Bayesian epistemology was not initially designed to
address the concerns just expressed. But those concerns need not be taken as
objections, but rather as encouragements to Bayesians to explore new territories.
 In fact, Bayesians have begun such explorations in some
of their more recent works, as you will see in the
 <a href="#sec-closing">closing section, 7</a>.</p>
 
<p>
The above finishes the introductory tutorial on Bayesian epistemology.
The following sections, as well as many other encyclopedia entries cited above,
elaborate on one or another more specific topic in Bayesian
epistemology. Indeed, the above tutorial only shows you what topics
there are and aims to help you jump to the sections below, or to the relevant entries,  
that interest you.</p>

<div id="sec-math">

<h2 id="BitMathForm">2. A Bit of Mathematical Formalism</h2>

<p>
To facilitate subsequent discussions, a bit of mathematical formalism
is needed. Indeed, the two core Bayesian norms were only stated above
in a simple, finite setting
 (<a href="#sec-core">section 1.2</a>),
 but there can be an infinity of possibilities under consideration.
For example, think about this question: What&rsquo;s the objective,
physical chance for a carbon-14 atom to decay in 20 years? Every
possible chance in the unit interval \([0, 1]\) is a possibility to
which a credence can be assigned. So the two core Bayesian norms need
to be stated in a more general way than above.</p>

<p>
Let \(\Omega\) be a set of possibilities that are mutually exclusive
and jointly exhaustive. There is no restriction on the size of
\(\Omega\); it can be finite or infinite. Let \(\cal A\) be a set of
propositions identified with some subsets of \(\Omega\). Assume that
\(\cal A\) contains \(\Omega\) and the empty set \(\varnothing\), and
is closed under the standard Boolean operations: conjunction
(intersection), disjunction (union), and negation (complement). This
closure assumption means that, whenever \(A\) and \(B\) are in \(\cal
A\), so are their intersection \(A \cap B\), union \(A \cup B\), and
complement \(\Omega \mcomplement A\), which are often written in 
logical notation as conjunction \(A \wedge B\), disjunction \(A \vee
B\), and negation \(\neg A\). When \(\cal A\) satisfies the assumption
just stated, it is called an <em>algebra</em> of
 sets/propositions.<sup>[<a href="notes.html#note-2" id="ref-2">2</a>]</sup></p>
 
<p>
Let \(\Cr\) be an assignment of credences to some propositions. We
will often think of \(\Cr(A)\) as denoting one&rsquo;s credence in
proposition \(A\) and refer to \(\Cr\) as one&rsquo;s <em>credence
function</em> or <em>credence assignment</em>. Next, we need a
definition from probability theory:</p>

<div class="indent">

<ul class="hanging">

<li>

<p>
<em class="sc">Definition (Probability Measure).</em> A credence
function \(\Cr(\wcdot)\) is said to be <em>probabilistic</em>, also
called a <em>probability measure</em>, if it is a real-valued function
defined on an algebra \({\cal A}\) of propositions and satisfies the
three axioms of probability:</p>

<ul class="subhang">

<li>(<em class="sc">Non-Negativity</em>) \(\Cr(A) \ge 0\) for every
\(A\) in \(\cal A\).</li>

<li>(<em class="sc">Normalization</em>) \(\Cr(\Omega) = 1\).</li>

<li>(<em class="sc">Finite Additivity</em>) \(\Cr(A \cup B) = \Cr(A) +
\Cr(B)\) for any two incompatible propositions (i.e., disjoint sets)
\(A\) and \(B\) in \(\cal A\).</li>
</ul></li>
</ul>

</div>

<p>
Now Probabilism can be stated as follows:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Probabilism (Standard Version).</em> One&rsquo;s
assignment of credences at each time ought to be a probability
measure.</li>
</ul>
</div>

<p>
When it is clear from the context that the credence assignment \(\Cr\)
is assumed to be probabilistic, it is often written \(\Pr\) or \(P\).
The process of conditionalization can be defined as follows:</p>

<div class="indent">

<ul class="hanging">

<li>

<p>
<em class="sc">Definition (Conditionalization).</em> Suppose that
\(\Cr(E) \neq 0\). A (new) credence function \(\Cr'(\wcdot)\) is
said to be obtained from (old) credence function \(\Cr(\wcdot)\) by
<em>conditionalization</em> on \(E\) if, for each \(X \in {\cal
A}\),</p> 

  \[\Cr'(X) = \frac{\Cr(X\cap E)}{\Cr(E)}.\]

 </li>
</ul>
</div>

<p>
Conditionalization changes the credence in \(X\) from \(\Cr(X)\) to
\(\Cr'(X)\), which can be understood as involving two steps:</p>

\[\Cr(X) \ovrightarrow{(i)}
\Cr(X \cap E) \ovrightarrow{(ii)} \frac{\Cr(X\cap E)}{\Cr(E)} = \Cr'(X) .\]

<p>
Transition (i) corresponds to the zeroing step in the the informal
presentation in <a href="#sec-core">section 1.2</a> of conditionalization;
 transition (ii), the rescaling step. Now the second norm can be
stated as follows:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">The Principle of Conditionalization (Standard
Version).</em> One&rsquo;s credences ought to change by and only by
conditionalization on the new evidence received.</li>
</ul>
</div>

<p>
The two norms just stated reduce to the informal versions presented in
the tutorial
 <a href="#sec-core">section 1.2</a>
 when \(\Omega\) contains only finitely many possibilities and \(\cal
A\) is the set of all subsets of \(\Omega\).</p>

<p>
Let \(\Cr(X \mid E)\) denote one&rsquo;s credence in \(X\) on the
supposition of the truth of \(E\) (whether or not one will actually
receive \(E\) as new evidence); it is also called credence in \(X\)
given \(E\), or credence in \(X\) conditional on \(E\). So \(\Cr(X
\mid E)\) denotes a <em>conditional</em> credence, while \(\Cr(X)\)
denotes an <em>unconditional</em> one. The connection between those
two kinds of credences is often expressed by</p>

<div class="indent">

<p style="margin:0pt">
<em class="sc">The Ratio Formula</em></p> 

      \[\Cr(X\mid E) = \frac{\Cr(X \cap E)}{\Cr(E)} \quad\text{ if } \Cr(E) \neq 0.\]

</div>

<p>
It is debatable whether this formula should be construed as a
definition or as a normative constraint. See H&aacute;jek (2003) for some
objections to the definitional construal and for further discussion.
\(\Cr(X \mid E)\) is often taken as shorthand for the credence in
\(X\) that results from conditionalization on \(E\), assuming that the
Ratio Formula holds.</p>

<p>
Many applications of Bayesian epistemology make use <em>Bayes&rsquo;
theorem</em>. It has different versions, of which two are particularly
simple:</p>

<div class="indent">

<ul class="hanging">

<li>

<p>
<em class="sc">Bayes&rsquo; Theorem (Simplest Version).</em> Suppose
that \(\Cr\) is probabilistic and assigns nonzero credences to \(H\)
and \(E\), and that the Ratio Formula
 holds.<sup>[<a href="notes.html#note-3" id="ref-3">3</a>]</sup>
 Then we have:</p> 

\[
        \Cr(H\mid E) = \frac{\Cr(E \mid H) \cdot \Cr(H)}{\Cr(E)} .
        \]</li></ul></div>

<div class="indent">

<ul class="hanging">

<li>

<p>
<em class="sc">Bayes&rsquo; Theorem (Finite Version).</em> Suppose
further that hypotheses \(H_1, \ldots, H_N\) are mutually exclusive
and finite in number, and that each is assigned a nonzero
credence and their disjunction is assigned credence 1 by \(\Cr\). Then
we have:</p> 

\[
        \Cr(H_i\mid E) = \frac{\Cr(E \mid H_i) \cdot \Cr(H_i)}{\sum_{j=1}^{N} \Cr(E \mid H_j) \cdot \Cr(H_j)} .
   \]

 </li>
</ul>

</div>

<p>
This theorem is often useful for calculating credences that result
from conditionalization on evidence \(E\), which are represented on the left side
of the formula. Indeed, this theorem is very useful and important in
statistical applications of Bayesian epistemology (see
 <a href="#sec-chance-credence">section 3.5</a>
 below). For more on the significance of this theorem, see the entry
on
 <a href="../bayes-theorem/index.html">Bayes&rsquo; theorem</a>.
 But this theorem is not essential to some other applications of
Bayesian epistemology. Indeed, the case studies in the tutorial
section make no reference to Bayes&rsquo; theorem. As Earman (1992:
ch. 1) points out in his presentation of Bayes&rsquo; (1763) seminal
essay, Bayesian epistemology is <em>Bayesian</em> not really because
Bayes&rsquo; theorem is used in a certain way, but because
Bayes&rsquo; essay already contains the core ideas of Bayesian
epistemology: Probabilism and the Principle of Conditionalization.</p>

<p>
Here are some introductory textbooks on Bayesian epistemology (and
related topics) that include presentations of elementary probability theory:
Skyrms (1966 [2000]), Hacking (2001), Howson &amp; Urbach (2006),
Huber (2018), Weisberg (2019
 [<a href="#Oth">Other Internet Resources</a>]),
 and Titelbaum (forthcoming).</p>
</div>

<div id="sec-sync-coh">

<h2 id="SyncNormIRequCohe">3. Synchronic Norms (I): Requirements of Coherence</h2>

<p>
A coherence norm states how one&rsquo;s opinions ought to fit together
on pain of incoherence. Most Bayesians agree that the correct
coherence norms include at least Probabilism, but they disagree over
which version of Probabilism is right. There is also the question of
whether there are correct coherence norms that go beyond Probabilism
and, if so, what they are. Those issues were only sketched in the
tutorial
 <a href="#sec-intro-coh">section 1.4</a>.
 They will be detailed in this section.</p>

<p>
To argue that a certain norm is not just correct but ought to be
followed <em>on pain of incoherence</em>, Bayesians traditionally
proceed by way of a Dutch Book argument (as presented in the tutorial
 <a href="#sec-foundation">section 1.6</a>).
 For the susceptibility to a Dutch Book is traditionally taken by
Bayesians to imply one&rsquo;s personal incoherence. So, as you will
see below, the norms discussed in this section have all been defended
with one or another type of Dutch Book argument, although it is
debatable whether some types are more plausible than others.</p>

<div id="sec-ver-prob">

<h3 id="VersProb">3.1 Versions of Probabilism</h3>

<p>
Probabilism is often stated as follows:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Probabilism (Standard Version).</em> One&rsquo;s
assignment of credences ought to be probabilistic in this sense: it is
a probability measure.</li>
</ul>
</div>

<p>
This norm implies that one should have a credence in a logical truth 
(indeed, a credence of 1) and that, when one has credences in some
propositions, one should <em>also</em> have credences in their
conjunctions, disjunctions, and negations. So Probabilism in its
standard version asks one to have credences in certain propositions.
But that seems to be in tension with the fact that Probabilism is
often understood as a <em>coherence</em> norm. To see why, note that
coherence is a matter of fitting things together nicely. So coherence
is supposed to put a constraint on the combinations of attitudes that
one may have, <em>without</em> saying that one must have an attitude
toward such and such propositions&mdash;contrary to the above version
of Probabilism. If so, the right version of Probabilism must be weak
enough to allow the absence of some credences, also called
<em>credence gaps</em>.</p>

<p>
The above line of thought has led some Bayesians to develop and defend
a weaker version of Probabilism (de Finetti 1970 [1974], Jeffrey 1983,
Zynda 1996):</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Probabilism (Extensibility Version).</em>
One&rsquo;s assignment of credences ought to be probabilistically
extensible in this sense: either it is already a probability measure,
or it can be turned into a probability measure by assigning new
credences to some more propositions without changing the existing
credences.</li>
</ul>
</div>

<p>
It is the second disjunct that allows credence gaps. De Finetti (1970
[1974: sec. 3]) also argues that, when the Dutch Book argument for
Probabilism is carefully examined, it can be seen to support only the
extensibility version rather than the standard one. de Finetti&rsquo;s
idea is to adopt a liberal conception of betting dispositions. Namely,
he holds that one is permitted to lack any betting disposition about a
proposition, which in turn permits one to lack a credence in that
proposition.</p>

<p>
The above two versions of Probabilism are still similar in that they
both imply that any credence ought to be sharp&mdash;being an
individual real number. But some Bayesians maintain that coherence
does not require that much but allows credences to be <em>unsharp</em>
in a certain sense. An even weaker version of Probabilism has been
developed accordingly, defended with a Dutch Book argument that works
with a more liberal conception of betting dispositions than mentioned
above (Smith 1961; Walley 1991: ch. 2 and 3). See
 <a href="supplement.html#sec-unsharp-dutch">supplement A</a>
 for some non-technical details. Bayesians actually disagree over
whether coherence allows credences to be unsharp. For this debate, see
the survey by Mahtani (2019) and the entry on
 <a href="../imprecise-probabilities/index.html">imprecise probabilities</a>.</p>
 </div>

<div id="sec-countable">

<h3 id="CounAddi">3.2 Countable Additivity</h3>

<p>
Probabilism, as stated in
 <a href="#sec-math">section 2</a>,
 implies Finite Additivity, the norm that one&rsquo;s credence in the
disjunction of two incompatible disjuncts ought to be equal to the sum
of the credences in those two disjuncts. Finite Additivity can be
naturally strengthened as follows:</p>

<div class="indent">

<ul class="hanging">

<li>

<p>
<em class="sc">Countable Additivity.</em> It ought to be that, for any
propositions \(A_1,\) \(A_2,\)&hellip;, \(A_n,\)&hellip; that are
mutually exclusive, if one has credences in those propositions and in
their disjunction \(\bigcup_{n=1}^{\infty} A_n\), then one&rsquo;s
credence function \(\Cr\) satisfies the following formula:</p>

  \[\Cr\left( \bigcup_{n=1}^{\infty} A_n \right) = \sum_{n = 1}^{\infty} \Cr\left(A_n\right).\]

 </li>
</ul>
</div>

<p>
Countable Additivity has extensive applications, both in statistics
and in philosophy of science; for a concise summary and relevant
references, see J. Williamson (1999: sec. 3).</p>

<p>
Although Countable Additivity is a natural strengthening of Finite
Additivity, the former is much more controversial. de Finetti (1970
[1974]) proposes a counterexample:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Infinite Lottery).</em> There is a fair
lottery with a countable infinity of tickets. Since it is fair, there
is one and only one winning ticket, and all tickets are equally likely
to win. For an agent taking all those for granted (i.e., with full
credence), what should be her credence in the proposition \(A_n\) that
the <i>n</i>-th ticket will win?</li>
</ul>
</div>

<p>
The answer seems to be 0. To see why, note that all those propositions
\(A_n\) should be assigned equal credences \(c\), by the fairness of
the lottery. Then it is not hard to show that, in order to satisfy
Probabilism, a positive \(c\) is too high and a negative \(c\) is too
 low.<sup>[<a href="notes.html#note-4" id="ref-4">4</a>]</sup>
 So, by Probabilism, the only alternative is \(c = 0\). But this
result violates Countable Additivity: by the fairness of the lottery,
the left side is</p> 

    \[\Cr\left(\bigcup_{n = 1}^{\infty} A_n\right) = 1,\] 

<p>
but the right side is</p> 

  \[\sum_{n = 1}^{\infty} \Cr\left(A_n\right) = \bigcup_{n=1}^{\infty} c = 0.\]

<p>
De Finetti thus concludes that this is a counterexample to Countable
Additivity. For closely related worries about Countable Additivity,
see Kelly (1996: ch. 13) and Seidenfeld (2001). Also see Bartha (2004:
sec. 3) for discussions and further references.</p>

<p>
Despite the above controversy, attempts have been made to argue for
Countable Additivity, partly because of the interest in saving its
extensive applications. For example, J. Williamson (1999) defends
the idea that there is a good Dutch Book argument for Countable
Additivity even though the Dutch Book involved has to contain a
countable infinity of bets and the agent involved has to be able to
accept or reject that many bets. Easwaran (2013) provides further
defense of the Dutch Book argument for Countable Additivity (and
another argument for it). The above two authors also argue that the Infinite Lottery Case only appears
to be a counterexample to Countable Additivity and can be explained
away.</p>

<p>
It is debatable whether we really need to defend Countable Additivity
in order to save its extensive applications. Bartha (2004) thinks that
the answer is negative. He argues that, even if Countable Additivity
is abandoned due to the Infinite Lottery Case, this poses no serious
threat to its extensive applications.</p>
</div>

<div id="sec-regular">

<h3 id="Regu">3.3 Regularity</h3>

<p>
A contingent proposition is true in some cases, while a logical
falsehood is true in no cases at all. So perhaps the credence in
the former should always be greater than the credence in the latter,
which must be 0. This line of thought motivates the following
norm:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Regularity.</em> It ought to be that, if one has a
credence in a logically consistent proposition, it is greater than
0.</li>
</ul>

</div>

<p>
Regularity has been defended with a Dutch Book argument&mdash;a
somewhat nonstandard one. Kemeny (1955) and Shimony (1955) show that
any violation of Regularity opens the door to a nonstandard,
<em>weak</em> Dutch Book, which is a set of bets that guarantees no
gain but has a possible loss. In contrast, a standard Dutch Book has a
sure loss. This raises the question whether it is really so bad to be
vulnerable to a weak Dutch Book.</p>

<p>
One might object to Regularity on the ground that it is in conflict
with Conditionalization. To see the conflict, note that
conditionalization on a contingent proposition \(E\) drops the
credence in another contingent proposition, \(\neg E\), down to zero.
But that violates Regularity. In reply, defenders of Regularity can
replace conditionalization by a generalization of it called
<em>Jeffrey Conditionalization</em>, which need not drop any credence
down to zero. Jeffrey Conditionalization will be defined and discussed
in
 <a href="#sec-jeffrey">section 5.3</a>.</p>
 
<p>
There is a more serious objection to Regularity. Consider the
following case:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Coin).</em> An agent is interested in the
<em>bias</em> of a certain coin&mdash;the objective, physical chance
for that coin to land heads when tossed. This agent&rsquo;s credences
are <em>distributed uniformly</em> over the possible biases of the
coin. This means that her credence in &ldquo;the bias falls within
interval \([a, b]\)&rdquo; is equal to the length of the interval,
\(b-a\), provided that the interval is nested within \([0, 1]\). Now
think about &ldquo;the coin is fair&rdquo;, which says that the bias
is equal to 0.5, i.e., that the bias falls within the trivial interval
\([0.5, 0.5]\). So &ldquo;the coin is fair&rdquo; is assigned credence
\(0.5 - 0.5\), which equals 0 and violates Regularity.</li>
</ul>
</div>

<p>
But there seems to be nothing incoherent in this agent&rsquo;s
credences.</p>

<p>
One possible response is to insist on Regularity and hold that the
agent in the Coin Case is actually incoherent in a subtle way. Namely,
that agent&rsquo;s credence in &ldquo;the coin is fair&rdquo; should
not be zero but should be an <em>infinitesimal</em>&mdash;smaller than
any positive real number but still greater than zero (Lewis 1980). On
this view, the fault lies not with Regularity but with the standard
version of Probabilism, which needs to be relaxed to permit
infinitesimal credences. For worries about this appeal to
infinitesimals, see H&aacute;jek (2012) and Easwaran (2014). For a
survey of infinitesimal credences/probabilities, see Wenmackers
(2019).</p>

<p>
The above response to the Coin Case implements a general strategy. The
idea is that some doxastic states are so nuanced that even real
numbers are too coarse-grained to distinguish them, so real-valued
credences need to be supplemented with <em>something else</em> for a
better representation of one&rsquo;s doxastic states. The above
response proposes that the supplement be <em>infinitesimal
credences</em>. A second response proposes, instead, that the
supplement be <em>comparative probability</em>, with a very different
result: abandoning Regularity rather than saving it.</p>

<p>
This second response can be developed as follows. While being assigned
a higher numerical credence implies being taken as more probable,
being assigned the same numerical credence does not really imply being
taken as equally probable. That is, (real-valued) numerical credences
actually do not have enough structure to represent everything there is
in a qualitative ordering of comparative probability, as H&aacute;jek
(2003) suggests. So, in the Coin Case, the contingent proposition
&ldquo;the coin is fair&rdquo; is assigned credence 0, the same
credence as a logical falsehood is assigned. But it does not mean
that those two propositions, one contingent and one
self-contradictory, should be taken as equally probable. Instead, the
contingent proposition &ldquo;the coin is fair&rdquo; should still be
taken as more probable than a logical falsehood. That is, the
following norm still holds:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Comparative Regularity.</em> It ought to be that,
whenever one has a judgment of comparative probability between a
contingent proposition and a logical falsehood, the former is
taken to be more probable than the latter.</li>
</ul>
</div>

<p>
So, although the second response bites the bullet and abandons
Regularity (due to the Coin Case), it manages to settle on a
variant, Comparative Regularity. But even Comparative Regularity can
be challenged: see T. Williamson (2007) for a putative counterexample.
And see Haverkamp and Schulz (2012) for a reply in support of
Comparative Regularity.</p>

<p>
Note that the second response makes use of one&rsquo;s ordering of
comparative probability, which can be too nuanced to be fully captured
by real-valued credences. As it turns out, such an ordering can still
be fully captured by real-valued <em>conditional</em> credences (as
explained in
 <a href="supplement.html#sec-order-cond">supplement B</a>),
 provided that it makes sense for a person to have a credence in a
proposition conditional on a <em>zero-credence</em> proposition. It is
to this kind of conditional credence that I now turn.</p>
</div>

<div id="sec-prim-condi">

<h3 id="NormCondCred">3.4 Norms of Conditional Credences</h3>

<p>
In Bayesian epistemology, a doxastic state is standardly represented
by a credence assignment \(\Cr\), with conditional credences
characterized by</p>

<div class="indent">

<p style="margin:0pt">
<em class="sc">The Ratio Formula</em></p> 

      \[ \Cr(A\mid B) = \frac{\Cr(A \cap B)}{\Cr(B)}\quad \text{ if } \Cr(B) \neq 0.\]

</div>

<p>
The Ratio Formula might be taken to define conditional credences (on
the left) in terms of unconditional credences (on the right), or be taken
as a normative constraint on those two kinds of mental states without
defining one by the other. See H&aacute;jek (2003) for some objections
to the definitional construal and for further discussion.</p>

<p>
Whether the Ratio Formula is construed as a definition or a norm, it
applies only when the conditioning proposition \(B\) is assigned a
nonzero credence: \(\Cr(B) \neq 0\). But perhaps this qualification is
too restrictive:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Coin, Continued).</em> Conditional on
&ldquo;the coin is fair&rdquo;, the agent has a 0.5 credence in
&ldquo;the coin will land heads the next time it is
tossed&rdquo;&mdash;and rightly so. But this agent assigns a
<em>zero</em> credence in the conditioning proposition, &ldquo;the
coin is fair&rdquo;, as in the previous Coin Case.</li>
</ul>
</div>

<p>
This 0.5 conditional credence seems to make perfect sense, but it
eludes the Ratio Formula. Worse, the above case is not rare: the above
conditional credence is a credence in an event conditional on a
statistical hypothesis, and such conditional credences, often called
<em>likelihoods</em>, have been extensively employed in statistical
applications of Bayesian epistemology (as will be explained in
 <a href="#sec-chance-credence">section 3.5</a>).</p>
 

<p>
There are three possible ways out. They differ in the importance they
attribute to the Ratio Formula as a stand-alone norm. So you
can expect a reformatory approach which takes it to be unimportant, a 
conservative one which retains its importance, and a middle
way between the two.</p>

<p>
On the <em>reformatory</em> approach, the Ratio Formula is no longer
important and, instead, is derived as a mere consequence of something
more fundamental. While the standard Bayesian view takes norms of
unconditional credences to be fundamental and then uses the Ratio
Formula as a bridge to conditional credences, the reformatory approach
reverses the direction, taking norms of conditional credences as
fundamental. Following Popper (1959) and R&eacute;nyi (1970), this
idea can be implemented with a version of Probabilism designed
directly for conditional credences:</p>

<div class="indent">

<ul class="hanging">

<li>

<p>
<em class="sc">Probabilism (Conditional Version).</em> It ought to be
that one&rsquo;s assignment of conditional credences \(\Cr( \wcdot
\mid \wcdot)\) is a Popper-R&eacute;nyi function over an algebra
\({\cal A}\) of propositions, namely, a function satisfying the
following axioms:</p>

<ul class="hanging">

<li>(<em class="sc">Probability</em>) For any logically consistent
proposition \(A \in {\cal A}\) held fixed, \(\Cr( \wcdot \mid A)\) is
a probability measure on \({\cal A}\) with \(\Cr( A \mid A) =
1\).</li>

<li>

<p>
(<em class="sc">Multiplication</em>) For any propositions \(A\),
\(B\), and \(C\) in \({\cal A}\) such that \(B \cap C\) is logically
consistent,</p> 

\[\Cr(A\cap B \mid C)  = \Cr(A \mid B \cap C) \cdot \Cr(B \mid C) .\]

 </li>
</ul> </li>
</ul>

</div>

<p>
This approach is often called the approach of <em>coherent conditional
probability</em>, because it seeks to impose coherence
constraints directly on conditional credences without a detour through
unconditional credences. Once those constraints are in place, one may
then add a constraint&mdash;normative or definitional&mdash;on
unconditional credences:</p> 

\[\Cr(A) = \Cr(A \mid \top),\]

<p>
where \(\top\) is a logical truth. From the above we can derive the
Ratio Formula and the standard version of Probabilism. See
H&aacute;jek (2003) for a defense of this approach. A Dutch Book
argument for the conditional version of Probabilism is developed by
Stalnaker (1970).</p>

<p>
In contrast to the reformatory nature of the above approach, the
second one is <em>conservative</em>. On this approach, the Ratio
Formula is sufficient by itself as a norm (or definition) for
conditional credences. It makes sense to have a credence
conditional on &ldquo;the coin is fair&rdquo; because one&rsquo;s
credence in that conditioning proposition ought to be an infinitesimal rather than zero. This
approach may be called the approach of <em>infinitesimals</em>. It
forms a natural package with the infinitesimal approach to saving
Regularity from the Coin Case, which was discussed in
 <a href="#sec-regular">section 3.3</a>.</p>
 

<p>
Between the conservative and the reformatory, there is the
<em>middle</em> way, due to Kolmogorov (1933). The idea is to think
about the cases where the Ratio Formula applies, and then use them to
&ldquo;approximate&rdquo; the cases where it does not apply. If this
can be done, then although the Ratio Formula is not all there
is to norms for conditional credences, it comes close. To be more
precise, when we try to conditionalize on a zero-credence proposition
\(B\), we can approximate \(B\) by a sequence of propositions \(B_1,\)
\(B_2,\)&hellip; such that:</p>

<ul>

<li>those propositions \(B_1, B_2, \ldots\) are progressively more
specific (i.e., \(B_i \supset B_{i+1}\)),</li>

<li>they jointly say what \(B\) says (i.e., \(\bigcap_{i=1}^{\infty}
B_i = B\)).</li>
</ul>

<p>
In that case, it seems tempting to accept the norm or definition that
conditionalization on \(B\) be approximated by successive
conditionalizations on \(B_1, B_2, \ldots\), or in symbols:</p>

\[\Cr(A \mid B) = \lim_{i \to \infty}\Cr(A \mid B_i),\]

<p>
where each term \(\Cr(A \mid B_i)\) is governed by the Ratio Formula
because \(\Cr(B_i)\) is nonzero by design. An important consequence of
this approach is that, when one chooses a different sequence of
propositions to approximate \(B\), the limit of conditionalizations
might be different, and, hence, a credence conditional on \(B\) is, or
ought to be, relativized to how one presents \(B\) as the limit of a
sequence of approximating propositions. This relativization is often
illustrated with what&rsquo;s called the <em>Borel-Kolmogorov
paradox</em>; see Rescorla (2015) for an accessible presentation and discussion. Once the
mathematical details are refined, this approach becomes what&rsquo;s
known as the theory of <em>regular conditional
 probability</em>.<sup>[<a href="notes.html#note-5" id="ref-5">5</a>]</sup>
 A Dutch Book argument for this way of assigning conditional credences
is developed by Rescorla (2018).</p>

<p>
For a critical comparison of those three approaches to conditional
credences, see the survey by Easwaran (2019).</p>
</div>

<div id="sec-chance-credence">

<h3 id="ChanCredPrin">3.5 Chance-Credence Principles</h3>

<p>
Recall the Coin Case discussed above: one&rsquo;s credence in
&ldquo;the coin will land heads the next time it is tossed&rdquo;
conditional on &ldquo;the coin is fair&rdquo; is equal to 0.5. This
0.5 conditional credence seems to be the only permissible alternative
until the result of the next coin toss is observed. This example
suggests a general norm, which connects chances to conditional
credences:</p>

<div class="indent">

<ul class="hanging">

<li>

<p>
<em class="sc">The Principal Principle/Direct Inference
Principle.</em> Let \(\Cr\) be one&rsquo;s prior, i.e., the credence
assignment that one has at the beginning of an inquiry. Let \(E\) be
the event that such and such things will happen at a certain future
time. Let \(A\) be a proposition that entails \(\Ch(E) = c\), which
says that the chance for \(E\) to come out true is equal to \(c\).
Then one&rsquo;s prior \(\Cr\) ought to be such that
        \(\Cr(E \mid A) = c\),
if \(A\) is an &ldquo;ordinary&rdquo; proposition in that it is
logically equivalent to the conjunction of \(\Ch(E) = c\) with an
&ldquo;admissible&rdquo; proposition.</p></li>
</ul>

</div>

<p>
The if-clause refers to &ldquo;admissible&rdquo; propositions, which
are roughly propositions that give no more information about whether
or not \(E\) is true than is already contained in \(\Ch(E) = c\). To
see why we need the qualification imposed by the if-clause, suppose
for instance that the event \(E\) is &ldquo;the coin will land heads
the next time it is tossed&rdquo;. If the conditioning proposition
\(A\) is &ldquo;the coin is fair&rdquo;, it is a paradigmatic example
of an &ldquo;ordinary&rdquo; proposition. This reproduces the Coin
Case, with the conditional credence being the chance 0.5.
Alternatively, if the conditioning proposition \(A\) is the
conjunction of &ldquo;the coin is fair&rdquo; and \(E\), then the
conditional credence \(\Cr(E \mid A)\) should be 1 rather than the 0.5
chance of \(E\) that \(A\) entails. After all, to be given this \(A\)
is to be given a lot of information, which entails \(E\). So this case
is supposed to be ruled out by an account of &ldquo;admissible&rdquo;
propositions. Lewis (1980) initiates a systematic quest for such an
account, which has invited counterexamples and responses. See Joyce
(2011: sec. 4.2) for a survey.</p>

<p>
The Principal Principle has been defended with an argument based on
considerations about the accuracies of credences (Pettigrew 2012), and
with a nonstandard Dutch Book argument (Pettigrew 2020a: sec.
2.8).</p>

<p>
The Principal Principle is important perhaps mainly because of its
extensive applications in Bayesian statistics, in which this principle
is more often called the Direct Inference Principle. To illustrate,
suppose that you are somehow certain that one of the following two
hypotheses is true: \(H_1 =\) &ldquo;the coin has a bias 0.4&rdquo;
and \(H_2 =\) &ldquo;the coin has a bias 0.6&rdquo;, which are
paradigmatic examples of &ldquo;ordinary&rdquo; hypotheses. Then your
credence in the first hypothesis \(H_1\) given evidence \(E\) that the
coin lands heads ought to be expressible as
 follows:<sup>[<a href="notes.html#note-6" id="ref-6">6</a>]</sup></p>
 

\[\begin{align}
        \Cr(H_1 \mid E)
        &amp;= \frac{ \Cr(E \mid H_1) \cdot \Cr(H_1) }{ \sum_{i =1}^2 \Cr(E \mid H_i) \cdot \Cr(H_i) } &amp;{\text{by Bayes' Theorem}\\ \text{(as stated in &sect;2)}}
        \\
        &amp;= \frac{ 0.4 \cdot \Cr(H_1) }{ 0.4 \cdot \Cr(H_1) + 0.6 \cdot \Cr(H_2) } &amp;{\text{by the Principal}\\ \text{Principle}}
    \end{align}\]

<p>
So Bayes&rsquo; Theorem works by expressing posterior credences in
terms of some prior credences \(\Cr(H_i)\) and some prior conditional
credences \(\Cr(E \mid H_i)\). The latter, called
<em>likelihoods</em>, are <em>subjective</em> opinions, but they can
be replaced by <em>objective</em> chances thanks to the Principal
Principle. So this principle is often taken to be an important way to
reduce some subjective factors in the Bayesian account of scientific
inference. For discussions of other subjective factors, see
 <a href="#sec-subjective">section 4.1</a>.</p>
 

<p>
Even though the Principal Principle has important, extensive
applications in Bayesian statistics as just explained, de Finetti
(1970 [1974]) argues that it is actually dispensable and thus need not
be accepted as a norm. To be more specific, he argues that the
Principal Principle is dispensable in a way that changes little of the
actual practice of Bayesian statistics. His argument relies on his
<em>exchangeability theorem</em>. See Gillies (2000: 69&ndash;82) for
a non-technical introduction to this topic; also see Joyce (2011: sec.
4.1) for a more advanced survey.</p>
</div>

<h3 id="ReflOtheDefePrin">3.6 Reflection and Other Deference Principles</h3>

<p>
We have just discussed the Principal Principle, which in a sense asks
one to defer to a kind of expert (Gaifman 1986): the chance of an
event \(E\) can be understood as an expert at predicting whether \(E\)
will come out true. So, conditional on that expert&rsquo;s saying so
and so about \(E\), one&rsquo;s opinion ought to defer to that expert.
Construed that way, the Principal Principle is a kind of <em>deference
principle</em>. There can be different deference principles, referring
to different kinds of experts.</p>

<p>
Here is another example of a deference principle, proposed by van
Fraassen (1984):</p>

<div class="indent">

<ul class="hanging">

<li>

<p>
<em class="sc">The Reflection Principle.</em> One&rsquo;s credence at
any time \(t_1\) in a proposition \(A\), conditional on the
proposition that one&rsquo;s future credence at \(t_2\) \((&gt; t_1)\)
in \(A\) will be equal to \(x\), ought to be equal to \(x\); or put
symbolically:</p> 

\[\Cr_{t_1}( A \mid \Cr_{t_2}(A) = x )  = x.\]

<p>
More generally, it ought to be that</p> 

\[\Cr_{t_1}( A \mid \Cr_{t_2}(A) \in [x, x'] )  \in  [x, x'].\]

 </li>
</ul>
</div>

<p>
Here, one&rsquo;s future self is taken as an expert to which one ought
to defer. The Reflection Principle admits of a Dutch Book argument
(van Fraassen 1984). There is another way to defend the Reflection
Principle: this synchronic norm is argued to follow from the
<em>synchronic</em> norm that one ought, at any time, to be fully
certain that one will follow the <em>diachronic</em> Principle of
Conditionalization (as suggested by Weisberg&rsquo;s 2007 modification
of van Fraassen&rsquo;s 1995 argument).</p>

<p>
The Reflection Principle has invited some putative counterexamples.
Here is one, adapted from Talbott (1991):</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Dinner).</em> Today is March 15, 1989.
Someone is very confident that she is now having spaghetti for dinner.
She is also very confident that, on March 15, 1990 (exactly one year
from today), she will have completely forgotten what she is having for
dinner now.</li>
</ul>
</div>

<p>
So, this person&rsquo;s current assignment of credences
\(\Cr_\textrm{1989}\) has the following properties, where \(A\) is the
proposition that she has spaghetti for dinner on March 15, 1989:</p>

\[\begin{align}
    \Cr_\textrm{1989} \big( A \big) &amp;= \text{high} 
    \\
    \Cr_\textrm{1989} \Big( \Cr_\textrm{1989+1}(A) \mbox{ is low}  \Big) &amp;= \text{high} .
    \end{align}\]

<p>
But conditionalization on a proposition with a high credence can only
slightly change the credence assignment. For such a conditionalization
involves lowering just a small bit of credence down to zero and hence
it only requires a slight rescaling, by a factor close to 1. So,
assuming that \(\Cr\) is a probability measure, we have:</p>

\[
    \Cr_\textrm{1989} \Big( A \Bigm\vert \Cr_\textrm{1989+1}(A) \mbox{ is low}  \Big) = \text{still high} ,
\]

<p>
which violates the Reflection Principle.</p>

<p>
The Dinner Case serves as a putative counterexample to the Reflection
Principle by allowing one to suspect that one will lose some memories.
So it allows one to have a specific kind of <em>epistemic
self-doubt</em>&mdash;to doubt one&rsquo;s own ability to achieve or
retain an epistemically favorable state. In fact, some are worried that the
Reflection Principle is generally incompatible with epistemic
self-doubt, which seems rational and permissible. For more on this
worry, see the entry on
 <a href="../epistemic-self-doubt/index.html">epistemic self-doubt</a>.</p>
 </div>

<div id="sec-prior">

<h2 id="SyncNormIIProbPrio">4. Synchronic Norms (II): The Problem of the Priors</h2>

<p>
Much of what Bayesians have to say about confirmation and inductive
inference depends crucially on the norms that govern one&rsquo;s prior
credences (the credences that one has at the beginning of an inquiry).
But what are those norms? This is known as the <em>problem of the
priors</em>. Some potential solutions were only sketched in the
tutorial
 <a href="#sec-intro-prior">section 1.5</a>.
 They will be detailed in this section.</p>

<div id="sec-subjective">

<h3 id="SubjBaye">4.1 Subjective Bayesianism</h3>

<p>
Subjective Bayesianism is the view that every prior is permitted
unless it fails to be coherent (de Finetti 1970 [1974]; Savage 1972;
Jeffrey 1965; van Fraassen 1989: ch. 7). Holding that view as the
common ground, subjective Bayesians often disagree over what coherence
requires (which was the topic of the preceding
 <a href="#sec-sync-coh">section 3</a>).</p>
 
<p>
The most common worry for subjective Bayesianism is that, on that
view, anything goes. For example, under just Probabilism and
Regularity, there is a prior that follows enumerative induction and
there also is a prior whose posterior never generalizes from data,
defying enumerative induction (see Carnap 1955 for details, but see
Fitelson 2006 for a concise presentation). Under just Probabilism and
the Principal Principle, there is a prior that follows Ockham&rsquo;s
razor in statistical model selection but there also is a prior that
does not (Forster 1995: sec. 3; Sober 2002: sec.
 6).<sup>[<a href="notes.html#note-7" id="ref-7">7</a>]</sup>
 So, although subjective Bayesianism does not really say that anything
goes, it seems to permit too much, failing to account for some
important aspects of scientific objectivity&mdash;or so the worry
goes. Subjective Bayesians have replied with at least two
strategies.</p>

<p>
Here is one: argue that, despite appearances, coherence alone
captures everything there is to scientific objectivity. For example,
it might be argued that it is actually correct to permit a wide range
of priors, for people come with different background opinions and it
seems wrong&mdash;objectively wrong&mdash;to require all of them to
change to the same opinion at once. What ought to be the case is,
rather, that people&rsquo;s opinions be brought closer and closer to
each other as their shared evidence accumulates. This idea of
<em>merging-of-opinions</em> as a kind of scientific objectivity can
be traced back to Peirce (1877), although he develops this idea for
the epistemology of all-or-nothing beliefs rather than credences. Some
subjective Bayesians propose to develop this Peircean idea in the
framework of subjective Bayesianism: to have the ideal of
merging-of-opinions be derived as a norm&mdash;derived solely from
coherence norms. That is, they prove so-called <em>merging-of-opinions
theorems</em> (Blackwell &amp; Dubins 1962; Gaifman &amp; Snir 1982).
Such a theorem states that, under such and such contingent initial
conditions together with such and such coherence norms, two agents
must be <em>certain</em> that their credences in the hypotheses under consideration will merge with each other <em>in the long run</em> as the
shared evidence accumulates indefinitely.</p>

<p>
The above theorem is stated with two italicized parts, which are the
targets of some worries. The merging of the two agents&rsquo; opinions
might not happen and is only believed with certainty to happen in the
long run. And the long run might be too long. There is another worry:
the proof of such a theorem requires Countable Additivity as a norm of
credences, which is controversial, as was discussed in
 <a href="#sec-countable">section 3.2</a>.
 See Earman (1992: ch. 6) for more on those
 worries.<sup>[<a href="notes.html#note-8" id="ref-8">8</a>]</sup>
 For a recent development of merging-of-opinions theorems and a
defense of their use, see Huttegger (2015).</p>

<p>
Whether or not merging-of-opinions theorems can capture the intended
kind of scientific objectivity, it is still debated whether there are
other kinds of scientific objectivity that elude subjective
Bayesianism. For more on this issue, see
 <a href="../scientific-objectivity/index.html#StatEvid">section 4.2 of the entry on scientific objectivity</a>,
 Gelman &amp; Hennig (2017) (including peer discussions), Sprenger (2018), and Sprenger
&amp; Hartmann (2019: ch. 11).</p>

<p>
Here is a second strategy in defense of scientific objectivity for subjective Bayesians: distance
themselves from any substantive theory of inductive inference and hold
instead that Bayesian epistemology can be construed as a kind of
deductive logic. This view draws on some parallel features between
deductive logic and Bayesian epistemology. First, the coherence of
credences can be construed as an analogue of the logical consistency
of propositions or all-or-nothing beliefs (Jeffrey 1983). Second, just
as premises are inputs into a deductive reasoning process, prior
credences are inputs into the process of an inquiry. And, just as 
the job of deductive logic is not to say what premises we should have
except that they be logically consistent, Bayesian epistemology need
not say what prior credences we should have except that they be
coherent (Howson 2000: 135&ndash;145). Call this view the
<em>deductive construal</em> of Bayesian epistemology, for lack of a
standard name.</p>

<p>
Yet it might be questioned whether the above parallelism really works
in favor of subjective Bayesianism. Just as substantive theories of
inductive inferences have been developed with deductive logic as their basis, to
take the parallelism seriously it seems that there should also be a
substantive account of inductive inferences with the deductive
construal of Bayesian epistemology as their basis. Indeed, the anti-subjectivists to
be discussed below&mdash;objective Bayesians and forward-looking
Bayesians&mdash;all think that a substantive account of inductive
inferences is furnished by norms that go beyond the consideration of
coherence. It is to such a view that I turn now. But for more on
subjective Bayesianism, see the survey by Joyce (2011).</p>
</div>

<h3 id="ObjeBaye">4.2 Objective Bayesianism</h3>

<p>
<em>Objective Bayesians</em> contend that, in addition to coherence,
there is another epistemic virtue or ideal that needs to be codified
into a norm for prior credences: freedom from bias and avoidance of
overly strong opinions (Jeffreys 1939; Carnap 1945; Jaynes 1957, 1968;
Rosenkrantz 1981; J. Williamson 2010). This view is often motivated by
a case like this:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Six-Faced Die).</em> Suppose that there is
a cubic die with six faces that look symmetric, and we are going to
toss it. Suppose further that we have no other idea about this die.
Now, what should our credence be that the die will come up 6?</li>
</ul>
</div>

<p>
An intuitive answer is \(1/6\), for it seems that we ought to
distribute our credences evenly, with an equal credence, \(1/6\), in
each of the six possible outcomes. While subjective Bayesians would
only say that we <em>may</em> do so, objective Bayesians would make
the stronger claim that we <em>ought</em> to do so. More generally,
objective Bayesians are sympathetic to this norm:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">The Principle of Indifference.</em> A
person&rsquo;s credences in any two propositions should be equal if
her total evidence no more supports one than the other (the
<em>evidential symmetry</em> version), or if she has no sufficient
reason to have a higher credence in one than in the other (the
<em>insufficient reason</em> version).</li>
</ul>
</div>

<p>
A standard worry about the Indifference Principle comes from
<em>Bertrand&rsquo;s paradox</em>. Here is a simplified version
(adapted from van Fraassen 1989):</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Square).</em> Suppose that there is a
square and that we know for sure that its side length is between 1 and
4 centimeters. Suppose further that we have no other idea about that
square. Now, how confident should we be that the square has a side
length between 1 and 2 centimeters?</li>
</ul>
</div>

<p>
Now, have a look at the two groups of propositions listed in the table
below. The left group (1)&ndash;(3) focuses on possible side lengths
and divides up possibilities by 1-cm-long intervals; the right group
\((1')\)&ndash;\((15')\) focuses on possible areas
instead:</p>

<div>

<table class="centered cellpad-med-dense hrulesTH">
<thead>
<tr class="header">
<th style="text-align: left;"><span class="sc">Partition By
Length</span></th>
<th style="text-align: left;"><span class="sc">Partition By
Area</span></th> </tr> </thead>
<tbody>
<tr class="even">
  <td style="text-align: left;">(1) The side length is 1 to 2 cm.</td>
  <td style="text-align: left;">\((1')\) The area is 1 to 2
cm<sup>2</sup>.</td> </tr>
<tr class="odd">
  <td style="text-align: left;">(2) The side length is 2 to 3 cm.</td>
  <td style="text-align: left;">\((2')\) The area is 2 to 3
cm<sup>2</sup>.</td> </tr>
<tr class="even">
  <td style="text-align: left;">(3) The side length is 3 to 4 cm.</td>
  <td style="text-align: left;">\((3')\) The area is 3 to 4
cm<sup>2</sup>.</td> </tr>
<tr class="odd">
  <td style="text-align: left;"></td>
  <td style="text-align: left;">\(\;\;\vdots\)</td> </tr>
<tr class="even">
  <td style="text-align: left;"></td>
  <td style="text-align: left;">\((15')\) The area is 15 to 16
cm<sup>2</sup></td> </tr> </tbody>
</table>
</div>

<p>
The Indifference Principle seems ask us to assign a \(1/3\) credence
to each proposition in the left group \((1)\)&ndash;\((3)\) and,
simultaneously, assign \(1/15\) to each one in the right group
\((1')\)&ndash;\((15')\). If so, it asks us to assign unequal
credences to equivalent propositions: \(1/3\) to \((1)\), and \(3/15\)
to the disjunction \((1') \!\vee (2') \!\vee (3')\). That
violates Probabilism.</p>

<p>
In reply, objective Bayesians may reply that Bertrand&rsquo;s paradox
provides no conclusive reason against the Indifference Principle and
perhaps the fault lies elsewhere. Following White (2010), let&rsquo;s
think about how the Indifference Principle works: it outputs a
normative recommendation for credence assignment only when it receives
one or another <em>input</em>, which is a judgement about insufficient
reason or evidential symmetry. Indeed, Bertrand&rsquo;s paradox has to
be generated by at least two inputs, such as, first, the
lack-of-evidence judgement about the left group in the above table
and, second, that about the right group. So perhaps the fault lies not
with the Indifference Principle but with one of the two
inputs&mdash;after all, garbage in, garbage out. White (2010)
substantiates the above idea with an argument to this effect: at least
one of the two inputs in Bertrand&rsquo;s paradox must be mistaken,
because they already contradict each other even when we only assume
certain weak, plausible principles that have nothing to do with
credences and concern just the evidential support relation.</p>

<p>
There still remains the task of developing a systematic account to
guide one&rsquo;s judgments of evidential symmetry (or insufficient
reason) before those judgments are passed as inputs to the
Indifference Principle. An important source of inspiration has been
the symmetry in the Six-Faced Die Case: it is a kind of
<em>physical</em> symmetry due to the cubic shape of the die; it is
also a kind of <em>permutation</em> symmetry because nothing essential
changes when the six faces of the die are relabeled. Those two aspects
of the symmetry&mdash;physical and permutational&mdash;are extended by
two influential approaches to the Indifference Principle,
respectively, which are presented in turn below.</p>

<p>
The first approach to the Indifference Principle looks for a wider
range of <em>physical</em> symmetries, including especially the
symmetries associated with a change of coordinate or unit. This
approach, developed by Jeffreys (1946) and Jaynes (1968, 1973), yields
a consistent, somewhat surprising answer 1/2 (rather than 1/3 or 1/15)
to the question in the Square Case. See
 <a href="supplement.html#sec-jaynes">supplement C</a>
 for some non-technical details.</p>

<p>
The second approach to the Indifference Principle focuses on
<em>permutation</em> symmetries and proposes to look for those not in
a physical system but in the <em>language in use</em>. This approach
is due to Carnap (1945, 1955). He maintains, for example, that two
sentences ought to be assigned equal prior credences if one differs
from the other only by a permutation of the names in use. Although
Carnap says little about the Square Case, he has much to say about how
his approach to the Indifference Principle helps to justify
enumerative induction; see the survey by Fitelson (2006). So objective
Bayesianism is often regarded as a substantive account of inductive
inference, while many subjective Bayesians often take their view as a
quantitative analogue of deductive logic (as presented in
 <a href="#sec-subjective">section 4.1</a>).
 For refinement of Carnap&rsquo;s approach, see Maher (2004). The most
common worry for Carnap&rsquo;s approach is that it renders the
normative import of the Indifference Principle too sensitive to the
choice of a language; for a reply, see J. Williamson (2010: chap. 9).
For more criticisms, see Kelly &amp; Glymour (2004).</p>

<p>
The Indifference Principle has been challenged for another
reason. This principle is often understood to dictate equal
<em>real-valued</em> credences in cases of ignorance, but there is the
worry that sometimes we are too ignorant to be justified in having
sharp, real-valued credences, as suggested by this case (Keynes 1921:
ch. 4):</p>

<div class="indent">

<ul class="hanging">

<li>

<p>
<em class="sc">Example (Two Urns)</em>. Suppose that there are two
urns, <i>a</i> and <i>b</i>. Urn <i>a</i> contains 10 balls. Exactly
half of those are white; the other half, black. Urn <i>b</i> contains
10 balls, each of which is either black or white, but we have no idea
about the white-to-black ratio. Those two urns are each shaken well. A
ball is to be drawn from each. What should our credences be in the
following propositions?</p>

<ul class="subhang">

<li> (<i>A</i>) The ball from urn <i>a</i> is white.</li>

<li> (<i>B</i>) The ball from urn <i>b</i> is white.</li>
</ul> </li>
</ul>
</div>

<p>
By the Principle of Indifference, the answers seems to be 0.5 and 0.5,
respectively. If so, there should be equal credences (namely 0.5) in
<i>A</i> and in <i>B</i>. But this result sounds wrong to Keynes. 
He thinks that, compared with urn <i>a</i>, we have
much less background information about urn <i>b</i>, and that this
severe lack of background information should be reflected in the
difference between the doxastic attitudes toward propositions <i>A</i> and
<i>B</i>&mdash;a difference that the Principle of Indifference fails
to make. If so, what is the difference? It is relatively
uncontroversial that the credence in <i>A</i> should be 0.5, being the
ratio of the white balls in urn <i>a</i> (perhaps thanks to the
Principal Principle). On the other hand, some Bayesians (Keynes 1921;
Joyce 2005)
argue that the credence in <i>B</i> does not have to be an individual
real number but, instead, is at least permitted to be unsharp, being
the interval \([0, 1]\), which covers all the possible white-to-black
ratios under consideration. This is only one motivation for an
interval account of <em>unsharp</em> credences; for another
motivation, see
 <a href="supplement.html#sec-unsharp-dutch">supplement A</a>.</p>
 

<p>
In reply to the Two Urns Case, objective Bayesians have defended one
or another version of the Indifference Principle. White (2010) does it
while maintaining that credences ought to be sharp. Weatherson (2007:
sec. 4) defends a version that allows credences to be unsharp. Eva
(2019) defends a version that governs comparative probabilities rather
than numerical credences. For more on this debate, see the survey by
Mahtani (2019) and the entry on 
 <a href="../imprecise-probabilities/index.html">imprecise probabilities</a>.</p>
 

<p>
The Principle of Indifference appears unhelpful when one has had
substantive reason or evidence against some assignments of credences
(making the principle inapplicable with a false if-clause). The
standard remedy appeals to a generalization of the Indifference
Principle, called <em>the Principle of Maximum Entropy</em> (Jaynes
1968); for more on this, see
 <a href="supplement.html#sec-max-ent">supplement D</a>.</p>
 
<p>
The above has only mentioned the versions of objective Bayesianism that
are more well-known in philosophy. There are other versions,
developed and discussed mostly by statisticians. For a survey, see
Kass &amp; Wasserman (1996) and Berger (2006).</p>

<h3 id="ForwLookBaye">4.3 Forward-Looking Bayesianism</h3>

<p>
Some Bayesians propose that some norms for priors can be obtained by
looking into possible futures, with two steps (Good 1976):</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Step I (Think Ahead)</em>. Develop a normative
constraint <i>C</i> on the posteriors in some possible futures in
which new evidence is acquired.</li>

<li><em class="sc">Step II (Solve Backwards)</em>. Require one&rsquo;s
priors to be such that, after conditionalization on new evidence, its
posterior must satisfy <i>C</i>.</li>
</ul>
</div>

<p>
For lack of a standard name, this approach may be called
<em>forward-looking</em> Bayesianism. This name is used here as an umbrella term
to cover different possible implementations, of which two are
presented below.</p>

<p>
Here is one implementation. It might be held that one ought to favor a
hypothesis if it explains the available evidence better than any other
competing hypotheses do. This view is called <em>inference to the best
explanation</em> (IBE) if construed as a method for theory choice, as 
originally developed in the epistemology of all-or-nothing beliefs
(Harman 1986). It can be carried over to Bayesian epistemology as
follows:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Explanationist Bayesianism (Preliminary
Version).</em> One&rsquo;s prior ought to be such that, given each
body of evidence under consideration, a hypothesis that explains the
evidence better has a higher posterior.</li>
</ul>
</div>

<p>
What&rsquo;s stated here is only a preliminary version. More
sophisticated versions are developed by Lipton (2004: ch. 7) and
Weisberg (2009a). This view is resisted by some Bayesians to varying
degrees. van Fraassen (1989: ch. 7) argues that IBE should be rejected
because it is in tension with the two core Bayesian norms. Okasha
(2000) argues that IBE only serves as a good heuristic for guiding
one&rsquo;s credence change. Henderson (2014) argues that IBE need not
be assumed to guide one&rsquo;s credence change because it can be
justified by little more than the two core Bayesian norms. For more on
IBE, see the entry on
 <a href="../abduction/index.html">abduction</a>,
 in which sections 3.1 and 4 discuss explanationist Bayesianism.</p>

<p>
Here is another implementation of forward-looking Bayesianism. It
might be thought that, although a scientific method for theory choice
is subject to error due to its inductive nature, it is supposed to be
able, in a sense, to correct itself. This view is called <em>the
self-corrective thesis</em>, originally developed in the epistemology
of all-or-nothing beliefs by Peirce (1903) and Reichenbach (1938: sec.
38&ndash;40). But it can be carried over to Bayesian epistemology as
follows:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Self-Correctionist Bayesianism (Preliminary
Version).</em> One&rsquo;s prior ought, if possible, to have at least
the following self-corrective property in every possible state of the
world under consideration: one&rsquo;s posterior credence in the true
hypothesis under consideration would eventually become high and stay
so if the evidence were to accumulate indefinitely.</li>
</ul>
</div>

<p>
An early version of this view is developed by Freedman (1963) in
statistics; see Wasserman (1998: sec. 1&ndash;3) for a minimally
technical overview. The self-corrective property concerns the long
run, so it invites the standard, Keynesian worry that the long run
might be too long. For replies, see Diaconis &amp; Freedman (1986b:
pp. 63&ndash;64) and Kelly (2000: sec. 7). A related worry is that a long-run norm puts no
constraint on what matters, namely, our doxastic states in the short
run (Carnap 1945). A possible reply is that the self-corrective
property is only a minimum qualification of permissible priors and can
be conjoined with other norms for credences to generate a significant
constraint on priors. To substantiate that reply, it has been argued
that such a constraint on priors is actually stronger than what the
rival Bayesians have to offer in some important cases of statistical
inference (Diaconis &amp; Freedman 1986a) and enumerative induction
(Lin forthcoming).</p>

<p>
The above two versions of forward-looking Bayesianism both encourage
Bayesians to do this: assimilate some ideas (such as IBE or
self-correction) that have long been taken seriously in some
non-Bayesian traditions of epistemology. Forward-looking Bayesianism
seems to be a convenient template for doing that.</p>

<h3 id="ConnUniqDeba">4.4 Connection to the Uniqueness Debate</h3>

<p>
The above approaches to the problem of the priors are mostly developed
with this question in mind:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">The Question of Norms.</em> What are the correct
norms that we can articulate to govern prior credences?</li>
</ul>
</div>

<p>
The interest in this question leads naturally to a different but
closely related question. Imagine that you are unsympathetic to
subjective Bayesianism. Then you might try to add one norm after
another to narrow down the candidate pool for the permissible priors,
and you might be wondering what this process might end up with. This
raises a more abstract question:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">The Question of Uniqueness.</em> Given each
possible body of evidence, is there exactly one permissible credence
assignment or doxastic state (whether or not we can articulate norms
to single out that state)?</li>
</ul>
</div>

<p>
<em>Impermissive</em> Bayesianism is the view that says
&ldquo;yes&rdquo;; <em>permissive</em> Bayesianism says  &ldquo;no&rdquo;.
The question of uniqueness is often addressed in a way that is
somewhat orthogonal to the question of norms, as is suggested by the
&lsquo;whether-or-not&rsquo; clause in the parentheses. Moreover, the
uniqueness question is often debated in a broader context that
considers not just credences but all possible doxastic states,
thus going beyond Bayesian epistemology. Readers interested in
the uniqueness question are referred to the survey by Kopec and
Titelbaum (2016).</p>

<p>
Let me close this section with some clarifications. The two terms
&lsquo;objective Bayesianism&rsquo; and &lsquo;impermissive
Bayesianism&rsquo; are sometimes used interchangeably. But those two
terms are used in the present entry to distinguish two different
views, and neither implies the other. For example, many prominent
objective Bayesians such as Carnap (1955), Jaynes (1968), and J.
Williamson (2010) are not committed to impermissivism, even though
some objective Bayesians tend to be sympathetic to impermissivism. For
elaboration on the point just made, see
 <a href="supplement.html#sec-obj-vs-imp">supplement E</a>.</p>
 </div>

<div id="sec-condi">

<h2 id="IssuAbouDiacNorm">5. Issues about Diachronic Norms</h2>

<p>
The Principle of Conditionalization has been challenged with several
putative counterexamples. This section will examine some of the most
influential ones. We will see that, to save that principle, some
Bayesians have tried to refine it into one or another version. A
number of versions have been systematically compared in papers such as those of Meacham (2015, 2016), Pettigrew (2020b), and Rescorla
(2021), while the emphasis below will be centered on the proposed 
counterexamples.</p>

<div id="sec-old-evi">

<h3 id="OldEvid">5.1 Old Evidence</h3>

<p>
Let&rsquo;s start with the problem of old evidence, which was
presented above (in the tutorial
 <a href="#sec-intro-old">section 1.8</a>)
 but is reproduced below for ease of reference:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Mercury).</em> It is 1915. Einstein has
just developed a new theory, General Relativity. He assesses the new
theory with respect to some old data that have been known for at
least fifty years: the anomalous rate of the advance of
Mercury&rsquo;s perihelion (which is the point on Mercury&rsquo;s
orbit that is closest to the Sun). After some derivations and
calculations, Einstein soon recognizes that his new theory entails the
old data about the advance of Mercury&rsquo;s perihelion, while the
Newtonian theory does not. Now, Einstein increases his credence in his
new theory, and rightly so.</li>
</ul>

</div>

<p>
There appears to be no change in the body of Einstein&rsquo;s evidence
when he is simply doing some derivations and calculations. But the
<em>limiting case</em> of no new evidence seems to be just the case in
which the new evidence <i>E</i> is trivial, being a logical truth,
ruling out no possibilities. Now, conditionalization on new evidence
<i>E</i> as a logical truth changes no credence; but Einstein changes
his credences nonetheless&mdash;and rightly so. This is called <em>the
problem of old evidence</em>, formulated as a counterexample to the
Principle of Conditionalization.</p>

<p>
To save the Principle of Conditionalization, a standard reply is to
note that Einstein seems to discover something new, a logical
fact:</p>

<div class="indent">

<ul class="hanging">

<li>\((E_\textrm{logical})\) The new theory, together with such and
such auxiliary hypotheses, logically implies such and such old
evidence.</li>
</ul>

</div>

<p>
The hope is that, once this proposition has a less-than-certain credence,
Einstein&rsquo;s credence change can then be explained and justified
as a result of conditionalization on this proposition (Garber 1983,
Jeffrey 1983, and Niiniluoto 1983). There are four worries about this
approach.</p>

<p>
An initial worry is that the discovery of the logical fact
\(E_\textrm{logical}\) does not sound like adding anything to the body
of Einstein&rsquo;s evidence but seems <em>only</em> to make clear the
evidential relation between the new theory and the existing,
unaugmented body of evidence. If so, there is no new evidence after
all. This worry might be addressed by providing a modified version of the
Conditionalization Principle, according to which the thing to be
conditionalized on is not exactly what one acquires as new evidence
but, rather, what one <em>learns</em>. Indeed, it seems to sound
natural to say that Einstein learns something nontrivial from his
derivations. For more on the difference between learning and acquiring evidence, see Maher (1992: secs 2.1 and 2.3). So this approach to the
problem of old evidence is often called <em>logical learning</em>.</p>

<p>
A second worry for the logical learning approach points to an internal
tension: On the one hand, this approach has to work by permitting a
less-than-certain credence in a logical fact such as
\(E_\textrm{logical}\), and that amounts to permitting one to make a
certain kind of logical error. On the other hand, this approach has
been developed on the assumption of Probabilism, which seems to
require that one be logically omniscient and make no logical error (as
mentioned in the tutorial
 <a href="#sec-intro-ideal">section 1.9</a>).
 van Fraassen (1988) argues that these two aspects of the logical
learning approach contradict each other under some weak
assumptions.</p>

<p>
A third worry is that the logical learning approach depends for its
success on certain questionable assumptions about prior credences. For
criticisms of those assumptions as well as possible improvements, see
Sprenger (2015), Hartmann &amp; Fitelson (2015), and Eva &amp;
Hartmann (2020).</p>

<p>
There is a fourth worry, which deserves a subsection of its own.</p>
</div>

<h3 id="NewTheo">5.2 New Theory</h3>

<p>
The logical learning approach to the problem of old evidence invites
another worry. It seems to fail to address a variant of the Mercury
Case, due to Earman (1992: sec. 5.5):</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Physics Student).</em> A physics student
just started studying Einstein&rsquo;s theory of general relativity.
Like most physics students, the first thing she learns about the
theory, even before hearing any details of the theory itself, is the
logical fact \(E_\textrm{logical}\) as formulated above. After
learning that, this student forms an <em>initial</em> credence 1 in
\(E_\textrm{logical}\), and an initial credence in the new,
Einsteinian theory. She also lowers her credence in the old, Newtonian
theory.</li>
</ul>

</div>

<p>
The student&rsquo;s <em>formation</em> of a new, initial credence in
the new theory seems to pose a relatively little threat to the
Principle of Conditionalization, which is most naturally construed as
a norm that governs, not credence formation, but credence change. So
the more serious problem lies in the student&rsquo;s <em>change</em>
of her credence in the old theory. If this credence drop really
results from conditionalization on what was just learned,
\(E_\textrm{logical}\), then the credence in \(E_\textrm{logical}\)
must be boosted to 1 from somewhere below 1, which unfortunately never
happens. So it seems that the student&rsquo;s credence drop violates
the Principle of Conditionalization and rightly so, which is known as
<em>the problem of new theory</em>. The following presents two reply
strategies for Bayesians.</p>

<p>
One reply strategy is to qualify the Conditionalization Principle and
make it weaker in order to avoid counterexamples. The following is one
way to implement this strategy (see
 <a href="supplement.html#sec-shimony">supplement F</a>
 for another one):</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">The Principle of Conditionalization (Plan/Rule
Version)</em>. It ought to be that, if one has a plan (or follows
a rule) for changing credences in the case of learning <i>E</i>, then
the plan (or rule) is to conditionalize on <i>E</i>.</li>
</ul>
</div>

<p>
Note how this version is immune from the Physics Student Case: what is
learned, \(E_\textrm{logical}\), is something entirely new to the
student, so the student simply did not have in mind a plan for
responding to \(E_\textrm{logical}\)&mdash;so the if-clause is not
satisfied. The Bayesians who adopt this version, such as van Fraassen
(1989: ch. 7), often add that one is <em>not</em> required to have a
plan for responding to any particular piece of new evidence.</p>

<p>
The plan version is independently motivated. Note that this version
puts a normative constraint on the <em>plan</em> that one has <em>at
each time</em> when one has a plan, whereas the standard version
constrains the <em>act</em> of credence change <em>across different
times</em>. So the plan version is different from the standard, act
version. But it turns out to be the former, rather then the latter,
that is supported by the major existing arguments for the Principle of
Conditionalization. See, for example, the Dutch Book argument by Lewis
(1999), the expected accuracy argument by Greaves &amp; Wallace
(2006), and the accuracy dominance argument by Briggs &amp; Pettigrew
(2020).</p>

<p>
While the plan version of the Conditionalization Principle is weak
enough to avoid the Physics Student counterexample, it might be
worried that it is too weak. There are actually two worries here. The
first worry is that the plan version is too weak because it leaves
open an important question: Even if one&rsquo;s plan for credence
change is always a plan to conditionalize on new evidence, should one
actually follow such a plan whenever new evidence is acquired? For
discussions of this issue, see Levi (1980: ch. 4), van Fraassen (1989:
ch. 7), and Titelbaum (2013a: parts III and IV). (Terminological note:
instead of &lsquo;plan&rsquo;, Levi uses &lsquo;confirmational
commitment&rsquo; and van Fraassen uses &lsquo;rule&rsquo;.) The
second worry is that the plan version is too weak
because it only avoids the problem of new theory, without giving a
positive account as to why the student&rsquo;s credence in the old
theory ought to drop.</p>

<p>
A positive account is promised by the next strategy for solving the
problem of new theory. It operates with a series of ideas. The first
idea is that, typically, a person only considers possibilities that
are not jointly exhaustive, and she only has credences
<em>conditional</em> on the set <i>C</i> of the considered
possibilities&mdash;lacking an unconditional credence in <i>C</i>
(Shimony 1970; Salmon 1990). This deviates from the standard Bayesian
view in allowing two things: credence gaps
 (<a href="#sec-ver-prob">section 3.1</a>),
 and primitive conditional credences
 (<a href="#sec-prim-condi">section 3.4</a>).
 The second idea is that the set <i>C</i> of the considered
possibilities might shrink or expand in time. It might shrink because
some of those possibilities are ruled out by new evidence, or it might
expand because a new possibility&mdash;a new theory&mdash;is taken
into consideration. The third and last idea is a diachronic norm
(sketched by Shimony 1970 and Salmon 1990, developed in detail by
Wenmackers &amp; Romeijn 2016):</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">The Principle of Generalized
Conditionalization (Considered Possibilities Version)</em>.
It ought to be that, if two possibilities are under consideration at
an earlier time and remain so at a later time, then their credence
ratio be preserved across those two times.</li>
</ul>
</div>

<p>
Here, a credence ratio has to be understood in such a way that it can
exist without any unconditional credence. To see how this is possible,
suppose for simplicity that an agent starts with two old theories as
the only possibilities under consideration, \(\mathsf{old}_1\) and
\(\mathsf{old}_2\), with a credence ratio \(1:2\) but without any
unconditional credence. This can be understood to mean that, while the
agent lacks an unconditional credence in the set \(\{\mathsf{old}_1 ,
\mathsf{old}_2\}\), she still has a conditional credence
\(\frac{1}{1+2}\) in \(\mathsf{old}_1\) given that set. Now, suppose
that this agent then thinks of a new theory: \(\mathsf{new}\). Then,
by the diachronic norm stated above, the credence ratio among
\(\mathsf{old}_1\), \(\mathsf{old}_2\), \(\mathsf{new}\) should now be
\(1:2:x\). Notice the change of this agent&rsquo;s conditional
credence in \(\mathsf{old}_1\) given the <em>varying</em> set of the
considered possibilities: it drops from \(\frac{1}{1+2}\) down to
\(\frac{1}{1+2+x}\), provided that \(x&gt;0\). Wenmackers &amp;
Romeijn (2016) argues that this is why there appears to be a drop in
the student&rsquo;s credence in the old theory&mdash;it is actually a
drop in a conditional credence given the varying set of the considered
possibilities.</p>

<p>
The above account invites a worry from the perspective of rational
choice theory. According to the standard construal of Bayesian
decision theory, the kind of doxastic state that ought to enter
decision-making is <em>unconditional</em> credence rather than
conditional credence. So Earman (1992: sec. 7.3) is led to think that what
we really need is an epistemology for <em>unconditional</em> credence,
which the above account fails to provide. A possible reply is
anticipated by some Bayesian decision theorists, such as Savage (1972:
sec. 5.5) and Harsanyi (1985). They argue that, when making a
decision, we often only have conditional credences&mdash;conditional
on a simplifying assumption that makes the decision problem in
question manageable. For other Bayesian decision theorists who follow
Savage and Harsanyi, see the references in Joyce (1999: sec. 2.6, 4.2,
5.5 and 7.1). For more on rational choice theory, see the entry on
 <a href="../decision-theory/index.html">decision theory</a>
 and the entry on
 <a href="../rationality-normative-utility/index.html">normative theories of rational choice: expected utility</a>.</p>
 

<div id="sec-jeffrey">

<h3 id="UnceLear">5.3 Uncertain Learning</h3>

<p>
When we change our credences, the Principle of Conditionalization
requires us to raise the credence in some proposition, such as the credence in the new evidence, all the way to 1. But it seems that we often have credence changes that do not accompany such as a radical
rise to certainty, as witnessed by the following case:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Mudrunner).</em> A gambler is very
confident that a certain racehorse, called Mudrunner, performs
exceptionally well on muddy courses. A look at the extremely cloudy
sky has an immediate effect on this gambler&rsquo;s opinion: an
increase in her credence in the proposition \((\textsf{muddy})\) that
the course will be muddy&mdash;an increase <em>without</em> reaching
certainty. Then this gambler raises her credence in the hypothesis
\((\textsf{win})\) that Mudrunner will win the race, but nothing
becomes fully certain. (Jeffrey 1965 [1983: sec. 11.3])</li>
</ul>
</div>

<p>
Conditionalization is too inflexible to accommodate this case.</p>

<p>
Jeffrey proposes a now-standard solution that replaces
conditionalization by a more flexible process for credence change,
called <em>Jeffrey conditionalization</em>. Recall that
conditionalization has a defining feature: it preserves the credence
ratios of the possibilities inside new evidence <i>E</i> while the
credence in <i>E</i> is raised all the way to 1. Jeffrey
conditionalization does something similar: it preserves the same
credence ratios <em>without</em> having to raise any credence to 1,
and also preserves some <em>other</em> credence ratios, i.e., the
credence ratios of the possibilities outside <i>E</i>. A simple
version of Jeffrey&rsquo;s norm can be stated informally as follows
(in the style of the tutorial
 <a href="#sec-core">section 1.2</a>):</p>
 

<div class="indent">

<ul class="hanging">

<li>

<p>
<em class="sc">The Principle of Jeffrey Conditionalization
(Simplified Version).</em> It ought to be that, if the direct experiential
impact on one&rsquo;s credences causes the credence in <i>E</i> to
rise to a real number <i>e</i> (which might be less than 1), then
one&rsquo;s credences are changed as follows:</p>

<ul class="subhang">

<li>For the possibilities inside <i>E</i>, rescale their credences
upward by a common factor so that they sum to <i>e</i>; for the
possibilities outside <i>E</i>, rescale their credences downward by a
common factor so that they sum to \(1-e\) (to obey the rule of
Sum-to-One).</li>

<li>Reset the credence in each proposition <i>H</i> by adding up the
new credences in the possibilities inside <i>H</i> (to obey the rule
of Additivity).</li>
</ul> </li>
</ul>
</div>

<p>
This reduces to standard conditionalization in the special case that
\(e = 1\). The above formulation is quite simplified; see
 <a href="supplement.html#sec-jeffrey-general">supplement G</a>
 for a general statement. This principle has been defended with a
Dutch Book argument; see Armendt (1980) and Skyrms (1984) for
discussions.</p>

<p>
Jeffrey conditionalization is flexible enough to accommodate the Mudrunner
Case. Suppose that the immediate effect of the gambler&rsquo;s
sky-looking experience is to raise the credence in \(E\), i.e. \(\Cr(\mathsf{muddy})\). One feature of Jeffrey conditionalization is that,
since certain credence ratios are required to be held constant,
one has to hold constant the conditional credences given \(E\) and
also those given \(\neg E\), such as \(\Cr(\mathsf{win} \mid
\mathsf{muddy})\) and \(\Cr(\mathsf{win} \mid \neg\mathsf{muddy})\).
The credences mentioned above can be used to express \(\Cr(\mathsf{win})\) as follows (thanks to Probabilism and the Ratio Formula):</p> 

\[\begin{multline}
        \Cr(\mathsf{win}) = \underbrace{\Cr(\mathsf{win} \mid \mathsf{muddy})}_\textrm{high, held constant} \wcdot \underbrace{\Cr(\mathsf{muddy})}_\textrm{raised} 
    \\
      {}  + 
        \underbrace{\Cr(\mathsf{win} \mid \neg\mathsf{muddy})}_\textrm{low, held constant} \wcdot \underbrace{\Cr(\neg\mathsf{muddy})}_\textrm{lowered}.
    \end{multline}\]

<p>
It seems natural to suppose that the first conditional credence is
high and the second is low, by the description of the Mudrunner Case.
The annotations in the above equation imply that \(\Cr(\mathsf{win})\)
must go up. This is how Jeffrey conditionalization accommodates the
Mudrunner Case.</p>

<p>
Although Jeffrey conditionalization is more flexible than
conditionalization, there is the worry that it is still too inflexible
due to something it inherits from conditionalization: the preservation
of certain credence ratios or conditional credences (Bacchus, Kyburg,
&amp; Thalos 1990; Weisberg 2009b). Here is an example due to Weisberg
(2009b: sec. 5):</p>

<div class="indent">

<ul class="hanging">

<li>

<p>
<em class="sc">Example (Red Jelly Bean).</em> An agent with a prior
\(\Cr_\textrm{old}\) has a look at a jelly bean. The reddish
appearance of that jelly bean has only one immediate effect on this
agent&rsquo;s credences: an increased credence in the proposition
that</p>

<dl class="sentag tag5em subhang">
<dt>\((\textsf{red})\)</dt>
<dd> there is a red jelly bean.</dd>
</dl>

<p class="subhang" style="margin-top:10pt">
Then this agent comes to have a posterior \(\Cr_\textrm{new}\). If
this agent later learns that</p>

<dl class="sentag tag5em subhang">
<dt>\((\textsf{tricky})\)</dt>
<dd> the lighting is tricky,</dd>
</dl>

<p class="subhang" style="margin-top:10pt">
her credence in the redness of the jelly bean will drop. So,</p>

<dl class="sentag tag5em subhang">
<dt>(\(a\))</dt>
<dd> \(\Cr_\textrm{new}( \textsf{red} \mid \textsf{tricky} ) &lt;
\Cr_\textrm{new}( \textsf{red} )\).</dd>
</dl>

<p class="subhang" style="margin-top:10pt">
But if, instead, the tricky lighting had been learned <em>before</em>
the look at the jelly bean, it would not have changed the credence in
the jelly bean&rsquo;s redness; that is:</p>

<dl class="sentag tag5em subhang">
<dt>(\(b\))</dt>
<dd>\(\Cr_\textrm{old}( \textsf{red} \mid \textsf{tricky} ) =
\Cr_\textrm{old}( \textsf{red} ).\)</dd>
</dl> </li>
</ul>

</div>

<p>
Yet it can be proved (with elementary probability theory) that
\(\Cr_\textrm{new}\) cannot be obtained from \(\Cr_\textrm{old}\) by a
Jeffrey conditionalization on \(\textsf{red}\) (assuming the two
conditions \((a)\) and \((b)\) in the above case, the Ratio Formula,
and that \(\Cr_\textrm{old}\) is probabilistic). See
 <a href="supplement.html#sec-proof-weisberg">supplement H</a>
 for a sketch of proof.</p>

<p>
The above example is used by Weisberg (2009b) not just to argue
against the Principle of Jeffrey Conditionalization, but also to
illustrate a more general point: that principle is in tension with an
influential thesis called <em>confirmational holism</em>, most
famously defended by Duhem (1906) and Quine (1951). Confirmational
holism says roughly that how one should revise one&rsquo;s beliefs
depends on a good deal of one&rsquo;s background opinions&mdash;such
as the opinions about the quality of the lighting, the reliability of
one&rsquo;s vision, the details of one&rsquo;s experimental setup
(which are conjoined with a tested scientific theory to predict
experimental outcomes). In reply, Konek (forthcoming) develops and
defends an even more flexible version of conditionalization, flexible
enough to be compatible with confirmational holism. For more on
confirmational holism, see the entry on
 <a href="../scientific-underdetermination/index.html">underdetermination of scientific theory</a>
 and the survey by Ivanova (2021).</p>

<p>
For a more detailed discussion of Jeffrey conditionalization, see the
surveys by Joyce (2011: sec. 3.2 and 3.3) and Weisberg (2011: sec. 3.4
and 3.5).</p>
</div>
</div>

<div id="sec-memory">

<h3 id="MemoLoss">5.4 Memory Loss</h3>

<p>
Conditionalization in the standard version preserves certainties,
which fails to accommodate cases of memory loss (Talbott 1991):</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Dinner).</em> At 6:30 PM on March 15,
1989, Bill is certain that he is having spaghetti for dinner that
night. But by March 15 of the next year, Bill has completely forgotten
what he had for dinner one year ago.</li>
</ul>
</div>

<p>
There are even putative counterexamples that appear to be
worse&mdash;with an agent who faces only the danger of memory loss
rather than actual memory loss. Here is one such example (Arntzenius
2003):</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Shangri-La).</em> A traveler has reached a
fork in the road to Shangri-La. The guardians will flip a fair coin to
determine her path. If it comes up heads, she will travel the path by
the Mountains and correctly remember that all along. If instead it
comes up tails, she will travel by the Sea&mdash;with her memory
altered upon reaching Shangri-La so that she will incorrectly remember
having traveled the path by the Mountains. So, either way, once in
Shangri-La the traveler will remember having traveled the path by the
Mountains. The guardians explain this entire arrangement to the
traveler, who believes those words with certainty. It turns out that
the coin comes up heads. So the traveler travels the path by the
Mountains and has credence 1 that she does. But once she reaches
Shangri-La and recalls the guardians&rsquo; words, that credence
suddenly drops from 1 down to 0.5.</li>
</ul>

</div>

<p>
That credence drop violates the Principle of Conditionalization, and
all that happens without any actual loss of memory.</p>

<p>
It may be replied that conditionalization can be plausibly generalized
to accommodate the above case. Here is an attempt made by Titelbaum
(2013a: ch. 6), who develops an idea that can be traced back to Levi
(1980: sec. 4.3):</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">The Principle of Generalized
Conditionalization (Certainties Version).</em> It ought to be
that, if two considered possibilities each entail one&rsquo;s
certainties at an earlier time and continue to do so at a later time,
then their credence ratio are preserved across those two times.</li>
</ul>
</div>

<p>
This norm allows the set of one&rsquo;s certainties to expand or
shrink, while incorporating the core idea of conditionalization:
preservation of credence ratios. To see how this norm accommodates the
Shangri-La Case, assume for simplicity that the traveler starts at the
initial time with a set of certainties, which expands upon seeing the
coin toss result at a later time, but shrinks back to the
<em>original</em> set of certainties upon reaching Shangri-La at the
final time. Note that there is no change in one&rsquo;s certainties
across the initial time and the final time. So, by the above norm,
one&rsquo;s credences at the final time (upon reaching Shangri-La)
should be identical to those at the initial time (the start of the
trip). In particular, one&rsquo;s final credence in traveling the path
by the Mountains should be the same as the initial credence, which is
0.5. For more on the attempts to save conditionalization from cases of
actual or potential memory loss, see Meacham (2010), Moss (2012), and
Titelbaum (2013a: ch. 6 and 7).</p>

<p>
The Principle of Generalized Conditionalization, as stated above,
might be thought to be an incomplete diachronic norm because it leaves
open the question of how one&rsquo;s certainties ought to change.
Early attempts at a positive answer are due to Harper (1976, 1978) and
Levi (1980: ch. 1&ndash;4). Their ideas are developed independently of
the issue of memory loss, but are motivated by the scenarios in which an
agent finds a need to revise or even retract what she used to take to
be her evidence. Although Harper&rsquo;s and Levi&rsquo;s approaches
are not identical, they share the common idea that one&rsquo;s
certainties ought to change under the constraint of certain diachronic
 axioms, now known as the <em>AGM axioms</em> in the belief revision literature.<sup>[<a href="notes.html#note-9" id="ref-9">9</a>]</sup>
 For some reasons against the Harper-Levi approach to norms of
certainty change, see Titelbaum (2013a: sec. 7.4.1).</p>

<h3 id="SelfLocaCred">5.5 Self-Locating Credences</h3>

<p>
One&rsquo;s <em>self-locating</em> credences are, for example,
credences about who one is, where one is, and what time it is. Such
credences pose some challenges to conditionalization. Let me mention
two below.</p>

<p>
To begin with, consider the following case, adapted from Titelbaum
(2013a: ch. 12):</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Writer).</em> At \(t_1\) it&rsquo;s midday
on Wednesday, and a writer is sitting in an office finishing a
manuscript for a publisher, with a deadline by the end of next day,
being certain that she only has three more sections to go. Then, at
\(t_2\), she notices that it gets dark out&mdash;in fact, she has lost
sense of time because of working too hard, and she is now only sure
that it is either Wednesday evening or early Thursday morning. She
also notices that she has only got one section done since the midday.
So the writer utters to herself: &ldquo;Now, I still have two more
sections to go&rdquo;. That is the new evidence for her to change
credences.</li>
</ul>
</div>

<p>
The problem is that it is not immediately clear what exactly is the
proposition <i>E</i> that the writer should conditionalize on. The
right <i>E</i> appears to be the proposition expressed by the
writer&rsquo;s utterance: &ldquo;Now, I still have two more sections
to go&rdquo;. And the expressed proposition must be one of the
following two candidates, depending on when the utterance is actually
made (assuming the standard account of indexicals, due to Kaplan
1989):</p>

<dl class="sentag tag3em">
<dt>\((A)\)</dt>
<dd> The writer still has two more sections to go on Wednesday
evening.</dd>
</dl>

<dl class="sentag tag3em">
<dt>\((B)\) </dt>
<dd> The writer still has two more sections to go on early Thursday
Morning.</dd>
</dl>

<p>
But, with the lost sense of time, it also seems that the writer should
conditionalize on a less informative body of evidence: the disjunction
\(A \vee B\). So exactly what should she conditionalize on? \(A\),
\(B\), or \(A \vee B\)? See Titelbaum (2016) for a survey of some
proposed solutions to this problem.</p>

<p>
While the previous problem concerns only the inputs that should be
passed to the conditionalization process, conditionalization itself is
challenged when self-locating credences meet the danger of memory
loss. Consider the following case, made popular in epistemology by
Elga (2000):</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Example (Sleeping Beauty).</em> Sleeping Beauty
participates in an experiment. She knows for sure that she will be
given a sleeping pill that induces limited amnesia. She knows for sure
that, after she falls asleep, a fair coin will be flipped. If it lands
heads, she will be awakened on Monday and asked: &ldquo;How confident
are you that the coin landed heads?&rdquo;. She will not be informed
which day it is. If the coin lands tails, she will be awaken on both
Monday and on Tuesday and asked the same question each time. The
amnesia effect is designed to ensure that, if awakened on Tuesday she
will not remember being woken on Monday. And Sleeping Beauty knows all
that for sure.</li>
</ul>
</div>

<p>
What should her answer be when she is awakened on Monday and asked how
confident she is in the coin&rsquo;s landing heads? Lewis (2001)
employs the Principle of Conditionalization to argue that the answer
is \(1/2\). His reasoning proceeds as follows: Sleeping Beauty, upon
her awakening, acquires no new evidence or acquires only a piece of
new evidence that she is already certain of, so by conditionalization
her credence in the coin&rsquo;s landing heads ought to remain the same as it was
before the sleep: \(1/2\).</p>

<p>
But Elga (2000) argues that the answer is \(1/3\) rather than \(1/2\).
If so, that will seem to be a counterexample to the Principle of
Conditionalization. Here is a sketch of his argument. Imagine that we
are Sleeping Beauty and reason as follows. We just woke up, and there
are only three possibilities on the table, regarding how the coin
landed and what day it is today:</p>

<dl class="sentag tag3em">
<dt>\((A)\)</dt>
<dd> Heads and it&rsquo;s Monday.</dd>
</dl>

<dl class="sentag tag3em">
<dt>\((B)\) </dt>
<dd> Tails and it&rsquo;s Monday.</dd>
</dl>

<dl class="sentag tag3em">
<dt>\((C)\) </dt>
<dd>Tails and it&rsquo;s Tuesday.</dd>
</dl>

<p>
If we are told that it&rsquo;s Monday (\(A \vee B\)), we will judge
that the coin&rsquo;s landing heads (\(A\)) is as probable as its
landing tails (\(B\)). So</p> 

 \[\Cr(A \mid A \vee B) = \Cr(B \mid A \vee B) = 1/2.\]

<p>
If we are told that it lands tails (\(B \vee C\)), we will judge that
today being Monday (\(B\)) and today being Tuesday (\(C\)) are equally
probable. So </p> 

\[\Cr(B \mid B \vee C) = \Cr(C \mid B \vee C) = 1/2.\]

<p>
The only way to meet the above conditions is to distribute the 
unconditional credences evenly: </p> 

\[\Cr(A) = \Cr(B) = \Cr(C) = 1/3.\]

<p>
Hence the credence in landing heads, \(A\), is equal to \(1/3\), or so
Elga concludes. This result seems to challenge the Principle of
Conditionalization, which recommends the answer \(1/2\) as explained above. For
more on the Sleeping Beauty problem, see the survey by Titelbaum
(2013b).</p>

<h3 id="BayeWithKine">5.6 Bayesianism without Kinematics</h3>

<p>
Confronted with the existing problems for the Principle of
Conditionalization, some Bayesians turn away from any diachronic norm
and develop another variety of Bayesianism: <em>time-slice
Bayesianism</em>. On this view, what credences you should (or may)
have at any particular time <em>depend solely</em> on the total
evidence you have at that same time&mdash;independently of your
earlier credences. To specify this dependency relation is to specify
exclusively synchronic norms&mdash;and to forget about diachronic norms.
Strictly speaking, there is still a diachronic norm, but it is derived
rather than fundamental: when the time flows from \(t\) to \(t'\),
your credences ought to change in a certain way&mdash;they ought to
change to the credences that you ought to have with respect to your
total evidence at the latter time \(t'\)&mdash;and the
earlier time \(t\) is to be ignored. Any diachronic norm, if correct, is at most an
epiphenomenon that arises when correct synchronic norms are applied
repeatedly across different times, according to time-slice Bayesianism.
(This view is stated above in terms of one&rsquo;s total evidence, but
that can be replaced by one&rsquo;s total reasons or information.)</p>

<p>
A particular version of this view is held by J. Williamson (2010: ch.
4), who is so firmly an objective Bayesian that he argues that the
Principle of Conditionalization should be rejected if it is in
conflict with repeated applications of certain synchronic norms, such
as Probabilism and the Principle of Maximum Entropy (which generalizes
the Principle of Indifference; see
 <a href="supplement.html#sec-max-ent">supplement D</a>).
 Time-slice Bayesianism as a general position is developed and
defended by Hedden (2015a, 2015b).</p>
</div>

<div id="sec-ideal">

<h2 id="ProbIdea">6. The Problem of Idealization</h2>

<p>
A worry about Bayesian epistemology is that the two core Bayesian
norms are so demanding that they can be followed only by highly
idealized agents&mdash;being <em>logically omniscient</em>, with
<em>precise</em> credences that always fit together
<em>perfectly</em>. This is the problem of idealization, which was
presented in the tutorial
 <a href="#sec-intro-ideal">section 1.9</a>.
 This section surveys three reply strategies for Bayesians, which
might complement each other. As will become clear below, the work on
this problem is quite interdisciplinary, with contributions from
epistemologists as well as scientists and other philosophers.</p>

<h3 id="DeIdeaUnde">6.1 De-idealization and Understanding</h3>

<p>
One reply to the problem of idealization is to look at how idealized
models are used and valued in science, and to argue that certain values of
idealization can be carried over to epistemology. When a scientist
studies a complex system, she might not really need an accurate
description of it but might rather want to pursue the following:</p>

<ol>

<li>some simplified, idealized models of the whole (such as a block
sliding on a frictionless, perfectly flat plane in vacuum);</li>

<li>gradual de-idealizations of the above (such as adding more and
more realistic considerations about friction);</li>

<li>an articulated reason why de-idealizations should be done this way
rather than another to improve upon the simpler models.</li>
</ol> 

<p>
Parts 1 and 2 do not have to be ladders that will be kicked away
once we reach a more realistic model. Instead, the three parts,
1&ndash;3, might work together to help the scientist achieve a
deeper understanding of the complex system under study&mdash;a kind of
understanding that an accurate description (alone) does not provide. The above
is one of the alleged values of idealized models in scientific
modeling; for more, see section 4.2 of the entry on
 <a href="../understanding/index.html">understanding</a>
 and the survey by Elliott-Graves and Weisberg (2014: sec. 3). Some
Bayesians have argued that certain values of idealization
are applicable not just in science but also in epistemology (Howson
2000: 173&ndash;177; Titelbaum 2013a: ch. 2&ndash;5; Schupbach 2018).
For more on the values of building more or less idealized models not
just in epistemology but generally in philosophy, see T. Williamson
(2017).</p>

<p>
The above reply to the problem of idealization has been reinforced by
a sustained project of de-idealization in Bayesian epistemology. The
following gives you the flavor of how this project may be pursued.
Let&rsquo;s start with the usual complaint that Probabilism
implies:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Strong Normalization.</em> An agent ought to assign
credence 1 to every logical truth.</li>

</ul>
</div>

<p>
The worry is that a person can meet this demand only by luck or with
an unrealistic ability&mdash;the ability to demarcate all logical
truths from the other propositions. But some Bayesians argue that the
standard version of Probabilism can be suitably de-idealized to obtain
a weak version that does not imply Strong Normalization. For example,
the extensibility version of Probabilism (discussed in
 <a href="#sec-ver-prob">section 3.1</a>)
 permits one to have credence gaps and, thus, have no credence in any
logical truth (de Finetti 1970 [1974]; Jeffrey 1983; Zynda 1996).
Indeed, the extensibility version of Probabilism only implies:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Weak Normalization.</em> It ought to be that, if an
agent has a credence in a logical truth, that credence is equal to
1.</li>
</ul>
</div>

<p>
Some Bayesians have tried to de-idealize Probabilism further, to set
it free from the commitment that any credence ought to be as sharp as
an individual real number, precise to every digit. For example, Walley
(1991: ch. 2 and 3) develops a version of Probabilism according to
which a credence is permitted to be unsharp in this way. A credence
can be bounded by one or another interval of real numbers
<em>without</em> being equal to any particular real number or any
particular interval&mdash;even the tightest bound on a credence can be
an <em>incomplete</em> description of that credence. This
interval-bound approach gives rise to a Dutch Book argument for an
even weaker version of Probabilism, which only implies:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Very Weak Normalization.</em> It ought to be that,
if an agent has a credence in a logical truth, then that credence is
bounded only by intervals that include 1.</li>
</ul>
</div>

<p>
See
 <a href="supplement.html#sec-unsharp-dutch">supplement A</a>
 for some non-technical details. For more details and related
controversies, see the survey by Mahtani (2019) and the entry on
 <a href="../imprecise-probabilities/index.html">imprecise probabilities</a>.</p>
 
<p>
The above are just some of the possible steps that might be taken in
the Bayesian project of de-idealization. There are more: Can Bayesians
provide norms for agents who can lose memories and forget what they
used to take as certain? See Meacham (2010), Moss (2012), and
Titelbaum (2013a: ch. 6 and 7) for positive accounts; also see
 <a href="#sec-memory">section 5.4</a>
 for discussion. Can Bayesians develop norms for agents who are
somewhat incoherent and incapable of being perfectly coherent? See
Staffel (2019) for a positive account. Can Bayesians provide norms
even for agents who are so cognitively underpowered that they only
have all-or-nothing beliefs without a numerical credence? See Lin
(2013) for a positive account. Can Bayesians develop norms that
explain how one may be rationally uncertain whether one is rational?
See Dorst (2020) for a positive account. Can Bayesians develop a
diachronic norm for cognitively bounded agents? See Huttegger (2017a,
2017b) for a positive account.</p>

<p>
While the project of de-idealization can be pursued gradually and
incrementally as illustrated above, Bayesians disagree about how far this
project should be pursued. Some Bayesians want to push it further:
they think that Very Weak Normalization is still too strong to be
plausible, so Probabilism needs to be abandoned altogether and
replaced by a norm that permits credences less than 1 in logical
truths. For example, Garber (1983) tries to do that for certain
logical truths; Hacking (1967) and Talbott (2016), for all logical
truths. On the other hand, Bayesians of the more traditional variety
retain a more or less de-idealized version of Probabilism, and try to
defend it by clarifying its normative content, to which I now turn.</p>

<h3 id="StriForIdea">6.2 Striving for Ideals</h3>

<p>
Probabilism is often thought to have a counterexample to this effect:
it implies that we should meet a very high standard, but it is not
the case that we should, because we cannot. In reply, some Bayesians hold that this
is actually not a counterexample, and that the apparent counterexample
can be explained away once an appropriate reading of
&lsquo;ought&rsquo; is in place and clearly distinguished from another
reading.</p>

<p>
To see that there are two readings of &lsquo;ought&rsquo;, think about
the following scenario. Suppose that this is true:</p>

<div class="indent">
<ul class="hanging">

<li> (i) We ought to launch a war now.</li>

</ul>
</div>

<p>
The truth of this particular norm might sound like a counterexample to
the general norm below:</p>

<div class="indent">
<ul class="hanging">

<li> (ii) There ought to be no war.</li>

</ul>
</div>

<p>
But perhaps there can be a context in which (i) and (ii) are both true
and hence the former is not a counterexample to the latter. An example
is the context in which we know for sure that we are able to launch a
war that ends all existing wars. Indeed, the occurrences of
&lsquo;ought&rsquo; in those two sentences seem to have very different
readings. Sentence (ii) can be understood to express a norm which
portrays what the state of the world <em>ought to be</em>
like&mdash;what the world would be like if things were <em>ideal</em>.
Such a norm is often called an <em>ought-to-be</em> norm or
<em>evaluative</em> norm, pointing to one or another ideal. On the
other hand, sentence (i) can be understood as a norm which specifies
what an agent <em>ought to do</em> in a less-than-ideal situation that
she turns out to be in&mdash;possibly with the goal to improve the
existing situation and bring it closer to the ideal specified by an
ought-to-be norm, or at least to prevent the situation from getting
worse. This kind of norm is often called an <em>ought-to-do</em> norm, a
<em>deliberative</em> norm, or a <em>prescriptive</em> norm. So,
although the truth of (i) can sound like a counterexample to (ii), the
tension between the two seems to disappear with appropriate readings
of &lsquo;ought&rsquo;.</p>

<p>
Similarly, suppose that an ordinary human has some incoherent
credences, and that it is not the case that she ought to remove the incoherence right away
because she has not detected the incoherence. The norm just stated can
be thought of as an ought-to-do norm and, hence, need not be taken as
a counterexample to Probabilism construed as an ought-to-be norm:</p>

<div class="indent">

<ul class="hanging">

<li><em class="sc">Probabilism (Ought-to-Be Version).</em> It
<em>ought to be</em> that one&rsquo;s credences fit together in the
probabilistic way.</li>
</ul>
</div>

<p>
The ought-to-be reading of &lsquo;ought&rsquo; has been employed
implicitly or explicitly to defend Bayesian norms&mdash;not just by
Bayesian philosophers (Zynda 1996; Christensen 2004: ch. 6; Titelbaum
2013a: ch. 3 and 4; Wedgwood 2014; Eder forthcoming), but also by
Bayesian psychologists (Baron 2012). The distinction between the
ought-to-be and the ought-to-do oughts is most often defended in the
broader context of normative studies, such as in deontic logic
(Casta&ntilde;eda 1970; Horty 2001: sec. 3.3 and 3.4) and in
metaethics (Broome 1999; Wedgwood 2006; Schroeder 2011).</p>

<p>
The ought-to-be construal of Probabilism still leaves us a
prescriptive issue: How should a person go about detecting and fixing
the incoherence of one&rsquo;s credences, noting that it is absurd to
strive for coherence at all costs? This is an issue about
ought-to-do/prescriptive norms, addressed by a prescriptive research
program in an area of psychology called <em>judgment and decision
making</em>. For a survey of that area, see Baron (2004, 2012) and
Elqayam &amp; Evans (2013). In fact, many psychologists even think
that, for better or worse, this prescriptive program has become the
&ldquo;new paradigm&rdquo; in the psychology of reasoning; for
references, see Elqayam &amp; Over (2013).</p>

<p>
The prescriptive issue mentioned above raises some other questions.
There is an <em>empirical, computational</em> question: What is the
extent to which a human brain can approximate the Bayesian ideal of
synchronic and diachronic coherence? See Griffiths, Kemp, &amp;
Tenenbaum (2008) for a survey of some recent results. And there are
<em>philosophical</em> questions: Why is it epistemically better for a
human&rsquo;s credences to be less incoherent? Speaking of being
<em>less</em> incoherent, how can we develop a measure of degrees of
incoherence? See de Bona &amp; Staffel (2018) and Staffel (2019) for
proposals.</p>

<h3 id="ApplEmpoIdea">6.3 Applications Empowered by Idealization</h3>

<p>
There is a third approach to the problem of idealization: to some
Bayesians, some aspects of the Bayesian idealization are to be
utilized rather than removed, because it is those aspects of
idealization that <em>empower</em> certain important applications of
Bayesian epistemology in science. Here is the idea. Consider a human
scientist confronted with an empirical problem. When some hypotheses
have been stated for consideration and some data have been collected,
there remains an inferential task&mdash;the task of inferring from the
data to one of the hypotheses. This inferential task can be done by
human scientists alone, but it has been done increasingly often this
way: by developing a computer program (in Bayesian statistics) to
simulate an idealized Bayesian agent as if that agent were hired to
perform the inferential task. The purpose of this inferential task
would be undermined if what is simulated by the computer were a
cognitively underpowered agent who mimics the limited capacities of
human agents. Howson (1992: sec. 6) suggests that this inferential
task is what Bayesian epistemology and Bayesian statistics were mainly
designed for at the early stages of their development. See Fienberg (2006)
for the historical development of Bayesian statistics.</p>

<p>
So, on the above view, idealization is essential to the existing
applications of Bayesian epistemology in science. If so, the real
issue is whether the kind of scientific inquiry <em>empowered by
Bayesian idealization</em> serves the purpose of the inferential task
better than do the non-Bayesian rivals, such as so-called
<em>frequentism</em> and <em>likelihoodism</em> in statistics. For a
critical comparison of those three schools of thought about
statistical inference, see Sober (2008: ch. 1), Hacking (2016), and
the entry on
 <a href="../statistics/index.html">philosophy of statistics</a>.
 For an introduction to both Bayesian statistics and frequentist
statistics written for philosophers, see Howson &amp; Urbach (2006:
ch. 5&ndash;8).</p>
</div>

<div id="sec-closing">

<h2 id="ClosExpaTerrBaye">7. Closing: The Expanding Territory of Bayesianism</h2>

<p>
Bayesian epistemology, despite the problems presented above, has been
expanding its scope of application. In addition to the more standard,
older areas of application listed in
 <a href="#sec-app">section 1.3</a>,
 the newer ones can be found in the entry on
 <a href="../epistemic-self-doubt/index.html">epistemic self-doubt</a>,
 sections 5.1 and 5.4 of the entry on
 <a href="../disagreement/index.html">disagreement</a>,
 Adler (2006 [2017]: sec. 6.3), and sections 3.6 and 4 of the entry on
 <a href="../epistemology-social/index.html">social epistemology</a>.</p>
 
<p>
In their more recent works, Bayesians have also started to contribute to some
epistemological issues that have traditionally been among the most
central concerns for many non-Bayesians, especially for those immersed
in the epistemology of all-or-nothing beliefs. I wish to close by
giving four groups of examples.</p>

<ol type="I">

<li> <em class="sc">Skeptical Challenges</em>: Central to
traditional epistemology is the issue of how to address certain
skeptical challenges. The Cartesian skeptic thinks that we are not
justified in believing that we are not a brain in a vat. Huemer (2016)
and Shogenji (2018) have each developed a Bayesian argument against
this variety of skepticism. There is also the Pyrrhonian skeptic, who
holds the view that no belief can be justified due to the regress
problem of justification: once a belief is justified with a reason,
that reason is in need of justification, too, which kickstarts a
regress. An attempt to reply to this skeptic quickly leads to a
difficult choice among three positions: first, foundationalism
(roughly, that the regress can be stopped); second, coherentism
(roughly, that it is permissible for the regress of justifications to be
circular); and third, infinitism (roughly, that it is permissible for the
regress of justifications to extend <em>ad infinitum</em>). To that
issue Bayesians have made some contributions. For example, White
(2006) develops a Bayesian argument against an influential version of
foundationalism, followed by a reply from Weatherson (2007); for more,
see
 <a href="../formal-epistemology/index.html#FouA">section 3.2 of the entry on formal epistemology</a>.
 Klein &amp; Warfield (1994) develop a probabilistic argument against
coherentism, which initiates a debate joined by many Bayesians; for
more, see
 <a href="../justep-coherence/index.html#TruConAnaDeb">section 7 of the entry on coherentist theories of epistemic justification</a>.
 Peijnenburg (2007) defends infinitism by developing a Bayesian
version of it. For more on the Cartesian and Pyrrhonian skeptical views, see the entry on
 <a href="../skepticism/index.html">skepticism</a>.</li>
 
<li> <em class="sc">Theories of Knowledge and Justified Beliefs</em>: While
traditional epistemologists praise knowledge and have extensively
studied what turns a belief into knowledge, Moss (2013, 2018) develops
a Bayesian counterpart: she argues that a credence can also be
knowledge-like, a property that can be studied by Bayesians.
Traditional epistemology also features a number of competing accounts
of justified belief, and the possibilities of their Bayesian
counterparts have been explored by Dunn (2015) and Tang (2016). For
more on the prospects of such Bayesian counterparts, see H&aacute;jek and
Lin (2017).</li>

<li> <em class="sc">The Scientific Realism/Anti-Realism Debate</em>: One of the
most classic debates in philosophy of science is that between scientific
realism and anti-realism. The scientific realist contends that science
pursues theories are true literally or at least approximately, while
the anti-realist denies that. An early contribution to this debate is
van Fraassen&rsquo;s (1989: part II) Bayesian argument against
inference to the best explanation (IBE), which is often used by
scientific realists to defend their view. Some Bayesians have joined
the debate and try to save IBE instead; see sections 3.1 and 4 of the
entry on
 <a href="../abduction/index.html">abduction</a>.
 Another influential defense of scientific realism proceeds with the
so-called <em>no-miracle argument</em>. (This argument runs roughly as
follows: scientific realism is correct because it is the only
philosophical view that does not render the success of science a
miracle.) Howson (2000: ch. 3) and Magnus &amp; Callender (2004)
maintain that the no-miracle argument commits a fallacy that can be
made salient from a Bayesian perspective. In reply, Sprenger &amp;
Hartmann (2019: ch. 5) contend that Bayesian epistemology makes
possible a better version of the no-miracle argument for scientific
realism. An anti-realist view is instrumentalism, which says that
science only need to pursue theories that are useful for making
observable predictions. Vassend (forthcoming) argues that
conditionalization can be generalized in a way that caters to both the
scientific realist and the instrumentalist&mdash;regardless of whether
evidence should be utilized in science to help us pursue truth or
usefulness.</li>

<li> <em class="sc">Frequentist Concerns</em>: Frequentists about statistical
inference design inference procedures for the purposes of, say,
testing a working hypothesis, identifying the truth among a set of
competing hypotheses, or producing accurate estimates of certain
quantities. And they want to design procedures that infer
<em>reliably</em>&mdash;with a low objective, physical chance of
making errors. Those concerns have been incorporated into Bayesian
statistics, leading to the Bayesian counterparts of some frequentist
accounts. In fact, those results have already appeared in standard
textbooks on Bayesian statistics, such as the influential one by
Gelman et al. (2014: sec. 4.4 and ch. 6). The line between frequentist
and Bayesian statistics is blurring.</li>
</ol>

<p>
So, as can be seen from the many examples in I&ndash;IV, Bayesians
have been assimilating ideas and concerns from the epistemological
tradition of all-or-nothing beliefs. In fact, there have also been
attempts to develop a joint epistemology&mdash;an epistemology for
agents who have both credences and all-or-nothing beliefs at the same
time; for details, see
 <a href="../formal-belief/index.html#Bridge">section 4.2 of the entry on formal representations of belief</a>.</p>
 
<p>
It is debatable which, if any, of the above topics can be adequately addressed
in Bayesian epistemology. But Bayesians have been expanding their
territory and their momentum will surely continue.</p>
</div>

</div>

<div id="bibliography">

<h2 id="Bib">Bibliography</h2>

<ul class="hanging">

<li>Adler, Jonathan, 2006 [2017], &ldquo;Epistemological Problems of
Testimony&rdquo;, <em>The Stanford Encyclopedia of Philosophy</em>
(Winter 2017 Edition), Edward N. Zalta (ed.), first written 2006. URL
=
 &lt;<a href="https://plato.stanford.edu/archives/win2017/entries/testimony-episprob/" target="other">https://plato.stanford.edu/archives/win2017/entries/testimony-episprob/</a>&gt;.</li>
 
<li>Armendt, Brad, 1980, &ldquo;Is There a Dutch Book Argument for
Probability Kinematics?&rdquo;, <em>Philosophy of Science</em>, 47(4):
583&ndash;588. doi:10.1086/288958</li>

<li>Arntzenius, Frank, 2003, &ldquo;Some Problems for
Conditionalization and Reflection&rdquo;, <em>Journal of
Philosophy</em>, 100(7): 356&ndash;370.
doi:10.5840/jphil2003100729</li>

<li>Bacchus, Fahiem, Henry E. Kyburg Jr, and Mariam Thalos, 1990,
&ldquo;Against Conditionalization&rdquo;, <em>Synthese</em>, 85(3):
475&ndash;506. doi:10.1007/BF00484837</li>

<li>Baron, Jonathan, 2004, &ldquo;Normative Models of Judgment and
Decision Making&rdquo;, in <em>Blackwell Handbook of Judgment and
Decision Making</em>, Derek J. Koehler and Nigel Harvey (eds.),
London: Blackwell, 19&ndash;36.</li>

<li>&ndash;&ndash;&ndash;, 2012, &ldquo;The Point of Normative Models
in Judgment and Decision Making&rdquo;, <em>Frontiers in
Psychology</em>, 3: art. 577. doi:10.3389/fpsyg.2012.00577</li>

<li>Bartha, Paul, 2004, &ldquo;Countable Additivity and the de Finetti
Lottery&rdquo;, <em>The British Journal for the Philosophy of
Science</em>, 55(2): 301&ndash;321. doi:10.1093/bjps/55.2.301</li>

<li>Bayes, Thomas, 1763, &ldquo;An Essay Towards Solving a Problem in
the Doctrine of Chances&rdquo;, <em>Philosophical Transactions of the
Royal Society of London</em>, 53: 370&ndash;418. Reprinted 1958,
<em>Biometrika</em>, 45(3&ndash;4): 296&ndash;315, with G.
A. Barnard&rsquo;s &ldquo;Thomas Bayes: A Biographical Note&rdquo;,
<em>Biometrika</em>, 45(3&ndash;4): 293&ndash;295.
doi:10.1098/rstl.1763.0053 doi:10.1093/biomet/45.3-4.296
doi:10.1093/biomet/45.3-4.293 (note)</li>

<li>Belot, Gordon, 2013, &ldquo;Bayesian Orgulity&rdquo;,
<em>Philosophy of Science</em>, 80(4): 483&ndash;503.
doi:10.1086/673249</li>

<li>Berger, James, 2006, &ldquo;The Case for Objective Bayesian
Analysis&rdquo;, <em>Bayesian Analysis</em>, 1(3): 385&ndash;402.
doi:10.1214/06-BA115</li>

<li>Blackwell, David and Lester Dubins, 1962, &ldquo;Merging of
Opinions with Increasing Information&rdquo;, <em>The Annals of
Mathematical Statistics</em>, 33(3): 882&ndash;886.
doi:10.1214/aoms/1177704456</li>

<li>Bovens, Luc and Stephan Hartmann, 2004, <em>Bayesian
Epistemology</em>, Oxford: Oxford University Press.
doi:10.1093/0199269750.001.0001</li>

<li>Briggs, R.A., 2019, &ldquo;Conditionals&rdquo;, in Pettigrew and
Weisberg 2019: 543&ndash;590.</li>

<li>Briggs, R.A. and Richard Pettigrew, 2020, &ldquo;An
Accuracy-Dominance Argument for Conditionalization&rdquo;,
<em>No&ucirc;s</em>, 54(1): 162&ndash;181. doi:10.1111/nous.12258</li>

<li>Broome, John, 1999, &ldquo;Normative Requirements&rdquo;,
<em>Ratio</em>, 12(4): 398&ndash;419. doi:10.1111/1467-9329.00101</li>

<li>Carnap, Rudolf, 1945, &ldquo;On Inductive Logic&rdquo;,
<em>Philosophy of Science</em>, 12(2): 72&ndash;97.
doi:10.1086/286851</li>

<li>&ndash;&ndash;&ndash;, 1955, &ldquo;Statistical and Inductive
Probability and Inductive Logic and Science&rdquo; (leaflet),
Brooklyn, NY: Galois Institute of Mathematics and Art.</li>

<li>&ndash;&ndash;&ndash;, 1963, &ldquo;Replies and Systematic
Expositions&rdquo;, in <em>The Philosophy of Rudolf Carnap</em>, Paul
Arthur Schilpp (ed.), La Salle, IL: Open Court, 859&ndash;1013.</li>

<li>Casta&ntilde;eda, Hector-Neri, 1970, &ldquo;On the Semantics of
the Ought-to-Do&rdquo;, <em>Synthese</em>, 21(3&ndash;4):
449&ndash;468. doi:10.1007/BF00484811</li>

<li>Christensen, David, 1996, &ldquo;Dutch-Book Arguments
Depragmatized: Epistemic Consistency For Partial Believers&rdquo;,
<em>Journal of Philosophy</em>, 93(9): 450&ndash;479.
doi:10.2307/2940893</li>

<li>&ndash;&ndash;&ndash;, 2004, <em>Putting Logic in Its Place:
Formal Constraints on Rational Belief</em>, Oxford: Oxford University
Press. doi:10.1093/0199263256.001.0001</li>

<li>De Bona, Glauber and Julia Staffel, 2018, &ldquo;Why Be
(Approximately) Coherent?&rdquo;, <em>Analysis</em>, 78(3):
405&ndash;415. doi:10.1093/analys/anx159</li>

<li>de Finetti, Bruno, 1937, <em>&ldquo;La Pr&eacute;vision: Ses Lois
Logiques, Ses Sources Subjectives&rdquo;</em>, <em>Annales de
l&rsquo;institut Henri Poincar&eacute;</em>, 7(1):1&ndash;68.
Translated as &ldquo;Foresight: its Logical Laws, its Subjective
Sources&rdquo;, Henry E. .Kyburg, Jr. (trans.), in <em>Studies in
Subjective Probability</em>, Henry Ely Kyburg and Henry Edward Smokler
(eds), New York: Wiley, 1964, 97&ndash;158. Second edition,
Huntington: Robert Krieger, 1980, 53&ndash;118.</li>

<li>&ndash;&ndash;&ndash;, 1970 [1974], <em>Teoria delle
probabilit&agrave;</em>, Torino: G. Einaudi. Translated as <em>Theory
of Probability</em>, two volumes, Antonio Machi and Adrian Smith
(trans), New York: John Wiley, 1974.</li>

<li>Diaconis, Persi and David Freedman, 1986a, &ldquo;On the
Consistency of Bayes Estimates&rdquo;, <em>The Annals of
Statistics</em>, 14(1): 1&ndash;26. doi:10.1214/aos/1176349830</li>

<li>&ndash;&ndash;&ndash;, 1986b, &ldquo;Rejoinder: On the Consistency
of Bayes Estimates&rdquo;, <em>The Annals of Statistics</em>, 14(1):
63&ndash;67. doi:10.1214/aos/1176349842</li>

<li>Dorling, Jon, 1979, &ldquo;Bayesian Personalism, the Methodology
of Scientific Research Programmes, and Duhem&rsquo;s Problem&rdquo;,
<em>Studies in History and Philosophy of Science Part A</em>, 10(3):
177&ndash;187. doi:10.1016/0039-3681(79)90006-2</li>

<li>Dorst, Kevin, 2020, &ldquo;Evidence: A Guide for the
Uncertain&rdquo;, <em>Philosophy and Phenomenological Research</em>,
100(3): 586&ndash;632. doi:10.1111/phpr.12561</li>

<li>Duhem, Pierre, 1906 [1954], <em>La th&eacute;orie physique: son
objet et sa structure</em>, Paris: Chevalier &amp; Rivi&egrave;re.
Translated as <em>The Aim and Structure of Physical Theory</em>,
Philip P. Wiener (trans.), Princeton, NJ: Princeton University Press,
1954.</li>

<li>Dunn, Jeff, 2015, &ldquo;Reliability for Degrees of Belief&rdquo;,
<em>Philosophical Studies</em>, 172(7): 1929&ndash;1952.
doi:10.1007/s11098-014-0380-2</li>

<li>Earman, John (ed.), 1983, <em>Testing Scientific Theories</em>,
(Minnesota Studies in the Philosophy of Science 10), Minneapolis, MN:
University of Minnesota Press.</li>

<li>&ndash;&ndash;&ndash;, 1992, <em>Bayes or Bust? A Critical
Examination of Bayesian Confirmation Theory</em>, Cambridge, MA: MIT
Press.</li>

<li>Easwaran, Kenny, 2011, &ldquo;Bayesianism I: Introduction and
Arguments in Favor&rdquo;, <em>Philosophy Compass</em>, 6(5):
312&ndash;320. doi:10.1111/j.1747-9991.2011.00399.x</li>

<li>&ndash;&ndash;&ndash;, 2013, &ldquo;Why Countable
Additivity?&rdquo;, <em>Thought: A Journal of Philosophy</em>, 2(1):
53&ndash;61. doi:10.1002/tht3.60</li>

<li>&ndash;&ndash;&ndash;, 2014, &ldquo;Regularity and Hyperreal
Credences&rdquo;, <em>Philosophical Review</em>, 123(1): 1&ndash;41.
doi:10.1215/00318108-2366479</li>

<li>&ndash;&ndash;&ndash;, 2019, &ldquo;Conditional
Probabilities&rdquo;, in Pettigrew and Weisberg 2019:
131&ndash;198.</li>

<li>Eder, Anna-Maria, forthcoming, &ldquo;Evidential Probabilities and
Credences&rdquo;, <em>The British Journal for the Philosophy of
Science</em>, first online: 24 December 2020.
doi:10.1093/bjps/axz043</li>

<li>Elga, Adam, 2000, &ldquo;Self-Locating Belief and the Sleeping
Beauty Problem&rdquo;, <em>Analysis</em>, 60(2): 143&ndash;147.
doi:10.1093/analys/60.2.143</li>

<li>Elliott-Graves, Alkistis and Michael Weisberg, 2014,
&ldquo;Idealization&rdquo;, <em>Philosophy Compass</em>, 9(3):
176&ndash;185. doi:10.1111/phc3.12109</li>

<li>Elqayam, Shira and Jonathan St. B. T. Evans, 2013,
&ldquo;Rationality in the New Paradigm: Strict versus Soft Bayesian
Approaches&rdquo;, <em>Thinking &amp; Reasoning</em>, 19(3&ndash;4):
453&ndash;470. doi:10.1080/13546783.2013.834268</li>

<li>Elqayam, Shira and David E. Over, 2013, &ldquo;New Paradigm
Psychology of Reasoning: An Introduction to the Special Issue Edited
by Elqayam, Bonnefon, and Over&rdquo;, <em>Thinking &amp;
Reasoning</em>, 19(3&ndash;4): 249&ndash;265.
doi:10.1080/13546783.2013.841591</li>

<li>Eriksson, Lina and Alan H&aacute;jek, 2007, &ldquo;What Are
Degrees of Belief?&rdquo;, <em>Studia Logica</em>, 86(2):
183&ndash;213. doi:10.1007/s11225-007-9059-4</li>

<li>Eva, Benjamin, 2019, &ldquo;Principles of Indifference&rdquo;,
<em>The Journal of Philosophy</em>, 116(7): 390&ndash;411.
doi:10.5840/jphil2019116724</li>

<li>Eva, Benjamin and Stephan Hartmann, 2020, &ldquo;On the Origins of
Old Evidence&rdquo;, <em>Australasian Journal of Philosophy</em>,
98(3): 481&ndash;494. doi:10.1080/00048402.2019.1658210</li>

<li>Fienberg, Stephen E., 2006, &ldquo;When Did Bayesian Inference
Become &lsquo;Bayesian&rsquo;?&rdquo;, <em>Bayesian Analysis</em>,
1(1): 1&ndash;40. doi:10.1214/06-BA101</li>

<li>Fishburn, Peter C., 1986, &ldquo;The Axioms of Subjective
Probability&rdquo;, <em>Statistical Science</em>, 1(3): 335&ndash;345.
doi:10.1214/ss/1177013611</li>

<li>Fitelson, Branden, 2006, &ldquo;Inductive Logic&rdquo;, in <em>The
Philosophy of Science: An Encyclopedia</em>, Sahotra Sarkar and
Jessica Pfeifer (eds), New York: Routledge, 384&ndash;394.</li>

<li>Fitelson, Branden and Andrew Waterman, 2005, &ldquo;Bayesian
Confirmation and Auxiliary Hypotheses Revisited: A Reply to
Strevens&rdquo;, <em>The British Journal for the Philosophy of
Science</em>, 56(2): 293&ndash;302. doi:10.1093/bjps/axi117</li>

<li>Foley, Richard, 1992, <em>Working without a Net: A Study of
Egocentric Epistemology</em>, New York: Oxford University Press.</li>

<li>Forster, Malcolm R., 1995, &ldquo;Bayes and Bust: Simplicity as a
Problem for a Probabilist&rsquo;s Approach to Confirmation&rdquo;,
<em>The British Journal for the Philosophy of Science</em>, 46(3):
399&ndash;424. doi:10.1093/bjps/46.3.399</li>

<li>Forster, Malcolm and Elliott Sober, 1994, &ldquo;How to Tell When
Simpler, More Unified, or Less <i>Ad Hoc</i> Theories Will Provide
More Accurate Predictions&rdquo;, <em>The British Journal for the
Philosophy of Science</em>, 45(1): 1&ndash;35.
doi:10.1093/bjps/45.1.1</li>

<li>Freedman, David A., 1963, &ldquo;On the Asymptotic Behavior of
Bayes&rsquo; Estimates in the Discrete Case&rdquo;, <em>The Annals of
Mathematical Statistics</em>, 34(4): 1386&ndash;1403.
doi:10.1214/aoms/1177703871</li>

<li>Gabbay, Dov M., Stephan Hartman, and John Woods (eds),
2011, <em>Handbook of the History of Logic, Volume 10: Inductive
Logic</em>, Boston: Elsevier. </li>

<li>Gaifman, Haim, 1986, &ldquo; A Theory of Higher Order
Probabilities&rdquo;, <em>Proceedings of the 1986 Conference on
Theoretical Aspects of Reasoning about Knowledge</em>, San Francisco:
Morgan Kaufmann Publishers, 275&ndash;292.</li>

<li>Gaifman, Haim and Marc Snir, 1982, &ldquo;Probabilities over Rich
Languages, Testing and Randomness&rdquo;, <em>Journal of Symbolic
Logic</em>, 47(3): 495&ndash;548. doi:10.2307/2273587</li>

<li>Garber, Daniel, 1983, &ldquo;Old Evidence and Logical Omniscience
in Bayesian Confirmation Theory&rdquo;, in Earman 1983: 99&ndash;131.
 [<a href="https://hdl.handle.net/11299/185350" target="other">Garber 1983 available online</a>]</li>
 
<li>Gelman, Andrew, John B. Carlin, Hal Steven Stern, David B. Dunson,
Aki Vehtari, and Donald B. Rubin, 2014, <em>Bayesian Data
Analysis</em>, third edition, (Chapman &amp; Hall/CRC Texts in
Statistical Science), Boca Raton, FL: CRC Press.</li>

<li>Gelman, Andrew and Christian Hennig, 2017, &ldquo;Beyond
Subjective and Objective in Statistics&rdquo;, <em>Journal of the
Royal Statistical Society: Series A (Statistics in Society)</em>,
180(4): 967&ndash;1033. Includes discussions of the paper.
doi:10.1111/rssa.12276</li>

<li>Gendler, Tamar Szabo and John Hawthorne (eds), 2010, <em>Oxford
Studies in Epistemology, Volume 3</em>, Oxford: Oxford University
Press.</li>

<li>Gillies, Donald, 2000, <em>Philosophical Theories of
Probability</em>, (Philosophical Issues in Science), London/New York:
Routledge.</li>

<li>Glymour, Clark N., 1980, &ldquo;Why I Am Not a Bayesian&rdquo;, in
his <em>Theory and Evidence</em>, Princeton, NJ: Princeton University
Press.</li>

<li>Good, Irving John, 1976, &ldquo;The Bayesian Influence, or How to
Sweep Subjectivism under the Carpet&rdquo;, in <em>Foundations of
Probability Theory, Statistical Inference, and Statistical Theories of
Science</em>, William Leonard Harper and Clifford Alan Hooker (eds.),
Dordrecht: Springer Netherlands, 125&ndash;174. Reprinted in his
<em>Good Thinking: The Foundations of Probability and Its
Applications</em>, Minneapolis, MN: University of Minnesota Press,
22&ndash;58. doi:10.1007/978-94-010-1436-6_5</li>

<li>Goodman, Nelson, 1955, <em>Fact, Fiction, and Forecast</em>,
Cambridge, MA: Harvard University Press.</li>

<li>Greaves, Hilary and David Wallace, 2006, &ldquo;Justifying
Conditionalization: Conditionalization Maximizes Expected Epistemic
Utility&rdquo;, <em>Mind</em>, 115(459): 607&ndash;632.
doi:10.1093/mind/fzl607</li>

<li>Griffiths, Thomas L., Charles Kemp, and Joshua B. Tenenbaum, 2008,
&ldquo;Bayesian Models of Cognition&rdquo;, in <em>The Cambridge
Handbook of Computational Psychology</em>, Ron Sun (ed.), Cambridge:
Cambridge University Press, 59&ndash;100.
doi:10.1017/CBO9780511816772.006</li>

<li>Hacking, Ian, 1967, &ldquo;Slightly More Realistic Personal
Probability&rdquo;, <em>Philosophy of Science</em>, 34(4):
311&ndash;325. doi:10.1086/288169</li>

<li>&ndash;&ndash;&ndash;, 2001, <em>An Introduction to Probability
and Inductive Logic</em>, Cambridge: Cambridge University Press.
doi:10.1017/CBO9780511801297</li>

<li>&ndash;&ndash;&ndash;, 2016, <em>Logic of Statistical
Inference</em>, Cambridge: Cambridge University Press.
doi:10.1017/CBO9781316534960</li>

<li>H&aacute;jek, Alan, 2003, &ldquo;What Conditional Probability
Could Not Be&rdquo;, <em>Synthese</em>, 137(3): 273&ndash;323.
doi:10.1023/B:SYNT.0000004904.91112.16</li>

<li>&ndash;&ndash;&ndash;, 2009, &ldquo;Dutch Book Arguments&rdquo;,
in <em>The Handbook of Rational and Social Choice</em>, Paul Anand,
Prasanta Pattanaik, and Clemens Puppe (eds.), New York: Oxford
University Press, 173&ndash;195.
doi:10.1093/acprof:oso/9780199290420.003.0008</li>

<li>&ndash;&ndash;&ndash;, 2012, &ldquo;Is Strict Coherence
Coherent?&rdquo;, <em>Dialectica</em>, 66(3): 411&ndash;424.
doi:10.1111/j.1746-8361.2012.01310.x</li>

<li>H&aacute;jek, Alan and Hanti Lin, 2017, &ldquo;A Tale of Two
Epistemologies?&rdquo;, <em>Res Philosophica</em>, 94(2):
207&ndash;232.</li>

<li>Harman, Gilbert, 1986, <em>Change in View: Principles of
Reasoning</em>, Cambridge, MA: MIT Press.</li>

<li>Harsanyi, John C., 1985, &ldquo;Acceptance of Empirical
Statements: A Bayesian Theory without Cognitive Utilities&rdquo;,
<em>Theory and Decision</em>, 18(1): 1&ndash;30.</li>

<li>Harper, William L., 1976, &ldquo;Rational Conceptual
Change&rdquo;, <em>PSA: Proceedings of the Biennial Meeting of the
Philosophy of Science Association</em>, 1976(2): 462&ndash;494.
doi:10.1086/psaprocbienmeetp.1976.2.192397</li>

<li>&ndash;&ndash;&ndash;, 1978, &ldquo;Bayesian Learning Models with
Revision of Evidence&rdquo;, <em>Philosophia</em>, 7(2):
357&ndash;367. doi:10.1007/BF02378821</li>

<li>Hartmann, Stephan and Branden Fitelson, 2015, &ldquo;A New
Garber-Style Solution to the Problem of Old Evidence&rdquo;,
<em>Philosophy of Science</em>, 82(4): 712&ndash;717.
doi:10.1086/682916</li>

<li>Haverkamp, Nick and Moritz Schulz, 2012, &ldquo;A Note on
Comparative Probability&rdquo;, <em>Erkenntnis</em>, 76(3):
395&ndash;402. doi:10.1007/s10670-011-9307-x</li>

<li>Heckerman, David, 1996 [2008], &ldquo;A Tutorial on Learning with
Bayesian Networks&rdquo;. Technical Report MSR-TR-95-06, Redmond, WA:
Microsoft Research. Reprinted in <em>Innovations in Bayesian Networks:
Theory and Applications</em>, Dawn E. Holmes and Lakhmi C. Jain
(eds.), (Studies in Computational Intelligence, 156),
Berlin/Heidelberg: Springer Berlin Heidelberg, 2008, 33&ndash;82.
doi:10.1007/978-3-540-85066-3_3</li>

<li>Hedden, Brian, 2015a, &ldquo;Time-Slice Rationality&rdquo;,
<em>Mind</em>, 124(494): 449&ndash;491. doi:10.1093/mind/fzu181</li>

<li>&ndash;&ndash;&ndash;, 2015b, <em>Reasons without Persons:
Rationality, Identity, and Time</em>, Oxford/New York: Oxford
University Press. doi:10.1093/acprof:oso/9780198732594.001.0001</li>

<li>Henderson, Leah, 2014, &ldquo;Bayesianism and Inference to the
Best Explanation&rdquo;, <em>The British Journal for the Philosophy of
Science</em>, 65(4): 687&ndash;715. doi:10.1093/bjps/axt020</li>

<li>Hitchcock, Christopher (ed.), 2004, <em>Contemporary Debates in
Philosophy of Science</em>, (Contemporary Debates in Philosophy 2),
Malden, MA: Blackwell.</li>

<li>Horgan, Terry, 2017, &ldquo;Troubles for Bayesian Formal
Epistemology&rdquo;, <em>Res Philosophica</em>, 94(2): 233&ndash;255.
doi:10.11612/resphil.1535</li>

<li>Horty, John F., 2001, <em>Agency and Deontic Logic</em>,
Oxford/New York: Oxford University Press.
doi:10.1093/0195134613.001.0001</li>

<li>Howson, Colin, 1992, &ldquo;Dutch Book Arguments and
Consistency&rdquo;, <em>PSA: Proceedings of the Biennial Meeting of
the Philosophy of Science Association</em>, 1992(2): 161&ndash;168.
doi:10.1086/psaprocbienmeetp.1992.2.192832</li>

<li>&ndash;&ndash;&ndash;, 2000, <em>Hume&rsquo;s Problem: Induction
and the Justification of Belief</em>, Oxford: Clarendon Press.</li>

<li>Howson, Colin and Peter Urbach, 2006, <em>Scientific Reasoning:
The Bayesian Approach</em>, third edition, Chicago: Open Court. First
edition, 1989.</li>

<li>Huber, Franz, 2018, <em>A Logical Introduction to Probability and
Induction</em>, New York: Oxford University Press.</li>

<li>Huemer, Michael, 2016, &ldquo;Serious Theories and Skeptical
Theories: Why You Are Probably Not a Brain in a Vat&rdquo;,
<em>Philosophical Studies</em>, 173(4): 1031&ndash;1052.
doi:10.1007/s11098-015-0539-5</li>

<li>Hume, David, 1748/1777 [2008], <em>An Enquiry Concerning Human
Understanding</em>, London. Last edition corrected by the author,
1777. 1777 edition reprinted, Peter Millican (ed.), (Oxford
World&rsquo;s Classics), New York/Oxford: Oxford University Press.
</li>

<li>Huttegger, Simon M., 2015, &ldquo;Merging of Opinions and
Probability Kinematics&rdquo;, <em>The Review of Symbolic Logic</em>,
8(4): 611&ndash;648. doi:10.1017/S1755020315000180</li>

<li>&ndash;&ndash;&ndash;, 2017a, &ldquo;Inductive Learning in Small
and Large Worlds&rdquo;, <em>Philosophy and Phenomenological
Research</em>, 95(1): 90&ndash;116. doi:10.1111/phpr.12232</li>

<li>&ndash;&ndash;&ndash;, 2017b, <em>The Probabilistic Foundations of
Rational Learning</em>, Cambridge: Cambridge University Press.
doi:10.1017/9781316335789</li>

<li>Ivanova, Milena, 2021, <em>Duhem and Holism</em>, Cambridge:
Cambridge University Press. doi:10.1017/9781009004657</li>

<li>Jaynes, Edwin T., 1957, &ldquo;Information Theory and Statistical
Mechanics&rdquo;, <em>Physical Review</em>, 106(4): 620&ndash;630.
doi:10.1103/PhysRev.106.620</li>

<li>&ndash;&ndash;&ndash;, 1968, &ldquo;Prior Probabilities&rdquo;,
<em>IEEE Transactions on Systems Science and Cybernetics</em>, 4(3):
227&ndash;241. doi:10.1109/TSSC.1968.300117</li>

<li>&ndash;&ndash;&ndash;, 1973, &ldquo;The Well-Posed Problem&rdquo;,
<em>Foundations of Physics</em>, 3(4): 477&ndash;492.
doi:10.1007/BF00709116</li>

<li>Jeffrey, Richard C., 1965 [1983], <em>The Logic of Decision</em>,
(McGraw-Hill Series in Probability and Statistics), New York:
McGraw-Hill. Second edition, Chicago: University of Chicago Press,
1983.</li>

<li>&ndash;&ndash;&ndash;, 1970, &ldquo;Dracula Meets Wolfman:
Acceptance vs. Partial Belief&rdquo;, in <em>Induction, Acceptance and
Rational Belief</em>, Marshall Swain (ed.), Dordrecht: Springer
Netherlands, 157&ndash;185. doi:10.1007/978-94-010-3390-9_8</li>

<li>&ndash;&ndash;&ndash;, 1983, &ldquo;Bayesianism with a Human
Face&rdquo;, in Earman 1983: 133&ndash;156.
 [<a href="https://conservancy.umn.edu/handle/11299/185349" target="other">Jeffrey 1983 available online</a>]</li>
 
<li>&ndash;&ndash;&ndash;, 1986, &ldquo;Probabilism and
Induction&rdquo;, <em>Topoi</em>, 5(1): 51&ndash;58.
doi:10.1007/BF00137829</li>

<li>Jeffreys, Harold, 1939, <em>Theory of Probability</em>, Oxford:
Oxford University Press.</li>

<li>&ndash;&ndash;&ndash;, 1946, &ldquo;An Invariant Form for the
Prior Probability in Estimation Problems&rdquo;, <em>Proceedings of
the Royal Society of London. Series A. Mathematical and Physical
Sciences</em>, 186(1007): 453&ndash;461.
doi:10.1098/rspa.1946.0056</li>

<li>Joyce, James M., 1998, &ldquo;A Nonpragmatic Vindication of
Probabilism&rdquo;, <em>Philosophy of Science</em>, 65(4):
575&ndash;603. doi:10.1086/392661</li>

<li>&ndash;&ndash;&ndash;, 1999, <em>The Foundations of Causal
Decision Theory</em>, Cambridge: Cambridge University Press.

doi:10.1017/CBO9780511498497</li>

<li>&ndash;&ndash;&ndash;, 2003 [2021], &ldquo;Bayes&rsquo;

Theorem&rdquo;, <em>The Stanford Encyclopedia of Philosophy</em> (Fall

2021 edition), Edward N. Zalta (ed.), URL =
 &lt;<a href="https://plato.stanford.edu/archives/fall2021/entries/bayes-theorem/" target="other">https://plato.stanford.edu/archives/fall2021/entries/bayes-theorem/</a>&gt;</li>

<li>&ndash;&ndash;&ndash;, 2005, &ldquo;How Probabilities Reflect
Evidence&rdquo;, <em>Philosophical Perspectives</em>, 19(1):
153&ndash;178. doi:10.1111/j.1520-8583.2005.00058.x</li>
 
<li>&ndash;&ndash;&ndash;, 2011, &ldquo;The Development of Subjective
Bayesianism&rdquo;, in Gabbay, Hartmann, and Woods 2011:
415&ndash;475. doi:10.1016/B978-0-444-52936-7.50012-4</li>

<li>Kaplan, David, 1989, &ldquo;Demonstratives. An Essay on the
Semantics, Logic, Metaphysics, and Epistemology of Demonstratives and
Other Indexicals&rdquo;, in <em>Themes from Kaplan</em>, Joseph Almog,
John Perry, and Howard Wettstein (eds.), New York: Oxford University
Press, 481&ndash;563.</li>

<li>Kass, Robert E. and Larry Wasserman, 1996, &ldquo;The Selection of
Prior Distributions by Formal Rules&rdquo;, <em>Journal of the
American Statistical Association</em>, 91(435): 1343&ndash;1370.</li>

<li>Kelly, Kevin T., 1996, <em>The Logic of Reliable Inquiry</em>,
(Logic and Computation in Philosophy), New York: Oxford University
Press.</li>

<li>&ndash;&ndash;&ndash;, 2000, &ldquo;The Logic of
Success&rdquo;, <em>The British Journal for the Philosophy of
Science</em>, 51(S1): 639&ndash;666. doi:10.1093/bjps/51.4.639</li>

<li>Kelly, Kevin T., and Clark Glymour, 2004, &ldquo;Why Probability
Does Not Capture the Logic of Scientific Justification&rdquo;, in
Hitchcock 2004: 94&ndash;114.</li>

<li>Kemeny, John G., 1955, &ldquo;Fair Bets and Inductive
Probabilities&rdquo;, <em>Journal of Symbolic Logic</em>, 20(3):
263&ndash;273. doi:10.2307/2268222</li>

<li>Keynes, John Maynard, 1921, <em>A Treatise on Probability</em>,
London: Macmillan.</li>

<li>Klein, Peter and Ted A. Warfield, 1994, &ldquo;What Price
Coherence?&rdquo;, <em>Analysis</em>, 54(3): 129&ndash;132.
doi:10.1093/analys/54.3.129</li>

<li>Kolmogorov, A. N., 1933, <em>Grundbegriffe der
Wahrscheinlichkeitsrechnung</em>, Berlin: Springer. Translated as
<em>Foundations of the Theory of Probability</em>, Nathan Morrison
(ed.), New York: Chelsea, 1950. Second English edition with an added
bibliography by A.T. Bharucha-Reid, New York: Chelsea, 1956. Second
edition reprinted Mineola, NY: Dover, 2018. </li>

<li>Konek, Jason, 2019, &ldquo;Comparative Probabilities&rdquo;, in
Pettigrew and Weisberg 2019: 267&ndash;348.</li>

<li>&ndash;&ndash;&ndash;, forthcoming, &ldquo;The Art of
Learning&rdquo;, in <em>Oxford Studies in Epistemology, Volume 7</em>,
Oxford: Oxford University Press.</li>

<li>Kopec, Matthew and Michael G. Titelbaum, 2016, &ldquo;The
Uniqueness Thesis&rdquo;, <em>Philosophy Compass</em>, 11(4):
189&ndash;200. doi:10.1111/phc3.12318</li>

<li>Laplace, Pierre Simon, 1814 [1902], <em>Essai philosophique sur
les probabilit&eacute;s</em>, Paris: Mme. Ve. Courcier. Translated as
<em>A Philosophical Essay on Probabilities</em>, Frederick Wilson
Truscott and Frederick Lincoln Emory (trans.), New York: J. Wiley,
1902.</li>

<li>Levi, Isaac, 1980, <em>The Enterprise of Knowledge: An Essay on
Knowledge, Credal Probability, and Chance</em>, Cambridge, MA: MIT
Press.</li>

<li>Lewis, David, 1980, &ldquo;A Subjectivist&rsquo;s Guide to
Objective Chance&rdquo;, in <em>Studies in Inductive Logic and
Probability, Volume 2</em>, R.C. Jeffrey (ed.), Berkeley, CA:
University of California Press, 263&ndash;293. Reprinted in
Lewis&rsquo;s <em>Philosophical Papers, Volume 2</em>, Oxford: Oxford
University Press, 1986, ch. 19.</li>

<li>&ndash;&ndash;&ndash;, 1999, &ldquo;Why Conditionalize?&rdquo;, in
his <em>Papers in Metaphysics and Epistemology</em>, Cambridge:
Cambridge University Press, 403&ndash;407.</li>

<li>&ndash;&ndash;&ndash;, 2001, &ldquo;Sleeping Beauty: Reply to
Elga&rdquo;, <em>Analysis</em>, 61(3): 171&ndash;176.
doi:10.1093/analys/61.3.171</li>

<li>Lin, Hanti, 2013, &ldquo;Foundations of Everyday Practical
Reasoning&rdquo;, <em>Journal of Philosophical Logic</em>, 42(6):
831&ndash;862. doi:10.1007/s10992-013-9296-0</li>

<li>&ndash;&ndash;&ndash;, forthcoming, &ldquo;Modes of Convergence to
the Truth: Steps toward a Better Epistemology of Induction&rdquo;,
<em>The Review of Symbolic Logic</em>, first online: 3 January 2022.
doi:10.1017/S1755020321000605</li>

<li>Lipton, Peter, 2004, <em>Inference to the Best Explanation</em>,
second edition, (International Library of Philosophy), London/New
York: Routledge/Taylor and Francis Group.</li>

<li>Magnus, P. D. and Craig Callender, 2004, &ldquo;Realist Ennui and
the Base Rate Fallacy&rdquo;, <em>Philosophy of Science</em>, 71(3):
320&ndash;338. doi:10.1086/421536</li>

<li>Maher, Patrick, 1992, &ldquo;Diachronic
Rationality&rdquo;, <em>Philosophy of Science</em>, 59(1):
120&ndash;141. doi:10.1086/289657</li>

<li>&ndash;&ndash;&ndash;, 2004, &ldquo;Probability Captures the Logic of
Scientific Confirmation&rdquo;, in Hitchcock 2004: 69&ndash;93.</li>

<li>Mahtani, Anna, 2019, &ldquo;Imprecise Probabilities&rdquo;, in
Pettigrew and Weisberg 2019: 107&ndash;130.</li>

<li>Meacham, Chris J.G., 2010, &ldquo;Unravelling the Tangled Web:
Continuity, Internalism, Non-uniqueness and Self-Locating
Beliefs&rdquo;, in Gendler and Hawthorne 2010: 86&ndash;125.</li>

<li>&ndash;&ndash;&ndash;, 2015, &ldquo;Understanding
Conditionalization&rdquo;, <em>Canadian Journal of Philosophy</em>,
45(5&ndash;6): 767&ndash;797. doi:10.1080/00455091.2015.1119611</li>

<li>&ndash;&ndash;&ndash;, 2016, &ldquo;Ur-Priors, Conditionalization,
and Ur-Prior Conditionalization&rdquo;, <em>Ergo, an Open Access
Journal of Philosophy</em>, 3: art. 17.
doi:10.3998/ergo.12405314.0003.017</li>

<li>Morey, Richard D., Jan-Willem Romeijn, and Jeffrey N. Rouder,
2013, &ldquo;The Humble Bayesian: Model Checking from a Fully Bayesian
Perspective&rdquo;, <em>British Journal of Mathematical and
Statistical Psychology</em>, 66(1): 68&ndash;75.
doi:10.1111/j.2044-8317.2012.02067.x</li>

<li>Moss, Sarah, 2012, &ldquo;Updating as Communication&rdquo;,
<em>Philosophy and Phenomenological Research</em>, 85(2):
225&ndash;248. doi:10.1111/j.1933-1592.2011.00572.x</li>

<li>&ndash;&ndash;&ndash;, 2013, &ldquo;Epistemology
Formalized&rdquo;, <em>Philosophical Review</em>, 122(1): 1&ndash;43.
doi:10.1215/00318108-1728705</li>

<li>&ndash;&ndash;&ndash;, 2018, <em>Probabilistic Knowledge</em>,
Oxford, United Kingdom: Oxford University Press.
doi:10.1093/oso/9780198792154.001.0001</li>

<li>Niiniluoto, Ilkka, 1983, &ldquo;Novel Facts and
Bayesianism&rdquo;, <em>The British Journal for the Philosophy of
Science</em>, 34(4): 375&ndash;379. doi:10.1093/bjps/34.4.375</li>

<li>Okasha, Samir, 2000, &ldquo;Van Fraassen&rsquo;s Critique of
Inference to the Best Explanation&rdquo;, <em>Studies in History and
Philosophy of Science Part A</em>, 31(4): 691&ndash;710.
doi:10.1016/S0039-3681(00)00016-9</li>

<li>Peijnenburg, Jeanne, 2007, &ldquo;Infinitism Regained&rdquo;,
<em>Mind</em>, 116(463): 597&ndash;602. doi:10.1093/mind/fzm597</li>

<li>Peirce, Charles Sanders, 1877, &ldquo;The Fixation of
Belief&rdquo;, <em>Popular Science Monthly</em>, 12: 1&ndash;15.
Reprinted in 1955, <em>Philosophical Writings of Peirce</em>, Justus Buchler (ed.), 
Dover Publications, 5&ndash;22.</li>

<li>&ndash;&ndash;&ndash;, 1903, &ldquo;The Three Normative
Sciences&rdquo;, fifth Harvard lecture on pragmatism delivered 30
April 1903. Reprinted in 1998, <em>The Essential Peirce, Vol. 2
(1893&ndash;1913)</em>, The Peirce Edition Project (ed.), Bloomington, IN: Indiana University Press,
196&ndash;207 (ch. 14).</li>

<li>Pettigrew, Richard, 2012, &ldquo;Accuracy, Chance, and the
Principal Principle&rdquo;, <em>Philosophical Review</em>, 121(2):
241&ndash;275. doi:10.1215/00318108-1539098</li>

<li>&ndash;&ndash;&ndash;, 2016, <em>Accuracy and the Laws of
Credence</em>, Oxford, United Kingdom: Oxford University Press.
doi:10.1093/acprof:oso/9780198732716.001.0001</li>

<li>&ndash;&ndash;&ndash;, 2020a, <em>Dutch Book Arguments</em>,
Cambridge: Cambridge University Press. doi:10.1017/9781108581813</li>

<li>&ndash;&ndash;&ndash;, 2020b, &ldquo;What Is Conditionalization,
and Why Should We Do It?&rdquo;, <em>Philosophical Studies</em>,
177(11): 3427&ndash;3463. doi:10.1007/s11098-019-01377-y</li>

<li>Pettigrew, Richard and Jonathan Weisberg (eds), 2019, <em>The Open
Handbook of Formal Epistemology</em>, PhilPapers Foundation.
 [<a href="https://philpapers.org/rec/PETTOH-2" target="other">Pettigrew and Weisberg (eds) 2019 available online</a>]</li>
 
<li>Pollock, John L., 2006, <em>Thinking about Acting: Logical
Foundations for Rational Decision Making</em>, Oxford/New York: Oxford
University Press.</li>

<li>Popper, Karl R., 1959, <em>The Logic of Scientific Discovery</em>,
New York: Basic Books. Reprinted, London: Routledge, 1992.</li>

<li>Putnam, Hilary, 1963, &ldquo;Probability and Confirmation&rdquo;,
<em>The Voice of America Forum Lectures, Philosophy of Science
Series</em>, No. 10, Washington, D.C.: United States Information
Agency, pp. 1&ndash;11. Reprinted in his <em>Mathematics, Matter, and
Method</em>, London/New York: Cambridge University Press, 1975,
293&ndash;304.</li>

<li>Quine, W. V., 1951, &ldquo;Main Trends in Recent Philosophy: Two
Dogmas of Empiricism&rdquo;, <em>The Philosophical Review</em>, 60(1):
20&ndash;43. doi:10.2307/2181906</li>

<li>Ramsey, Frank Plumpton, 1926 [1931], &ldquo;Truth and
Probability&rdquo;, manuscript. Printed in <em>Foundations of
Mathematics and Other Logical Essays</em>, R.B. Braithwaite (ed.),
London: Kegan, Paul, Trench, Trubner &amp; Co. Ltd., 1931,
156&ndash;198.</li>

<li>Rawls, John, 1971, <em>A Theory of Justice</em>, Cambridge, MA:
Harvard University Press. Revised edition 1999.</li>

<li>Reichenbach, Hans, 1938, <em>Experience and Prediction: An
Analysis of the Foundations and the Structure of Knowledge</em>,
Chicago: The University of Chicago Press.</li>

<li>R&eacute;nyi, Alfr&eacute;d, 1970, <em>Foundations of
Probability</em>, San Francisco: Holden-Day.</li>

<li>Rescorla, Michael, 2015, &ldquo;Some Epistemological Ramifications
of the Borel&ndash;Kolmogorov Paradox&rdquo;, <em>Synthese</em>, 192:
735&ndash;767.  doi:10.1007/s11229-014-0586-z</li>

<li>&ndash;&ndash;&ndash;, 2018, &ldquo;A Dutch Book Theorem and Converse
Dutch Book Theorem for Kolmogorov Conditionalization&rdquo;, <em>The
Review of Symbolic Logic</em>, 11(4): 705&ndash;735.
doi:10.1017/S1755020317000296</li>

<li>&ndash;&ndash;&ndash;, 2021, &ldquo;On the Proper Formulation of
Conditionalization&rdquo;, <em>Synthese</em>, 198(3): 1935&ndash;1965.
doi:10.1007/s11229-019-02179-9</li>

<li>Rosenkrantz, Roger D., 1981, <em>Foundations and Applications of
Inductive Probability</em>, Atascadero, CA: Ridgeview.</li>

<li>&ndash;&ndash;&ndash;, 1983, &ldquo;Why Glymour Is a
Bayesian&rdquo;, in Earman 1983: 69&ndash;97.
 [<a href="https://conservancy.umn.edu/handle/11299/185351" target="other">Rosenkrantz 1983 available online</a>]</li>
 
<li>Salmon, Wesley C., 1990, &ldquo;Rationality and Objectivity in
Science or Tom Kuhn Meets Tom Bayes&rdquo;, in <em>Scientific
Theories</em> (Minnesota Studies in the Philosophy of Science, 14), C.
W. Savage (ed.), Minneapolis, MN: University of Minnesota Press,
175&ndash;205.</li>

<li>Savage, Leonard J., 1972, <em>The Foundations of Statistics</em>,
second revised edtion, New York: Dover Publications.</li>

<li>Schoenfield, Miriam, 2014, &ldquo;Permission to Believe: Why
Permissivism Is True and What It Tells Us About Irrelevant Influences
on Belief&rdquo;, <em>No&ucirc;s</em>, 48(2):
193&ndash;218. doi:10.1111/nous.12006</li>

<li>Schroeder, Mark, 2011, &ldquo;Ought, Agents, and Actions&rdquo;,
<em>Philosophical Review</em>, 120(1): 1&ndash;41.
doi:10.1215/00318108-2010-017</li>

<li>Schupbach, Jonah N., 2018, &ldquo;Troubles for Bayesian
Formal Epistemology? A Response to Horgan&rdquo;, <em>Res
Philosophica</em>, 95(1): 189&ndash;197.
doi:10.11612/resphil.1652</li>

<li>Seidenfeld, Teddy, 1979, &ldquo;Why I Am Not an Objective
Bayesian; Some Reflections Prompted by Rosenkrantz&rdquo;, <em>Theory
and Decision</em>, 11(4): 413&ndash;440. doi:10.1007/BF00139451</li>

<li>&ndash;&ndash;&ndash;, 2001, &ldquo;Remarks on the Theory of
Conditional Probability: Some Issues of Finite Versus Countable
Additivity&rdquo;, in <em>Probability Theory: Philosophy, Recent
History and Relations to Science</em>, Vincent F. Hendricks, Stig
Andur Pedersen, and Klaus Frovin J&oslash;rgensen (eds.), (Synthese
Library 297), Dordrecht/Boston: Kluwer Academic Publishers,
167&ndash;178.</li>

<li>Shimony, Abner, 1955, &ldquo;Coherence and the Axioms of
Confirmation&rdquo;, <em>Journal of Symbolic Logic</em>, 20(1):
1&ndash;28. doi:10.2307/2268039</li>

<li>&ndash;&ndash;&ndash;, 1970, &ldquo;Scientific Inference&rdquo;,
in <em>The Nature and Function of Scientific Theories</em> (Pittsburgh
Studies in the Philosophy of Science, 4), Robert G. Colodny (ed.),
Pittsburgh, PA: University of Pittsburgh Press, 79&ndash;172.</li>

<li>Shogenji, Tomoji, 2018, <em>Formal Epistemology and Cartesian
Skepticism: In Defense of Belief in the Natural World</em>, (Routledge
Studies in Contemporary Philosophy 101), New York: Routledge, Taylor
&amp; Francis Group.</li>

<li>Skyrms, Brian, 1966 [2000], <em>Choice and Chance: An Introduction
to Inductive Logic</em>, Belmont, CA: Dickenson. Fourth edition,
Belmont, CA: Wadsworth, 2000.</li>

<li>&ndash;&ndash;&ndash;, 1984, <em>Pragmatics and Empiricism</em>,
New Haven, CT: Yale University Press.</li>

<li>Smith, Cedric A. B., 1961, &ldquo;Consistency in Statistical
Inference and Decision&rdquo;, <em>Journal of the Royal Statistical
Society: Series B (Methodological)</em>, 23(1): 1&ndash;25.
doi:10.1111/j.2517-6161.1961.tb00388.x</li>

<li>Sober, Elliott, 2002, &ldquo;Bayesianism&mdash;Its Scope and
Limits&rdquo;, in <em>Bayes&rsquo;s Theorem</em> (Proceedings of the
British Academy, 113), Richard Swinburne (ed.), Oxford: Oxford
University Press.</li>

<li>&ndash;&ndash;&ndash;, 2008, <em>Evidence and Evolution: The Logic
behind the Science</em>, Cambridge: Cambridge University Press.
doi:10.1017/CBO9780511806285</li>

<li>Sprenger, Jan, 2015, &ldquo;A Novel Solution to the Problem of Old
Evidence&rdquo;, <em>Philosophy of Science</em>, 82(3): 383&ndash;401.
doi:10.1086/681767</li>

<li>&ndash;&ndash;&ndash;, 2018, &ldquo;The Objectivity of Subjective
Bayesianism&rdquo;, <em>European Journal for Philosophy of
Science</em>, 8(3): 539&ndash;558. doi:10.1007/s13194-018-0200-1</li>

<li>Sprenger, Jan and Stephan Hartmann, 2019, <em>Bayesian Philosophy
of Science: Variations on a Theme by the Reverend Thomas Bayes</em>,
Oxford/New York: Oxford University Press.
doi:10.1093/oso/9780199672110.001.0001</li>

<li>Staffel, Julia, 2019, <em>Unsettled Thoughts: A Theory of Degrees
of Rationality</em>, Oxford/New York: Oxford University Press.
doi:10.1093/oso/9780198833710.001.0001</li>

<li>Stalnaker, Robert C., 1970, &ldquo;Probability and
Conditionals&rdquo;, <em>Philosophy of Science</em>, 37(1):
64&ndash;80. doi:10.1086/288280</li>

<li>Stef&aacute;nsson, H. Orri, 2017, &ldquo;What Is
&lsquo;Real&rsquo; in Probabilism?&rdquo;, <em>Australasian Journal of
Philosophy</em>, 95(3): 573&ndash;587.
doi:10.1080/00048402.2016.1224906</li>

<li>Strevens, Michael, 2001, &ldquo;The Bayesian Treatment of
Auxiliary Hypotheses&rdquo;, <em>The British Journal for the
Philosophy of Science</em>, 52(3): 515&ndash;537.
doi:10.1093/bjps/52.3.515</li>

<li>Talbott, William J., 1991, &ldquo;Two Principles of Bayesian
Epistemology&rdquo;, <em>Philosophical Studies</em>, 62(2):
135&ndash;150. doi:10.1007/BF00419049</li>

<li>&ndash;&ndash;&ndash;, 2016, &ldquo;A Non-Probabilist Principle of
Higher-Order Reasoning&rdquo;, <em>Synthese</em>, 193(10):
3099&ndash;3145. doi:10.1007/s11229-015-0922-y</li>

<li>Tang, Weng Hong, 2016, &ldquo;Reliability Theories of Justified
Credence&rdquo;, <em>Mind</em>, 125(497): 63&ndash;94.
doi:10.1093/mind/fzv199</li>

<li>Teller, Paul, 1973, &ldquo;Conditionalization and
Observation&rdquo;, <em>Synthese</em>, 26(2): 218&ndash;258.
doi:10.1007/BF00873264</li>

<li>Titelbaum, Michael G., 2013a, <em>Quitting Certainties: A Bayesian
Framework Modeling Degrees of Belief</em>, Oxford: Oxford University
Press. doi:10.1093/acprof:oso/9780199658305.001.0001</li>

<li>&ndash;&ndash;&ndash;, 2013b, &ldquo;Ten Reasons to Care About the
Sleeping Beauty Problem&rdquo;,
<em>Philosophy Compass</em>, 8(11): 1003&ndash;1017.
doi:10.1111/phc3.12080</li>

<li>&ndash;&ndash;&ndash;, 2016, &ldquo;Self-Locating
Credences&rdquo;, in <em>The Oxford Handbook of Probability and
Philosophy</em>, Alan H&aacute;jek, and Christopher Hitchcock (eds),
Oxford: Oxford University Press, p. 666&ndash;680.</li>

<li>&ndash;&ndash;&ndash;, forthcoming, <em>Fundamentals of Bayesian
Epistemology</em>, Oxford University Press.</li>

<li>van Fraassen, Bas C., 1984, &ldquo;Belief and the Will&rdquo;,
<em>The Journal of Philosophy</em>, 81(5): 235&ndash;256.
doi:10.2307/2026388</li>

<li>&ndash;&ndash;&ndash;, 1988, &ldquo;The Problem of Old
Evidence&rdquo;, in <em>Philosophical Analysis</em>, David F. Austin
(ed.), Dordrecht: Springer Netherlands, 153&ndash;165.
doi:10.1007/978-94-009-2909-8_10</li>

<li>&ndash;&ndash;&ndash;, 1989, <em>Laws and Symmetry</em>,
Oxford/New York: Oxford University Press.
doi:10.1093/0198248601.001.0001</li>

<li>&ndash;&ndash;&ndash;, 1995, &ldquo;Belief and the Problem of
Ulysses and the Sirens&rdquo;, <em>Philosophical Studies</em>, 77(1):
7&ndash;37. doi:10.1007/BF00996309</li>

<li>Vassend, Olav Benjamin, forthcoming, &ldquo;Justifying the Norms
of Inductive Inference&rdquo;, <em>The British Journal for the
Philosophy of Science</em>, first online: 17 December 2020.
doi:10.1093/bjps/axz041</li>

<li>von Mises, Richard, 1928 [1981], <em>Wahrscheinlichkeit,
Statistik, und Wahrheit</em>, J. Springer; third German edition, 1951.
Third edition translated as <em>Probability, Statistics, and
Truth</em>, second revised edition, Hilda Geiringer (trans.), London:
George Allen &amp; Unwin, 1951. Reprinted New York: Dover, 1981.</li>

<li>Walley, Peter, 1991, <em>Statistical Reasoning with Imprecise
Probabilities</em>, London: Chapman and Hall.</li>

<li>Wasserman, Larry, 1998, &ldquo;Asymptotic Properties of
Nonparametric Bayesian Procedures&rdquo;, in <em>Practical
Nonparametric and Semiparametric Bayesian Statistics</em>, Dipak Dey,
Peter M&uuml;ller, and Debajyoti Sinha (eds.), (Lecture Notes in
Statistics 133), New York: Springer New York, 293&ndash;304.
doi:10.1007/978-1-4612-1732-9_16</li>

<li>Weatherson, Brian, 2007, &ldquo;The Bayesian and the
Dogmatist&rdquo;, <em>Proceedings of the Aristotelian Society
(Hardback)</em>, 107(1pt2): 169&ndash;185.
doi:10.1111/j.1467-9264.2007.00217.x</li>

<li>Wedgwood, Ralph, 2006, &ldquo;The Meaning of &lsquo;Ought&rsquo;
&rdquo;, <em>Oxford Studies in Metaethics, Volume 1</em>, Russ
Shafer-Landau (ed.), Oxford: Clarendon Press, 127&ndash;160.</li>

<li>&ndash;&ndash;&ndash;, 2014, &ldquo;Rationality as a Virtue:
Rationality as a Virtue&rdquo;, <em>Analytic Philosophy</em>, 55(4):
319&ndash;338. doi:10.1111/phib.12055</li>

<li>Weisberg, Jonathan, 2007, &ldquo;Conditionalization, Reflection,
and Self-Knowledge&rdquo;, <em>Philosophical Studies</em>, 135(2):
179&ndash;197. doi:10.1007/s11098-007-9073-4</li>

<li>&ndash;&ndash;&ndash;, 2009a, &ldquo;Locating IBE in the Bayesian
Framework&rdquo;, <em>Synthese</em>, 167(1): 125&ndash;143.
doi:10.1007/s11229-008-9305-y</li>

<li>&ndash;&ndash;&ndash;, 2009b, &ldquo;Commutativity or Holism? A
Dilemma for Conditionalizers&rdquo;, <em>The British Journal for the
Philosophy of Science</em>, 60(4): 793&ndash;812.
doi:10.1093/bjps/axp007</li>

<li>&ndash;&ndash;&ndash;, 2011, &ldquo;Varieties of
Bayesianism&rdquo;, in Gabbay, Hartmann, and Woods 2011:
477&ndash;551. doi:10.1016/B978-0-444-52936-7.50013-6</li>

<li>Wenmackers, Sylvia, 2019, &ldquo;Infinitesimal
Probabilities&rdquo;, in Pettigrew and Weisberg 2019:
199&ndash;265.</li>

<li>Wenmackers, Sylvia and Jan-Willem Romeijn, 2016, &ldquo;New Theory
about Old Evidence: A Framework for Open-Minded Bayesianism&rdquo;,
<em>Synthese</em>, 193(4): 1225&ndash;1250.
doi:10.1007/s11229-014-0632-x</li>

<li>White, Roger, 2006, &ldquo;Problems for Dogmatism&rdquo;,
<em>Philosophical Studies</em>, 131(3): 525&ndash;557.
doi:10.1007/s11098-004-7487-9</li>

<li>&ndash;&ndash;&ndash;, 2010, &ldquo;Evidential Symmetry and Mushy
Credence&rdquo;, in Gendler and Hawthorne 2010: 161&ndash;186.</li>

<li>Williamson, Jon, 1999, &ldquo;Countable Additivity and Subjective
Probability&rdquo;, <em>The British Journal for the Philosophy of
Science</em>, 50(3): 401&ndash;416. doi:10.1093/bjps/50.3.401</li>

<li>&ndash;&ndash;&ndash;, 2010, <em>In Defence of Objective
Bayesianism</em>, Oxford/New York: Oxford University Press.
doi:10.1093/acprof:oso/9780199228003.001.0001</li>

<li>Williamson, Timothy, 2007, &ldquo;How Probable Is an Infinite
Sequence of Heads?&rdquo;, <em>Analysis</em>, 67(3): 173&ndash;180.
doi:10.1093/analys/67.3.173</li>

<li>&ndash;&ndash;&ndash;, 2017, &ldquo;Model-Building in
Philosophy&rdquo;, in <em>Philosophy&rsquo;s Future: The Problem of
Philosophical Progress</em>, Russell Blackford and Damien Broderick
(eds.), Hoboken, NJ: Wiley, 159&ndash;171.
doi:10.1002/9781119210115.ch12</li>

<li>Yalcin, Seth, 2012, &ldquo;Bayesian Expressivism&rdquo;,
<em>Proceedings of the Aristotelian Society (Hardback)</em>,
112(2pt2): 123&ndash;160. doi:10.1111/j.1467-9264.2012.00329.x</li>

<li>Zynda, Lyle, 1996, &ldquo;Coherence as an Ideal of
Rationality&rdquo;, <em>Synthese</em>, 109(2): 175&ndash;216.
doi:10.1007/BF00413767</li>
</ul>

</div>

<div id="academic-tools">

<h2 id="Aca">Academic Tools</h2>

<blockquote>

<table class="vert-top">
<tr>
  <td>
<img src="../../symbols/sepman-icon.jpg" alt="sep man icon" />
</td>
 
 <td><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=epistemology-bayesian" target="other">How to cite this entry</a>.</td>
 </tr>
<tr>
  <td>
<img src="../../symbols/sepman-icon.jpg" alt="sep man icon" />
</td>
 
 <td><a href="https://leibniz.stanford.edu/friends/preview/epistemology-bayesian/" target="other">Preview the PDF version of this entry</a>
 at the
 <a href="https://leibniz.stanford.edu/friends/" target="other">Friends of the SEP Society</a>.</td>
 </tr>
<tr>
  <td>
<img src="../../symbols/inpho.png" alt="inpho icon" />
</td>
 
 <td><a href="https://www.inphoproject.org/entity?sep=epistemology-bayesian&amp;redirect=True" target="other">Look up topics and thinkers related to this entry</a>
 at the Internet Philosophy Ontology Project (InPhO).</td> </tr>
<tr>
  <td>
<img src="../../symbols/pp.gif" alt="phil papers icon" />
</td>
 
 <td><a href="https://philpapers.org/sep/epistemology-bayesian/" target="other">Enhanced bibliography for this entry</a>
 at
 <a href="https://philpapers.org/" target="other">PhilPapers</a>,
 with links to its database.</td> </tr>
</table>
</blockquote>

</div>

<div id="other-internet-resources">

<h2 id="Oth">Other Internet Resources</h2>

<ul>

<li>Strevens, Michael, 2017,
 <a href="http://www.strevens.org/bct/" target="other"><em>Notes on Bayesian Confirmation Theory</em></a></li>
 
<li>Weisberg, Jonathan, 2019,
 <a href="https://jonathanweisberg.org/vip/" target="other"><em>Odds &amp; Ends: Introducing Probability &amp; Decision with a Visual Emphasis</em></a>,
 Version 0.3 Beta, Open Access Publication.</li>

<li>Talbott, William, &ldquo;Bayesian Epistemology&rdquo;,
 <em>Stanford Encyclopedia of Philosophy</em> (Spring 2022 Edition),
Edward N. Zalta (ed.), URL =
 &lt;<a href="https://plato.stanford.edu/archives/spr2022/entries/epistemology-bayesian/">https://plato.stanford.edu/archives/spr2022/entries/epistemology-bayesian/</a>&gt;.
 [This was the previous entry on this topic in the <em>Stanford
Encyclopedia of Philosophy</em> &mdash; see the
<a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=epistemology-bayesian" class="plain" target="other">version history</a>.]</li>

</ul>

</div>

<div id="related-entries">

<h2 id="Rel">Related Entries</h2>

<p>

 <a href="../abduction/index.html">abduction</a> |
 <a href="../bayes-theorem/index.html">Bayes&rsquo; Theorem</a> |
 <a href="../formal-belief/index.html">belief, formal representations of</a> |
 <a href="../conditionals/index.html">conditionals</a> |
 <a href="../confirmation/index.html">confirmation</a> |
 <a href="../decision-theory/index.html">decision theory</a> |
 <a href="../disagreement/index.html">disagreement</a> |
 <a href="../dutch-book/index.html">Dutch book arguments</a> |
 <a href="../epistemic-utility/index.html">epistemic utility arguments for probabilism</a> |
 <a href="../formal-epistemology/index.html">epistemology, formal</a> |
 <a href="../epistemology-social/index.html">epistemology: social</a> |
 <a href="../induction-problem/index.html">induction: problem of</a> |
 <a href="../justep-coherence/index.html">justification, epistemic: coherentist theories of</a> |
 <a href="../logic-inductive/index.html">logic: inductive</a> |
 <a href="../logic-belief-revision/index.html">logic: of belief revision</a> |
 <a href="../prediction-accommodation/index.html">prediction versus accommodation</a> |
 <a href="../imprecise-probabilities/index.html">probabilities, imprecise</a> |
 <a href="../probability-interpret/index.html">probability, interpretations of</a> |
 <a href="../rationality-normative-utility/index.html">rational choice, normative: expected utility</a> |
 <a href="../reflective-equilibrium/index.html">reflective equilibrium</a> |
 <a href="../scientific-objectivity/index.html">scientific objectivity</a> |
 <a href="../scientific-realism/index.html">scientific realism</a> |
 <a href="../epistemic-self-doubt/index.html">self-doubt, epistemic</a> |
 <a href="../skepticism/index.html">skepticism</a> |
 <a href="../statistics/index.html">statistics, philosophy of</a> |
 <a href="../scientific-underdetermination/index.html">underdetermination, of scientific theories</a> |
 <a href="../understanding/index.html">understanding</a>

</p>

</div>

<div id="acknowledgments">

<h3>Acknowledgments</h3>

<p>
I thank Alan H&aacute;jek for his incredibly extensive, extremely
helpful comments. I thank G. J. Mattey for his long-term support and editorial assistance. I also thank William Talbott, Stephan Hartmann, Jon
Williamson, Chlo&eacute; de Canson, Maomei Wang, Ted Shear, Jeremy
Strasser, Kramer Thompson, Joshua Thong, James Willoughby, Rachel
Boddy, and Tyrus Fisher for their comments and
suggestions.</p>

<script type="text/javascript" src="local.js"></script>
<script type="text/javascript" src="../../MathJax/MathJaxb198.js?config=TeX-MML-AM_CHTML"></script>

</div>

</div><!-- #aueditable --><!--DO NOT MODIFY THIS LINE AND BELOW-->

<!-- END ARTICLE HTML -->

</div> <!-- End article-content -->

  <div id="article-copyright">
    <p>
 <a href="../../info.html#c">Copyright &copy; 2022</a> by

<br />
Hanti Lin
&lt;<a href="m&#97;ilto:ika&#37;40ucdavis&#37;2eedu"><em>ika<abbr title=" at ">&#64;</abbr>ucdavis<abbr title=" dot ">&#46;</abbr>edu</em></a>&gt;
    </p>
  </div>

</div> <!-- End article -->

<!-- NOTE: article banner is outside of the id="article" div. -->
<div id="article-banner" class="scroll-block">
  <div id="article-banner-content">
    <a href="../../fundraising/index.html">
    Open access to the SEP is made possible by a world-wide funding initiative.<br />
    The Encyclopedia Now Needs Your Support<br />
    Please Read How You Can Help Keep the Encyclopedia Free</a>
  </div>
</div> <!-- End article-banner -->

    </div> <!-- End content -->

    <div id="footer">

      <div id="footer-menu">
        <div class="menu-block">
          <h4><i class="icon-book"></i> Browse</h4>
          <ul role="menu">
            <li><a href="../../contents.html">Table of Contents</a></li>
            <li><a href="../../new.html">What's New</a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
            <li><a href="../../published.html">Chronological</a></li>
            <li><a href="../../archives/index.html">Archives</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-info-sign"></i> About</h4>
          <ul role="menu">
            <li><a href="../../info.html">Editorial Information</a></li>
            <li><a href="../../about.html">About the SEP</a></li>
            <li><a href="../../board.html">Editorial Board</a></li>
            <li><a href="../../cite.html">How to Cite the SEP</a></li>
            <li><a href="../../special-characters.html">Special Characters</a></li>
            <li><a href="../../tools/index.html">Advanced Tools</a></li>
            <li><a href="../../contact.html">Contact</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-leaf"></i> Support SEP</h4>
          <ul role="menu">
            <li><a href="../../support/index.html">Support the SEP</a></li>
            <li><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
            <li><a href="../../support/donate.html">Make a Donation</a></li>
            <li><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
          </ul>
        </div>
      </div> <!-- End footer menu -->

      <div id="mirrors">
        <div id="mirror-info">
          <h4><i class="icon-globe"></i> Mirror Sites</h4>
          <p>View this site from another server:</p>
        </div>
        <div class="btn-group open">
          <a class="btn dropdown-toggle" data-toggle="dropdown" href="https://plato.stanford.edu/">
            <span class="flag flag-usa"></span> USA (Main Site) <span class="caret"></span>
            <span class="mirror-source">Philosophy, Stanford University</span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="../../mirrors.html">Info about mirror sites</a></li>
          </ul>
        </div>
      </div> <!-- End mirrors -->
      
      <div id="site-credits">
        <p>The Stanford Encyclopedia of Philosophy is <a href="../../info.html#c">copyright &copy; 2022</a> by <a href="http://mally.stanford.edu/">The Metaphysics Research Lab</a>, Department of Philosophy, Stanford University</p>
        <p>Library of Congress Catalog Data: ISSN 1095-5054</p>
      </div> <!-- End site credits -->

    </div> <!-- End footer -->

  </div> <!-- End container -->

   <!-- NOTE: Script required for drop-down button to work (mirrors). -->
  <script>
    $('.dropdown-toggle').dropdown();
  </script>

</body>

<!-- Mirrored from seop.illc.uva.nl/entries/epistemology-bayesian/ by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 22 Jun 2022 19:46:29 GMT -->
</html>
