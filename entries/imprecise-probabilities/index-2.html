<!DOCTYPE html>
<!--[if lt IE 7]> <html class="ie6 ie"> <![endif]-->
<!--[if IE 7]>    <html class="ie7 ie"> <![endif]-->
<!--[if IE 8]>    <html class="ie8 ie"> <![endif]-->
<!--[if IE 9]>    <html class="ie9 ie"> <![endif]-->
<!--[if !IE]> --> <html> <!-- <![endif]-->

<!-- Mirrored from seop.illc.uva.nl/entries/imprecise-probabilities/index.html by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 22 Jun 2022 20:05:40 GMT -->
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>
Imprecise Probabilities (Stanford Encyclopedia of Philosophy)
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="robots" content="noarchive, noodp" />
<meta property="citation_title" content="Imprecise Probabilities" />
<meta property="citation_author" content="Bradley, Seamus" />
<meta property="citation_publication_date" content="2014/12/20" />
<meta name="DC.title" content="Imprecise Probabilities" />
<meta name="DC.creator" content="Bradley, Seamus" />
<meta name="DCTERMS.issued" content="2014-12-20" />
<meta name="DCTERMS.modified" content="2019-02-19" />

<!-- NOTE: Import webfonts using this link: -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300,600,200&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css" />

<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap-responsive.min.css" />
<link rel="stylesheet" type="text/css" href="../../css/font-awesome.min.css" />
<!--[if IE 7]> <link rel="stylesheet" type="text/css" href="../../css/font-awesome-ie7.min.css"> <![endif]-->
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/style.css" />
<link rel="stylesheet" type="text/css" media="print" href="../../css/print.css" />
<link rel="stylesheet" type="text/css" href="../../css/entry.css" />
<!--[if IE]> <link rel="stylesheet" type="text/css" href="../../css/ie.css" /> <![endif]-->
<script type="text/javascript" src="../../js/jquery-1.9.1.min.js"></script>
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>

<!-- NOTE: Javascript for sticky behavior needed on article and ToC pages -->
<script type="text/javascript" src="../../js/jquery-scrolltofixed-min.js"></script>
<script type="text/javascript" src="../../js/entry.js"></script>

<!-- SEP custom script -->
<script type="text/javascript" src="../../js/sep.js"></script>
</head>

<!-- NOTE: The nojs class is removed from the page if javascript is enabled. Otherwise, it drives the display when there is no javascript. -->
<body class="nojs article" id="pagetopright">
<div id="container">
<div id="header-wrapper">
  <div id="header">
    <div id="branding">
      <div id="site-logo"><a href="../../index.html"><img src="../../symbols/sep-man-red.png" alt="SEP home page" /></a></div>
      <div id="site-title"><a href="../../index.html">Stanford Encyclopedia of Philosophy</a></div>
    </div>
    <div id="navigation">
      <div class="navbar">
        <div class="navbar-inner">
          <div class="container">
            <button class="btn btn-navbar collapsed" data-target=".collapse-main-menu" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Menu </button>
            <div class="nav-collapse collapse-main-menu in collapse">
              <ul class="nav">
                <li class="dropdown open"><a id="drop1" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-book"></i> Browse</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop1">
                    <li><a href="../../contents.html">Table of Contents</a></li>
                    <li><a href="../../new.html">What's New</a></li>
                    <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
                    <li><a href="../../published.html">Chronological</a></li>
                    <li><a href="../../archives/index.html">Archives</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop2" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-info-sign"></i> About</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop2">
                    <li><a href="../../info.html">Editorial Information</a></li>
                    <li><a href="../../about.html">About the SEP</a></li>
                    <li><a href="../../board.html">Editorial Board</a></li>
                    <li><a href="../../cite.html">How to Cite the SEP</a></li>
                    <li><a href="../../special-characters.html">Special Characters</a></li>
                    <li><a href="../../tools/index.html">Advanced Tools</a></li>
                    <li><a href="../../contact.html">Contact</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop3" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-leaf"></i> Support SEP</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop3">
                    <li><a href="../../support/index.html">Support the SEP</a></li>
                    <li><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
                    <li><a href="../../support/donate.html">Make a Donation</a></li>
                    <li><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
                  </ul>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End navigation -->
    
    <div id="search">
      <form id="search-form" method="get" action="https://seop.illc.uva.nl/search/searcher.py">
        <input type="search" name="query" placeholder="Search SEP" />
        <div class="search-btn-wrapper"><button class="btn search-btn" type="submit"><i class="icon-search"></i></button></div>
      </form>
    </div>
    <!-- End search --> 
    
  </div>
  <!-- End header --> 
</div>
<!-- End header wrapper -->

<div id="content">

<!-- Begin article sidebar -->
<div id="article-sidebar" class="sticky">
  <div class="navbar">
    <div class="navbar-inner">
      <div class="container">
        <button class="btn btn-navbar" data-target=".collapse-sidebar" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Entry Navigation </button>
        <div id="article-nav" class="nav-collapse collapse-sidebar in collapse">
          <ul class="nav">
            <li><a href="#toc">Entry Contents</a></li>
            <li><a href="#Bib">Bibliography</a></li>
            <li><a href="#Aca">Academic Tools</a></li>
            <li><a href="https://leibniz.stanford.edu/friends/preview/imprecise-probabilities/">Friends PDF Preview <i class="icon-external-link"></i></a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=imprecise-probabilities">Author and Citation Info <i class="icon-external-link"></i></a> </li>
            <li><a href="#pagetopright" class="back-to-top">Back to Top <i class="icon-angle-up icon2x"></i></a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</div>
<!-- End article sidebar --> 

<!-- NOTE: Article content must have two wrapper divs: id="article" and id="article-content" -->
<div id="article">
<div id="article-content">

<!-- BEGIN ARTICLE HTML -->


<div id="aueditable"><!--DO NOT MODIFY THIS LINE AND ABOVE-->

<h1>Imprecise Probabilities</h1><div id="pubinfo"><em>First published Sat Dec 20, 2014; substantive revision Tue Feb 19, 2019</em></div>

<div id="preamble">

<p>

It has been argued that imprecise probabilities are a natural and
intuitive way of overcoming some of the issues with orthodox precise
probabilities.  Models of this type have a long pedigree, and interest
in such models has been growing in recent years.  This article
introduces the theory of imprecise probabilities, discusses the
motivations for their use and their possible advantages over the
standard precise model.  It then discusses some philosophical issues
raised by this model.  There is also a historical appendix which
provides an overview of some important thinkers who appear sympathetic
to imprecise probabilities.
</p>

</div>

<div id="toc">
<!--Entry Contents-->
<ul>
<li><a href="#Int">1. Introduction</a>
   <ul>
   <li><a href="#SumTer">1.1 A summary of terminology</a></li>
   <li><a href="#SomImpDis">1.2 Some important distinctions</a></li>
   </ul></li>
<li><a href="#Mot">2. Motivations</a>
   <ul>
   <li><a href="#EllDec">2.1 Ellsberg decisions</a></li>
   <li><a href="#IncInc">2.2 Incompleteness and incomparability</a></li>
   <li><a href="#WeiEviBalEvi">2.3 Weight of evidence, balance of evidence</a></li>
   <li><a href="#SusJud">2.4 Suspending judgement</a></li>
   <li><a href="#UnkCor">2.5 Unknown correlations</a></li>
   <li><a href="#NonCha">2.6 Nonprobabilistic chances</a></li>
   <li><a href="#GroBel">2.7 Group belief</a></li>
   </ul></li>
<li><a href="#PhiQueForIP">3. Philosophical questions for IP</a>
   <ul>
   <li><a href="#Dil">3.1 Dilation</a></li>
   <li><a href="#BelIne">3.2 Belief inertia</a></li>
   <li><a href="#DecMak">3.3 Decision making</a></li>
   <li><a href="#IntIP">3.4 Interpreting IP</a>
      <ul>
      <li><a href="#WhaBel">3.4.1 What is a belief?</a></li>
      <li><a href="#WhaBelX">3.4.2 What is a belief in \(X\)?</a></li>
      </ul></li>
   <li><a href="#Reg">3.5 Regress</a></li>
   <li><a href="#WhaMakGooImpBel">3.6 What makes a good imprecise belief?</a></li>
   </ul></li>
<li><a href="#Sum">4. Summary</a></li>
<li><a href="#Bib">Bibliography</a></li>
<li><a href="#Aca">Academic Tools</a></li>
<li><a href="#OthIntResSec">Other Internet Resources</a></li>
<li><a href="#Rel">Related Entries</a></li>
</ul>
<!--Entry Contents-->
<hr />
</div>

<div id="main-text">

<h2><a id="Int">1. Introduction</a></h2>

<p>Probability theory has been a remarkably fruitful theory, with
applications in almost every branch of science. In philosophy, some
important applications of probability theory go by the name
Bayesianism; this has been an extremely successful
program <span class="citation">(see for example Howson and Urbach
2006; Bovens and Hartmann 2003; Talbott 2008)</span>. But probability
theory seems to impute much richer and more determinate attitudes than
seems warranted. What should your rational degree of belief be that
global mean surface temperature will have risen by more than four
degrees by 2080? Perhaps it should be 0.75? Why not 0.75001? Why not
0.7497? Is that event more or less likely than getting at least one
head on two tosses of a fair coin? It seems there are many events
about which we can (or perhaps should) take less precise attitudes
than orthodox probability requires. Among the reasons to question the
orthodoxy, it seems that the insistence that states of belief be
represented by a single real-valued probability function is quite an
unrealistic idealisation, and one that brings with it some rather
awkward consequences that we shall discuss later. Indeed, it has long
been recognised that probability theory offers only a rather idealised
model of belief. As far back as the mid-nineteenth century, we find
George Boole saying:</p>

<blockquote>

<p>It would be unphilosophical to affirm that the strength of that
expectation, viewed as an emotion of the mind, is capable of being
referred to any numerical standard. <span class="citation">(Boole 1958
[1854]: 244)</span></p>
</blockquote>

<p>For these, and many other reasons, there is growing interest
in <em>Imprecise Probability</em> (IP) models. Broadly construed,
these are models of belief that go beyond the probabilistic orthodoxy
in one way or another.</p>

<p>IP models are used in a number of fields including:</p>
<ul>

<li>Statistics <span class="citation">(Walley 1991; Ruggeri et al. 2005; Augustin et
al. 2014)</span></li>

<li>Psychology of reasoning <span class="citation">(Pfeifer and
Kleiter 2007)</span></li>

<li>Linguistic processing of
uncertainty <span class="citation">(Wallsten and Budescu
1995)</span></li>

<li>Neurological response to ambiguity and
conflict <span class="citation">(Smithson and Pushkarskaya
2015)</span></li>

<li>Philosophy <span class="citation">(Levi 1980; Joyce 2011; Sturgeon
2008; Kaplan 1983; Kyburg 1983)</span></li>

<li>Behavioural economics <span class="citation">(Ellsberg 1961;
Camerer and Weber 1992; Smithson and Campbell 2009)</span></li>

<li>Mathematical economics <span class="citation">(Gilboa
1987)</span></li>

<li>Engineering <span class="citation">(Ferson and Ginzburg 1996; Ferson and Hajagos 2004;
Oberguggenberger 2014)</span></li>

<li>Computer science <span class="citation">(Cozman 2000; Cozman and
Walley 2005)</span></li>

<li>Scientific computing <span class="citation">(Oberkampf and Roy 2010, chapter 13)</span></li>

<li>Physics <span class="citation">(Suppes and Zanotti 1991; Hartmann
and Suppes 2010; Frigg et al. 2014)</span></li>

</ul>

<p>This article identifies a variety of motivations for IP models;
introduces various formal models that are broadly in this area; and
discusses some open problems for these frameworks. The focus will be
on formal models of belief.</p>


<h3><a id="SumTer">1.1 A summary of terminology</a></h3>

<p>Throughout the article I adopt the convention of discussing the
beliefs of an arbitrary intentional agent whom I shall call
&ldquo;you&rdquo;. Prominent advocates of IP (including Good and
Walley) adopt this convention.</p>

<p>This article is about formal models of belief and as such, there
needs to be a certain amount of formal machinery introduced. There is
a set of states \(\Omega\) which represents
the ways the world could
be. Sometimes \(\Omega\) is described as the
set of &ldquo;possible worlds&rdquo;. The objects of belief&mdash;the
things you have beliefs about&mdash;can be represented by subsets of
the set of ways the world could
be \(\Omega\). We can identify a
proposition \(X\) with the set of states which
make it true, or, with the set of possible worlds where it is true. If
you have beliefs about \(X\)
and \(Y\) then you also have beliefs about
&ldquo;\(X\cap Y\)&rdquo;,
&ldquo;\(X \cup Y\)&rdquo; and
&ldquo;\(\neg X\)&rdquo;;
&ldquo;\(X\)
and \(Y\)&rdquo;,
&ldquo;\(X\)
or \(Y\)&rdquo; and &ldquo;it is not the case
that \(X\)&rdquo; respectively. The set of
objects of belief is the power set
of \(\Omega\), or
if \(\Omega\) is infinite, some measurable
algebra of the subsets of \(\Omega\).</p>

<p>The standard view of degree of belief is that degrees of belief are
represented by real numbers and belief states by probability
functions; this is a normative requirement. Probability functions are
functions, \(p\), from the algebra of beliefs
to real numbers satisfying:</p>
<ul>
<li>\(0 = p(\emptyset) \le p(X) \le p(\Omega) = 1\)</li>
<li>If \(X\cap Y = \emptyset\) then \(p(X\cup Y) = p(X) + p(Y)\)</li>
</ul>

<p>So if your belief state or doxastic state is represented
by \(p\), then your degree of belief
in \(X\) is the value assigned
to \(X\) by \(p\);
that is, \(p(X)\).</p>

<p>Further, learning in the Bayesian model of belief is effected
by <em>conditionalisation</em>. If you learn a
proposition \(E\) (and nothing further) then
your post-learning belief in \(X\) is given
by \(p(X\mid E) = p(X\cap E)/p(E)\).</p>

<p>The alternative approach that will be the main focus of this
article is the approach that represents belief by a <em>set</em> of
probability functions instead of a single probability. So instead of
having some \(p\) represent your belief state,
you have \(P\), a set of such
functions. <span class="citation">van Fraassen (1990)</span> calls
this your <em>representor</em>, Levi calls it a <em>credal set</em>. I
will discuss various ways you might interpret the representor later
but for now we can think of it as follows. Your representor is
a <em>credal committee</em>: each probability function in it
represents the opinions of one member of a committee that,
collectively, represents your beliefs.</p>

<p>From these concepts we can define some &ldquo;summary
functions&rdquo; that are often used in discussions of imprecise
probabilities. Often, it is assumed that your degree of belief in a
proposition, \(X\), is represented
by \(P(X) = \{p(X) : p\in P \}\). I will adopt
this notational convention, with the proviso that I don&rsquo;t
take \(P(X)\) to be an adequate representation
of your degree of belief
in \(X\). Your <em>lower envelope</em>
of \(X\)
is: \(\underline{P}(X)=\inf P(X)\). Likewise,
your <em>upper envelope</em>
is \(\overline{P}(X)=\sup P(X)\). They
are <em>conjugates</em> of each other in the following
sense: \(\overline{P}(X) = 1 - \underline{P}(\neg
X)\).</p>

<p>The standard assumption about updating for sets of probabilities is
that your degree of belief in \(X\) after
learning \(E\) is given
by \(P(X\mid E) = \{p(X\mid E), p\in P, p(E)
&gt; 0\}\). Your belief state after having
learned \(E\)
is \(P(\cdot\mid E) = \{p(\cdot\mid E), p\in P,
p(E) &gt; 0\}\). That is, by the set of conditional
probabilities.</p>

<p>I would like to emphasise already that these summary
functions&mdash;\(P(\cdot)\), \(\underline{P}(\cdot)\)
and \(\overline{P}(\cdot)\)&mdash;are not
properly representative of your belief. Information is missing from
the picture. This issue will be important later, in our discussion of
dilation.</p>

<p>We shall need to talk about decision making so we shall introduce a
simple model of decisions in terms of gambles. We can view bounded
real valued functions \(f\) as
&ldquo;gambles&rdquo; that are functions from some
set \(\Omega\) to real numbers. A
gamble \(f\) pays
out \(f(\omega)\)
if \(\omega\) is the true state. We assume
that you value each further unit of this good the same (the gambles&rsquo;
pay outs are linear in utility) and you are indifferent to concerns of
risk. Your attitude to these gambles reflects your attitudes about how
likely the various contingencies in \(\Omega\)
are. That is, gambles that win big
if \(\omega\) look more attractive the more
likely you consider \(\omega\) to be. In
particular, consider the indicator
function \(I_X\) on a
proposition \(X\) which
outputs \(1\) if \(X\)
is true at the actual world and \(0\)
otherwise. These are a particular kind of gamble, and your attitude
towards them straightforwardly reflects your degree of belief in the
proposition. The more valuable you
consider \(I_X\), the more likely you
consider \(X\) to be. Call these indicator
gambles.</p>

<p>Gambles are evaluated with respect to their expected
value. Call \(E_{p}(f)\) the expected value of
gamble \(f\) with respect to
probability \(p\), and define it as: </p>

\[ {E}_p(f) = \sum_{\Omega} p(\omega) f(\omega) \]

<p>How valuable you consider \(f\) to be in
state \(\omega\) depends on how
big \(f(\omega)\) is. How important the
goodness of \(f\)
in \(\omega\) is depends on how likely the
state is, measured by \(p(\omega)\). The
expectation is then the sum of these probability-weighted
values. See <span class="citation">Briggs (2014)</span> for more
discussion of expected utility.</p>

<p>Then we define \(\mathbf{E}_{P}(f)\)
as \(\mathbf{E}_{P}(f) = \{E_{p}(f) : p\in P
\}\). That is, the set of expected values for members
of \(P\). The same proviso holds
of \(\mathbf{E}_{P}(f)\) as held
of \(P(X)\): that is, the extent to
which \(\mathbf{E}_{P}(f)\) fully represents
your attitude to the value of a gamble is open to question. I will
often drop the subscript &ldquo;\(P\)&rdquo;
when no ambiguity arises from doing so. Further technical details can
be found in the
 <a href="supplement-formal.html">formal appendix</a>.</p>

<h3><a id="SomImpDis">1.2 Some important distinctions</a></h3>

<p>There are a number of distinctions that it is important to make in
what follows.</p>

<p>An important parameter in an IP theory is the normative force the
theory is supposed to have. Is imprecision obligatory or is it merely
permissible? Is it <em>always</em> permissible/obligatory, or only
sometimes? Or we might be interested in a purely descriptive project
of characterising the credal states of actual agents, with no interest
in normative questions. This last possibility will concern us little
in this article.</p>

<p>It is also helpful to distinguish belief itself from the
elicitation of that belief and also from your introspective access to
those beliefs. The same goes for other attitudes (values, utilities
and so on). It may be that you have beliefs that are not amenable to
(precise) elicitation, in practice or even in principle. Likewise,
your introspective access to your own beliefs might be imperfect. Such
imperfections could be a source of
imprecision. <span class="citation">Bradley (2009)</span>
distinguishes many distinct sources of imperfect introspection. The
imperfection could arise from your unawareness of the prospect in
question, the boundedness of your reasoning, ignorance of relevant
contingencies, or because of conflict in your evidence or in your
values (pp. 240&ndash;241). See <span class="citation">Bradley and
Drechsler (2014)</span> for further discussion of types of
uncertainty.</p>

<p>There are a variety of aspects of a body of evidence that could
make a difference to how you ought to respond to it. We can
ask <em>how much</em> evidence there is (weight of evidence). We can
ask whether the evidence is balanced or whether it tells heavily in
favour of one hypothesis over another (balance of evidence). Evidence
can be balanced because it is <em>incomplete</em>: there simply isn&rsquo;t
enough of it. Evidence can also be balanced if it
is <em>conflicted</em>: different pieces of evidence favour different
hypotheses. We can further ask whether evidence tells us
something <em>specific</em>&mdash;like that the bias of a coin is 2/3
in favour of heads&mdash;or unspecific&mdash;like that the bias of a
coin is between 2/3 and 1 in favour of heads. This specificity should
be distinguished from vagueness or indeterminacy of evidence: that a
coin has bias <em>about</em> 2/3 is vague but specific, while that a
coin has bias definitely somewhere between 2/3 and 1 is determinate
but unspecific. Likewise, a credal state could be indeterminate,
fuzzy, or it could be unspecific, or it could be both. It seems like
determinate but unspecific belief states will be rarer than
indeterminate ones.</p>

<p>Isaac Levi <span class="citation">(1974, 1985)</span> makes a
distinction between &ldquo;imprecise&rdquo; credences and
&ldquo;indeterminate&rdquo; credences (the scare quotes are indicating
that these aren&rsquo;t uses of the terms &ldquo;imprecise&rdquo; and
&ldquo;indeterminate&rdquo; that accord with the usage I adopt in this
article). The idea is that there are two distinct kinds of belief
state that might require a move to an IP representation of belief. An
&ldquo;imprecise&rdquo; belief in Levi&rsquo;s terminology is an imperfectly
introspected or elicited belief in mine, while an
&ldquo;indeterminate&rdquo; belief is a (possibly) perfectly
introspected belief that is still indeterminate or unspecific (or
both). Levi argues that the interesting phenomenon is
&ldquo;indeterminate&rdquo; credence. <span class="citation">Walley
(1991)</span> also emphasises the distinction between cases where
there is a &ldquo;correct&rdquo; but unknown probability from cases of
&ldquo;indeterminacy&rdquo;.</p>

<p>There is a further question about the interpretation of IP that
cross-cuts the above. This is the question of whether we
understand \(P\) as a &ldquo;complete&rdquo;
or &ldquo;exhaustive&rdquo; representation of your beliefs, or whether
we take the representation to be incomplete or non-exhaustive. Let&rsquo;s
talk in terms of the betting interpretation for a moment. The
exhaustive/non-exhaustive distinction can be drawn by asking the
following question: does \(P\) capture <em>all
and only</em> your dispositions to bet or
does \(P\) only partially capture your
dispositions to bet? Walley emphasises this distinction and suggests
that most models are non-exhaustive.</p>

<p>Partly because of Levi&rsquo;s injunction to distinguish
&ldquo;imprecise&rdquo; from &ldquo;indeterminate&rdquo; belief, some
have objected to the use of the term &ldquo;imprecise
probability&rdquo;. Using the above distinction between indeterminate,
unspecific and imperfectly introspected belief, we can keep separate
the categories Levi wanted to keep separate all without using the term
&ldquo;imprecise&rdquo;. We can then use &ldquo;imprecise&rdquo; as an
umbrella term to cover all these cases of lack of
precision. Conveniently, this allows us to stay in line with the
wealth of formal work on &ldquo;Imprecise Probabilities&rdquo; which
term is used to cover cases of indeterminacy. This usage goes back at
least to Peter Walley&rsquo;s influential book <em>Statistical Reasoning
with Imprecise Probabilities</em> <span class="citation">(Walley
1991)</span>.</p>

<p>So, &ldquo;Imprecise&rdquo; is not quite right, but neither is
&ldquo;Probability&rdquo; since the formal theory of IP is really
about <em>previsions</em> (sort of expectations) rather than just
about probability (expectations of indicator functions). Helpfully, if
I abbreviate Imprecise Probability to &ldquo;IP&rdquo; then I can
exploit some useful ambiguities.</p>

<h2><a id="Mot">2. Motivations</a></h2>

<p>Let&rsquo;s consider, in general terms, what sort of motivations one
might have for adopting models that fall under the umbrella of IP. The
focus will be on models of rational belief, since these are the models
that philosophers typically focus on, although it is worth noting that
statistical work using IP isn&rsquo;t restricted to this
interpretation. Note that no one author endorses all of these
arguments, and indeed, some authors who are sympathetic to IP have
explicitly stated that they don&rsquo;t consider certain of these arguments
to be good (for example Mark Kaplan does not endorse the claim that
concerns about descriptive realism suggest allowing
incompleteness).</p>

<h3><a id="EllDec">2.1 Ellsberg decisions</a></h3>

<p>There are a number of examples of decision problems where we are
intuitively drawn to go against the prescriptions of precise
probabilism. And indeed, many experimental subjects do seem to express
preferences that violate the axioms. IP offers a way of representing
these intuitively plausible and experimentally observed choices as
rational. One classic example of this is the <em>Ellsberg
problem</em> <span class="citation">(Ellsberg 1961)</span>.</p>

<blockquote>
<p>I have an urn that contains ninety marbles. Thirty marbles are
red. The remainder are blue or yellow in some unknown proportion.</p>
</blockquote>

<p>Consider the indicator gambles for various events in this
scenario. Consider a choice between a bet that wins if the marble
drawn is red (I), versus a bet that wins if the marble drawn is blue
(II). You might prefer I to II since I involves <em>risk</em> while II
involves <em>ambiguity</em>. A prospect is risky if its outcome is
uncertain but its outcomes occur with known probability. A prospect is
ambiguous if the outcomes occur with unknown or only partially known
probabilities. Now consider a choice between a bet that wins if the
marble drawn is not blue (III) versus a bet that wins if the marble
drawn is not red (IV). Now it is III that is ambiguous, while IV is
unambiguous but risky, and thus IV might seem better to you if you
preferred risky to ambiguous prospects. Such a pattern of preferences
(I preferred to II but IV preferred to III) cannot be rationalised as
the choices of a precise expected utility maximiser. The gambles are
summarised in the table.</p>

<div class="figure" id="table1">
<table class="hrules cell-center cellpad-med-dense avoid-break">
<tr>
<td></td>
<td>R</td>
<td>B</td>
<td>Y</td>
</tr>
<tr>
<td>I</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>II</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>III</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>IV</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
</table>
<p><span class="figlabel">Table 1:</span> The Ellsberg bets. The urn
contains 30 red marbles and 60 blue/yellow marbles</p>
</div>

<p>Let the probabilities for red, blue and yellow marbles
be \(r\), \(b\)
and \(y\) respectively. If you were an
expected utility maximiser and preferred I to II,
then \(r &gt; b\) and a preference for IV over
III entails that \(r+y &lt; y +b\). No numbers
can jointly satisfy these two constraints. Therefore, no probability
function is such that an expected utility maximiser with that
probability would choose in the way described above. While by no means
universal, these preferences are a robust feature of many experimental
subjects&rsquo; response to this sort of
example <span class="citation">(Camerer and Weber 1992; Fox and
Tversky 1995)</span>. Some experiments suggest that Ellsberg-type patterns of preference are rarer than normally recognised <span class ="citation">(Binmore et al. 2012; Voorhoeve et al. 2016)</span>. For more on ambiguity attitudes, see <span class="citation">Trautmann and van der Kuilen (2016)</span>.</p>

<p>The imprecise probabilist can model the situation as
follows: \(P(R)=1/3, P(B)=P(Y)=[0,2/3]\). Note
that this expression of the belief state misses out some important
details. For example, for all \(p\in P\), we
have \(p(B)=2/3-p(Y)\). For the point being made
here, this detail is not important. Modelling the ambiguity
allows us to rationalise real agents&rsquo; preferences for bets on red. To
flesh this story out would require a lot more to be said about
decision making, (<a href="#DecMak">see section 3.3</a>) but the
intuition is that aversion to ambiguity explains the preference for I
over II and IV over III.</p>

<p>As <span class="citation">Steele (2007)</span> points out, the
above analysis rationalises the Ellsberg choices only if we are
dealing with genuinely indeterminate or unspecific beliefs. If we were
dealing with a case of imperfectly introspected belief then there
would exist some \(p\) in the representor such
that rational choices maximise \(E_{p}\). For
the Ellsberg choices, there is no
such \(p\).</p>

<p>This view on the lessons of the Ellsberg game is not
uncontroversial. <span class="citation">Al-Najjar and Weinstein
(2009)</span> offer an alternative view on the interpretation of the
Ellsberg preferences. Their view is that the distinctive pattern of
Ellsberg choices is due to agents applying certain heuristics to solve
the decisions that assume that the odds are manipulable. In real-life
situations, if someone offers you a bet, you might think that they
must have some advantage over you in order for it to be worth their
while offering you the bet. Such scepticism, appropriately modelled,
can yield the Ellsberg choices within a simple game theoretic precise
probabilistic model.</p>

<h3><a id="IncInc">2.2 Incompleteness and incomparability</a></h3>

<p>Various arguments for (precise) probabilism assume that some
relation or other is complete. Whether this is a preference over acts,
or some &ldquo;qualitative probability ordering&rdquo;, the relation
is assumed to hold one way or the other between any two elements of
the domain. This hardly seems like it should be a principle of
rationality, especially in cases of severe uncertainty. That
is&mdash;to take the preference example&mdash;it is reasonable to have
no preference in either direction. This is an importantly different
attitude to being <em>indifferent</em> between the options. Mark
Kaplan argues this point as follows:</p>

<blockquote>
<p>Both when you are indifferent between \(A\)
and \(B\) and when you are undecided
between \(A\)
and \(B\) you can be said not to prefer either
state of affairs to the other. Nonetheless, indifference and
indecision are distinct. When you are indifferent
between \(A\)
and \(B\), your failure to prefer one to the
other is born of a determination that they are equally
preferable. When you are undecided, your failure to prefer one to the
other is born of no such determination. <span class="citation">(Kaplan
1996: 5)</span></p>
</blockquote>

<p>There is a standard <em>behaviourist</em> response to the claim
that incomparability and indifference should be distinguished. In
short, the claim is that it is a distinction that cannot be inferred
from actual agents&rsquo; choice behaviour. Ultimately, in a given choice
situation you must choose one of the options. Which you choose can be
interpreted as being (weakly) preferred. Joyce offers the following
criticism of this appeal to behaviourism.</p>

<blockquote>
<p>There are just too many things worth saying that cannot be said
within the confines of strict behaviorism&hellip; The basic difficulty
here is that it is impossible to distinguish contexts in which an
agent&rsquo;s behavior really does reveal what she wants from contexts in
which it does not without appealing to additional facts about her
mental state&hellip; An even more serious shortcoming is behaviorism&rsquo;s
inability to make sense of <em>rationalizing explanations</em> of
choice behavior. <span class="citation">(Joyce 1999: 21)</span></p>
</blockquote>

<p>On top of this, behaviourists cannot make sense of the fact that
incomparable goods are <em>insensitive to small
improvements</em>. That is, if \(A\)
and \(B\) are two goods that you have no
preference between (for example, bets on propositions with imprecise
probabilities) and if \(A^+\) is a good
slightly better than \(A\), then it might
still be incomparable with \(B\). This
distinguishes incomparability from indifference, since indifference
&ldquo;ties&rdquo; will be broken by small improvements. So the claim
that there is no behavioural difference between indifference and
incomparability is false.</p>

<p>Kaplan argues that not only is violating the completeness axiom
permissible, it is, in fact, sometimes obligatory.</p>

<blockquote>
<p>[M]y reason for rejecting as falsely precise the demand that you
adopt a &hellip; set of preferences [that satisfy the preference
axioms] is not the usual one. It is not that this demand is not
humanly satisfiable. For if <em>that</em> were all that was wrong, the
demand might still play a useful role as a regulative ideal&mdash;an
ideal which might then be legitimately invoked to get you to
&ldquo;solve&rdquo; your decision problem as the orthodox Bayesian
would have you do. My complaint about the orthodox Bayesian demand is
rather that it imposes the wrong regulative ideal. For if you have
[such a] set of preferences then you have a determinate assignment of
[\(p\)] to every hypothesis&mdash;and then you
are not giving evidence its due. <span class="citation">(Kaplan 1983:
571)</span></p>
</blockquote>

<p>He notes that it is not the case that it is always unreasonable or
impossible for you to have precise beliefs: in that case precision
could serve as a regulative ideal. Precise probabilism does still
serve as something of a regulative ideal, but it is the belief of an
ideal agent <em>in an idealised evidential position</em>. Idealised
evidential positions are approximated by cases where you have a coin
of a known bias. Precise probabilists and advocates of IP both agree
that precise probabilism is an idealisation, and a regulative
ideal. However, they differ as to what kind of idealisation is
involved. Precise probabilists think that what precludes us from
having precise probabilistic beliefs is merely a lack of computational
power and introspective capacity. Imprecise probabilists think that
even agents ideal in this sense might (and possibly <em>should</em>)
fail to have precise probabilistic beliefs when they are not in an
ideal <em>evidential</em> position.</p>

<p>At least some of the axioms of preference are not normative
constraints. We can now ask what can be proved in the absence of the
&ldquo;purely structural&rdquo;&mdash;non-normative&mdash;axioms? This
surely gives us a handle on what is really required of the structure
of belief.</p>

<p>It seems permissible to fail to have a preference between two
options. Or it seems reasonable to fail to consider either of two
possibilities more likely than the other. And these failures to assent
to certain judgements is not the same as considering the two elements
under consideration to be <em>on a par</em> in any substantive
sense. That said, precise probabilism is serving as a regulative
ideal. That is, precision might still be an unattained (possibly
unattainable) goal that informs agents as to how they might improve
their credences. Completeness of preference is what the thoroughly
informed agent ought to have. Without complete preference, standard
representation theorems don&rsquo;t work. However, for
each <em>completion</em> of the incomplete preference
ordering&mdash;for each complete ordering that extends the incomplete
preference relation&mdash;the theorem follows. So if we consider the
set of probability functions that are such that some completion of the
incomplete preference is represented by that function, then we can
consider this set to be representing the beliefs associated with the
incomplete preference. We also get, for each completion, a utility
function unique up to linear transformation. This, in essence, was
Kaplan&rsquo;s position <span class="citation">(see Kaplan 1983;
1996)</span>.</p>

<p>Joyce <span class="citation">(1999: 102&ndash;4)</span> and
Jeffrey <span class="citation">(1984: 138&ndash;41)</span> both make
similar claims. A particularly detailed argument along these lines for
comparative belief can be found in <span class="citation">Hawthorne
(2009)</span>. Indeed, this idea has a long and distinguished history
that goes back at least as far as <span class="citation">B.O. Koopman
(1940)</span>. I.J Good <span class="citation">(1962)</span>, Terrence
Fine <span class="citation">(1973)</span> and Patrick
Suppes <span class="citation">(1974)</span> all discussed ideas along
these lines. <span class="citation">Seidenfeld, Schervish, and Kadane
(1995)</span> give a representation theorem for preference that don&rsquo;t
satisfy completeness. <span class="citation">(See Evren and Ok 2011;
Pedersen 2014; and Chu and Halpern 2008, 2004; for very general
representation theorems)</span>.</p>

<h3><a id="WeiEviBalEvi">2.3 Weight of evidence, balance of evidence</a></h3>

<p>Evidence influences belief. <span class="citation">Joyce
(2005)</span> suggests that there is an important difference between
the weight of evidence and the balance of evidence. He argues that
this is a distinction that precise probabilists struggle to deal with
and that the distinction is worth representing. This idea has been
hinted at by a great many thinkers including J.M. Keynes, Rudolf
Carnap, C.S. Pierce and Karl Popper <span class="citation">(see
references in Joyce 2005; G&auml;rdenfors and Sahlin 1982)</span>. Here&rsquo;s
Keynes&rsquo; articulation of the intuition:</p>

<blockquote>
<p>As the relevant evidence at our disposal increases, the magnitude
of the probability of the argument may either decrease or increase,
according as the new knowledge strengthens the unfavourable or the
favourable evidence; but <em>something</em> seems to have increased in
either case,&mdash;we have a more substantial basis upon which to rest
our conclusion. I express this by saying than an accession of new
evidence increases the <em>weight</em> of an
argument. <span class="citation">(Keynes 1921: 78, Keynes&rsquo;
emphasis)</span></p>
</blockquote>

<p>Consider tossing a coin known to be fair. Let&rsquo;s say you have seen
the outcome of a hundred tosses and roughly half have come up
heads. Your degree of belief that the coin will land heads should be
around a half. This is a case where there is weight of evidence behind
the belief.</p>

<p>Now consider another case: a coin of unknown bias is to be
tossed. That is, you have not seen any data on previous tosses. In the
absence of any relevant information about the bias, symmetry concerns
might suggest you take the chance of heads to be around a half. This
opinion is different from the above one. There is no <em>weight</em>
of evidence, but there is nothing to suggest that your attitudes
to \(H\) and \(T\)
should be different. So, on <em>balance</em>, you should have the same
belief in both.</p>

<p>However, these two different cases get represented as having the
same probabilistic belief,
namely \(p(H)=p(T)=0.5\). In the fair coin
case, this probability assignment comes from having evidence that
suggests that the chance of heads is a half, and the prescription to
have your credences match chances (<em>ceteris paribus</em>). In the
unknown bias case, by contrast, one arrives at the same assignment in
a different way: nothing in your evidence supports one proposition
over the other so some &ldquo;principle of indifference&rdquo;
reasoning suggests that they should be assigned the same
credence <span class="citation">(see H&aacute;jek 2011, for discussion of the
principle of indifference)</span>.</p>

<p>If we take seriously the &ldquo;ambiguity aversion&rdquo; discussed
earlier, when offered the choice between betting on the fair coin&rsquo;s
landing heads as opposed to the unknown-bias coin&rsquo;s landing heads, it
doesn&rsquo;t seem unreasonable to prefer the former. Recall the preference
for unambiguous gambles in the Ellsberg game
in <a href="#EllDec">section 2.1</a>. But if both coins have the same
subjective probabilities attached, what rationalises this preference
for betting on the fair coin? Joyce argues that there is a difference
between these beliefs that is worth representing. IP <em>does</em>
represent the difference. The first case is represented
by \(P(H)=\{0.5\}\), while the second is
captured by \(P(H)=[0,1]\).</p>

<p>Scott Sturgeon puts this point nicely when he says:</p>

<blockquote>
<p><em>[E]vidence and attitude aptly based on it must match in
character</em>. When evidence is essentially sharp, it warrants sharp
or exact attitude; when evidence is essentially fuzzy&mdash;as it is
most of the time&mdash;it warrants at best a fuzzy attitude. In a
phrase: evidential precision begets attitudinal precision; and
evidential imprecision begets attitudinal
imprecision. <span class="citation">(Sturgeon 2008: 159 Sturgeon&rsquo;s
emphasis)</span></p>
</blockquote>

<p><span class="citation">Wheeler (2014)</span> criticises Sturgeon on
this &ldquo;character matching&rdquo; thesis. However, an argument for
IP based on the nature of evidence only requires that the character of
the evidence sometimes allows (or mandates?) imprecise belief and not
that the characters must always match. In
opposition, <span class="citation">Schoenfield (2012)</span> argues
that evidence always supports precise credence, but that for reasons
of limited computational capacity, real agents needn&rsquo;t be required to
have precise credences. However, her argument only really supports the
claim that <em>sometimes</em> indeterminacy is due to complexity of
the evidence and computational complexity. She doesn&rsquo;t have an
argument against the claims Levi, Kaplan, Joyce and others make that
there are evidential situations that warrant imprecise attitudes.</p>

<p>Strictly speaking, what we have here is only half the
story. There <em>is</em> a difference between the
representations of belief as regards weight and balance. But that
still leaves open the question of exactly what is representing the
weight of evidence? What aspect of the belief reflects this
difference? One might be tempted to
view \(\overline{P}(H)-\underline{P}(H)\) as a
measure of the weight of evidence
for \(H\). <span class="citation">Walley
(1991)</span> tentatively suggests as much. However, this would get
wrong cases of conflicting evidence. (Imagine two equally reliable
witnesses: one tells you the coin is biased towards heads, the other
says the bias is towards tails.) The question of whether and
how IP does better than precise probabilism has not yet received an
adequate answer. Researchers in IP have, however, made progress on
distinguishing cases where your beliefs happen to have certain
symmetry properties from cases where your beliefs capture evidence
about symmetries in the objects of belief. This <em>is</em> a
distinction that the standard precise model of belief fails to
capture <span class="citation">(de Cooman and Miranda
2007)</span>.</p>

<p>The precise probabilist can respond to the weight/balance
distinction argument by pointing to the property
of <em>resiliency</em> <span class="citation">(Skyrms 2011)</span>
or <em>stability</em> <span class="citation">(Leitgeb
2014)</span>. The idea is that probabilities determined by the weight
of evidence change less in response to new evidence than do
probabilities determined by balance of evidence alone. That is, if
you&rsquo;ve seen a hundred tosses of the coin, seeing it land heads doesn&rsquo;t
affect your belief much, while if you&rsquo;ve not seen any tosses of the
coin, seeing it land heads has a bigger effect on your beliefs. Thus,
the distinction is represented in the precise probabilistic framework
in the conditional probabilities. The distinction, though, is one that
cannot rationalise the preference for betting on the fair coin. One
could develop a resiliency-weighted expected value and claim that this
is what you should maximise, but this would be as much of a departure
from orthodox probabilism as IP is. If someone were to develop such a
theory, then its merits could be weighed against the merits of IP type
models.</p>

<p>Another potential precise response would be to suggest that there
is weight of evidence for \(H\) if many
propositions that are evidence for \(H\) are
fully believed, or if there is a chance proposition
(about \(H\)) that is near to fully
believed. This is in contrast to cases of mere balance where few
propositions that are evidence for \(H\) are
fully believed, or where probability is spread out over a number of
chance hypotheses. The same comments made above about resiliency apply
here: such distinctions can be made, but this doesn&rsquo;t get us to a
theory that can rationalise ambiguity aversion.</p>

<p>The phenomenon of dilation (<a href="#Dil">section 3.1</a>)
suggests that the kind of argument put forward in this section needs
more care and further elaboration.</p>

<h3><a id="SusJud">2.4 Suspending judgement</a></h3>

<p>You are sometimes in a position where none of your evidence seems
to speak for or against the truth of some proposition. Arguably, a
reasonable attitude to take towards such a proposition is suspension
of judgement.</p>

<blockquote>
<p>When there is little or no information on which to base our
conclusions, we cannot expect reasoning (no matter how clever or
thorough) to reveal a most probable hypothesis or a uniquely
reasonable course of action. There are limits to the power of
reason. <span class="citation">(Walley 1991: 2)</span></p>
</blockquote>

<p>Consider a coin of unknown bias. The Bayesian agent must have a
precise belief about the coin&rsquo;s landing heads on the next toss. Given
the complete lack of information about the coin, it seems like it
would be better just to suspend judgement. That is, it would be better
not to have any particular precise credence. It would be better to
avoid betting on the coin. But there just isn&rsquo;t room in the Bayesian
framework to do this. The probability function must output some
number, and that number will sanction a particular set of bets as
desirable.</p>

<p>Consider \(\underline{P}(X)\) as
representing the degree to which the evidence
supports \(X\). Now
consider \(I(X) = 1- (\underline{P}(X) +
\underline{P}(\neg X))\). This measures the degree to which the
evidence is silent
on \(X\). <span class="citation">Huber
(2009)</span> points out that precise probabilism can then be
understood as making the claim that no evidence is ever silent on any
proposition. That is, \(I(X)=0\) for
all \(X\). One can never suspend
judgement. This is a nice way of seeing the strangeness of the precise
probabilist&rsquo;s attitude to evidence. Huber is making this point about
Dempster-Shafer belief functions (see
 <a href="supplement-historical.html#ArtDemGleSha">historical appendix</a>,
 section 7), but it carries over to IP in general.</p>

<p>The committed precise probabilist would respond that
setting \(p(X)=0.5\) <em>is</em> suspending
judgement. This <em>is</em> the maximally noncommittal credence in the
case of a coin flip. More generally, suspending judgement should be
understood in terms of maximising
entropy <span class="citation">(Jaynes 2003; Williamson 2010:
49&ndash;72)</span>. The imprecise probabilist could argue that this
only <em>seems</em> to be the right way to be noncommittal if you are
wedded to the precise probabilist representation of belief. That is,
the MaxEnt approach makes sense if you are already committed to
representation of belief by a single precise probability, but loses
its appeal if credal sets are available. Suspending judgement is
something you do when the evidence doesn&rsquo;t determine your
credence. But for the precise probabilist, there is no way to signal
the difference between suspension of judgement and strong evidence of
probability half. This is just the weight/balance argument again.</p>

<p>To make things more stark, consider the following delightfully odd
example from Adam Elga:</p>

<blockquote>
<p>A stranger approaches you on the street and starts pulling out
objects from a bag. The first three objects he pulls out are a
regular-sized tube of toothpaste, a live jellyfish, and a travel-sized
tube of toothpaste. To what degree should you believe that the next
object he pulls out will be another tube of
toothpaste? <span class="citation">(2010: 1)</span></p>
</blockquote>

<p>In this case, unlike in the coin case, it really isn&rsquo;t clear what
intuition says about what would be the &ldquo;correct&rdquo; precise
probabilist suspension of judgement. What Maximum Entropy methods
recommend will depend on seemingly arbitrary choices about the formal
language used to model the situation. Williamson is well aware of this
language relativity problem. He argues that choice of a language
encodes some of our evidence.</p>

<p>Another response to this argument would be to take William James&rsquo;
response to W.K. Clifford <span class="citation">(Clifford 1901; James
1897)</span>. James argued that as long as your beliefs are consistent
with the evidence, then you are free to believe what you like. So
there is <em>no need</em> to ever suspend judgement. Thus, the precise
probabilist&rsquo;s inability to do so is no real flaw. This attitude, which
is sometimes called <em>epistemic voluntarism</em>, is close to the
sort of <em>subjectivism</em> espoused by Bruno de Finetti, Frank
Ramsey and others.</p>

<p>There does seem to be a case for an alternative method of
suspending judgement in order to allow you to avoid making any bets
when your evidence is very incomplete, ambiguous or imprecise. If your
credences serve as your standard for the acceptability of bets, they
should allow for both sides of a bet to fail to be acceptable. A
precise probabilist cannot do this since if a bet has (precise)
expected value \(e\) then taking the other
side of that bet (being the bookie) has expected
value \(-e\). If acceptability is understood
as nonnegative expectation, then at least one side of any bet is
acceptable to a precise agent. This seems unsatisfactory. Surely
genuine suspension of judgement involves being unwilling to risk money
on the truth of a proposition at any odds.</p>

<p>Inspired by the famous &ldquo;Bertrand
paradox&rdquo;, <span class="citation">Chandler (2014)</span>
offers a neat argument that the precise probabilist cannot jointly
satisfy two desiderata relating to suspension of judgment about a
variable. First desideratum: if you suspend judgement about the value
of a bounded real variable \(X\), then it
seems that different intervals of possible values
for \(X\) of the same size should be treated
the same by your epistemic state. Second desideratum:
if \(Y\) essentially describes the same
quantity as \(X\), then suspension of
judgement about \(X\) should entail suspension
of judgement about \(Y\). Let&rsquo;s imagine now
that you have precise probabilities and that you suspend judgement
about \(X\). By the first desideratum, you
have a uniform distribution over values
of \(X\). Now consider \(Y =
1/X\). \(Y\) essentially describes the
same quantity that \(X\) did. But a uniform
distribution over \(X\) entails a non-uniform
distribution over \(Y\). So you do not suspend
judgement over \(Y\). A real-world case of
variables so related is &ldquo;ice residence time in clouds&rdquo; and
&ldquo;ice fall rate in clouds&rdquo;. These are inversely related,
but describe essentially the same element of a climate
system <span class="citation">(Stainforth et al. 2007:
2154)</span>.</p>

<p>So a precise probabilist cannot satisfy these reasonable desiderata
of suspension of judgement. An imprecise probabilist can: for example,
the set of all probability functions
over \(X\) satisfies both desiderata. There
may be more informative priors that also represent suspension of
judgement, but it suffices for now to point out that IP seems better
able to represent suspension of judgement than precise
probabilism. Section 5.5 of <span class="citation">Walley
(1991)</span>,
discusses IP&rsquo;s prospects as a method for
dealing with suspension of judgement.</p>

<h3><a id="UnkCor">2.5 Unknown correlations</a></h3>

<p><span class="citation">Haenni et al. (2011)</span> motivate
imprecise probabilities by showing how they can arise from precise
probability judgements. That is, if you have a precise probability
for \(X\) and a precise probability
for \(Y\), then you can put bounds
on \(p(X\cap Y)\) and \(p(X
\cup Y)\), even if you don&rsquo;t know
how \(X\) and \(Y\)
are related. These bounds give you intervals of possible probability
values for the compound events.</p>

<p>For example, you know that \(p(X \cap Y)\) is bounded above by
\(p(X)\) and by \(p(Y)\) and thus by \(\min\{p(X),p(Y)\}\). If
\(p(X) \gt 0.5\) and \(p(Y) \gt 0.5\) then \(X\) and \(Y\) must
overlap. So \(p(X\cap Y)\) is bounded below by \(p(X)+p(Y)-1\). But,
by definition, \(p(X \cap Y)\) is also bounded below by \(0\). So we
have the following result: if you know \(p(X)\) and you know \(p(Y)\),
then, you know</p>

\[
\max\{0,p(X)+p(Y)-1\} \le p(X \cap Y) \le \min\{p(X),p(Y)\}.
\]

<p>Likewise, bounds can be put on \(p(X \cup Y)\). \(p(X\cup Y)\)
can&rsquo;t be bigger than when \(X\) and \(Y\) are disjoint, so it is
bounded above by \(p(X)+p(Y)\). It is also bounded above by \(1\), and
thus by the minimum of those expressions. It is also bounded below by
\(p(X)\) and by \(p(Y)\) and thus by their maximum. Putting this
together,</p>

\[\max\{p(X),p(Y)\} \le p(X\cup Y) \le \min\{p(X)+p(Y),1\}.\]

<p>

These constraints are effectively what you get from de Finetti&rsquo;s
Fundamental Theorem of Prevision <span class="citation">(de Finetti
1990 [1974]: 112; Schervish, Seidenfeld, and Kadane 2008)</span>.</p>

<p>So if your evidence constrains your belief
in \(X\) and in \(Y\),
but is silent on their interaction, then you will only be able to pin
down these compound events to certain intervals. Any choice of a
particular probability function will go beyond the evidence in
assuming some particular evidential relationship
between \(X\)
and \(Y\). That
is, \(p(X)\) and \(p(X\mid
Y)\) will differ in a way that has no grounding in your
evidence.</p>

<h3><a id="NonCha">2.6 Nonprobabilistic chances</a></h3>

<p>What if the objective chances were not probabilities? If we endorse
some kind of connection between known objective chances and
belief&mdash;for example, a principle of direct inference or Lewis&rsquo;
Principal Principle <span class="citation">(Lewis
1986)</span>&mdash;then we might have an additional reason to endorse
imprecise probabilism. It seems to be a truth universally acknowledged
that chances ought to be probabilities, but it is a
&ldquo;truth&rdquo; for which very little argument has been
offered. For example, <span class="citation">Schaffer (2007)</span>
makes obeying the probability axioms one of the things required in
order to play the &ldquo;chance role&rdquo;, but offers no argument
that this should be the case.  Joyce says &ldquo;some have held
objective chances are not probabilities. This seems unlikely, but
explaining why would take us too far
afield&rdquo; <span class="citation">(2009: 279,
fn.17)</span>. Various other discussions of chance&mdash;for example
in statistical mechanics <span class="citation">(Loewer 2001; Frigg
2008)</span> or &ldquo;Humean
chance&rdquo; <span class="citation">(Lewis 1986,
1994)</span>&mdash;take for granted that chances should be precise and
probabilistic (<span class="citation">Dardashti et al. 2014</span> is
an exception). Obviously things are confused by the use of the concept
of chance as a way of interpreting probability theory. There is,
however, a perfectly good pre-theoretic notion of chance: this is what
probability theory was originally invented to reason about, after
all. This pre-theoretic chance still seems like the sort of thing that
we should apportion our belief to, in some sense. And there is very
little argument that chances must always be probabilities. If the
chances were nonprobabilistic in a particular way, one might argue
that your credences ought to be nonprobabilistic in the same way. What
form a chance-coordination norm should take if chances and credences
were to have non-probabilistic formal structures is currently an open
problem.</p>

<p>I want to give a couple of examples of this idea. First consider
some physical process that doesn&rsquo;t have a limiting frequency but has a
frequency that varies, always staying within some interval. This would
be a process that is chancy, but fairly predictable. It might be that
the best description of such a system is to just put bounds on its
relative frequency. Such processes have been studied using IP
models <span class="citation">(Kumar and Fine 1985; Grize and Fine
1987; Fine 1988)</span>, and have been discussed as a potential source
of imprecision in credence <span class="citation">(H&aacute;jek and Smithson
2012)</span>. A certain kind of non-standard understanding of a
quantum-mechanical event leads naturally to upper probability
models <span class="citation">(Suppes and Zanotti 1991; Hartmann and
Suppes 2010)</span>. John Norton has discussed the limits of
probability theory as a logic of induction, using an example which, he
claims, admits no reasonable probabilistic
attitude <span class="citation">(Norton 2007, 2008a,b)</span>. One
might hope that IP offers an inductive logic along the lines Norton
sketches. Norton himself has expressed scepticism on this
line <span class="citation">(Norton 2007)</span>,
although <span class="citation">Ben&eacute;treau-Dupin (2015)</span> has defended IP as a candidate
system for Norton&rsquo;s project.
Finally, particular views on vagueness might well prompt a rethinking of the formal structure of chance <span class="citation">(Bradley 2016)</span>.</p>

<h3><a id="GroBel">2.7 Group belief</a></h3>

<p>Suppose we wanted our epistemology to apply not just to
individuals, but to &ldquo;group agents&rdquo; like committees,
governments, companies, and so on. Such agents may be made up of
members who disagree. Levi <span class="citation">(1986, 1999)</span>
has argued that representation of such conflict is better handled with
sets of probabilities than with precise probabilities. There is a rich
literature on combining or aggregating the (probabilistic) opinions of
members of groups <span class="citation">(Genest and Zidek
1986)</span> but the outcome of such aggregation does not adequately
represent the <em>disagreement</em> among the group. Some forms of
aggregation also fail to respect plausible constraints on group
belief. For example, if every member of the group agrees
that \(X\) and \(Y\)
are probabilistically independent, then it seems plausible to require
that the group belief respects this unanimity. It is, however, well
known that linear pooling&mdash;a simple and popular form of
aggregation&mdash;does not respect this desideratum. Consider two
probability functions \(p, q\) such
that \(p(X) = p(Y) = 1/3\)
and \(p(X\mid Y)=p(X)\)
while \(q(X) = q(Y) = 2/3\)
and \(q(X\mid Y)=q(X)\). Consider
aggregating these two probabilities by taking an unweighted average of
them: \(r = p/2 + q/2\). Now, calculation
shows that \(r(X\cap Y) = 5/18\)
while \(r(X)r(Y) = 1/4\), thus demonstrating
that \(r\) does not
consider \(X\)
and \(Y\) to be independent. So such an
aggregation method does not satisfy the above
desideratum <span class="citation">(Kyburg and Pittarelli 1992; Cozman
2012)</span>. For more on judgement aggregation in groups,
see <span class="citation">List and Pettit (2011)</span>, especially
chapter 2.</p>

<p>
<span class="citation">Elkin and Wheeler (2016)</span> argue that resolving disagreement among precise probabilist peers should involve an imprecise probability. <span class="citation">Stewart and Quintana (2018)</span> argue that imprecise aggregation methods have some nice properties that no precise aggregation method do.
</p>

<p>If committee members have credences <em>and</em> utilities that
differ among the group, then no precise probability-utility pair
distinct from the probabilities and utilities of the agents can
satisfy the <em>Pareto
condition</em> <span class="citation">(Seidenfeld, Kadane, and
Schervish 1989)</span>. The Pareto condition requires that the group
preference respect agreement of preference among the group. That is,
if all members of the group prefer \(A\)
to \(B\) (that is, if each group member finds
that \(A\) has higher expected utility
than \(B\)) then the aggregate preference (as
determined by the aggregate probability-utility pair) should satisfy
that preference. Since this &ldquo;consensus preservation&rdquo; is a
reasonable requirement on aggregation, this result shows that precise
models of group agents are problematic. Walley discusses an example of
a set of probabilities where each probability represents the beliefs
of a member of a group, then \(P\) is an
incomplete description of the beliefs of each agent, in the sense that
if all members of \(P\) agree on something,
then that thing is something each agent believes. Sets of
probabilities allow us to represent an agent who
is <em>conflicted</em> in their
judgements <span class="citation">(Levi 1986, 1999)</span>.</p>

<blockquote>
<p>Ideally rational agents may face choices where there is no best
option available to them. Indeterminacy in probability judgement and
unresolved conflicts between values lead to predicaments where at the
moment of choice the rational agent recognizes more than one such
preference ranking of the available options in [the set of available
choices] to be permissible. <span class="citation">(Levi 1999:
510)</span></p>
</blockquote>

<p>Levi also argued that individual agents can be in conflict in the
same way as groups, and thus that individuals&rsquo; credal states are also
better represented by sets of probabilities. (Levi also argued for
the <em>convexity</em> of credal states, which brings him into
conflict with the above argument about independence (see
 <a href="supplement-historical.html#IsaLev">historical appendix</a>
section 3).) One doesn&rsquo;t need to buy the claim that groups and
individuals must be modelled in the same way to take something away
from this idea. One merely needs to accept the idea that an individual
can be <em>conflicted</em> in such a way that a reasonable
representation of her belief state&mdash;or belief and value
state&mdash;is in terms of sets of
functions. <span class="citation">Bradley (2009)</span> calls members
of such sets &ldquo;avatars&rdquo;. This suggests that we interpret an
individual&rsquo;s credal set as a <em>credal committee</em> made up of her
avatars. This interpretation of the representor is due
to <span class="citation">Joyce (2011)</span>, though Joyce attributes
it Adam Elga. This committee represents all the possible prior
probabilities you could have that are consistent with the
evidence. Each credal committee member is a fully opinionated Jamesian
voluntarist. The committee as a whole, collectively, is a Cliffordian
objectivist.</p>

<h2><a id="PhiQueForIP">3. Philosophical questions for IP</a></h2>

<p>This section collects some problems for IP noted in
the literature.</p>

<h3><a id="Dil">3.1 Dilation</a></h3>

<p>Consider two logically unrelated
propositions \(H\)
and \(X\). Now consider the four &ldquo;state
descriptions&rdquo; of this simple model as set out
in <a href="#fig1">Figure 1</a>. So \(a=H\cap
X\) and so on. Now define \(Y=a \cup
d\). Alternatively, consider three propositions related in the
following way: \(Y\) is defined as
&ldquo;\(H\) if and only
if \(X\)&rdquo;.</p>
<div class="figure" id="fig1">
<img src="two-by-two.png" width="300" alt="[A square with four quadrants, first column is labeled 'H' and second 'not H'; first row is labeled 'X' and second 'not X'.  First quadrant (first column/first row) is shaded and has an 'a' on it; second quadrant (second column, first row) is not shaded and has a 'b' on it; third quadrant (first column, second row) is unshaded and has a 'c' on it; and last quadrant (second column, second row) is shaded and has a 'd' on it.]" />

<p class="center"><span class="figlabel">Figure 1:</span> A diagram of the relationships after <span class="citation">Seidenfeld (1994)</span>; \(Y\) is the shaded area</p>
</div>

<p>Further imagine that \(p(H\mid X) = p(H) =
1/2\). No other relationships between the propositions hold
except those required by logic and probability theory. It is
straightforward to verify that the above constraints require
that \(p(Y) = 1/2\). The probability
for \(X\), however, is unconstrained.</p>

<p>Let&rsquo;s imagine you were given the above information, and took your
representor to be the full set of probability functions that satisfied
these constraints. Roger White suggested an intuitive gloss on how you
might receive information about propositions so related and so
constrained <span class="citation">(White 2010)</span>. White&rsquo;s puzzle
goes like this. I have a proposition \(X\),
about which you know nothing at all. I have written whichever is true
out of \(X\) and \(\neg
X\) on the <em>Heads</em> side of a fair coin. I have painted
over the coin so you can&rsquo;t see which side is heads. I then flip the
coin and it lands with the \(X\)
uppermost. \(H\) is the proposition that the
coin lands heads up. \(Y\) is the proposition
that the coin lands with the
&ldquo;\(X\)&rdquo; side up.</p>

<p>Imagine if you had a precise prior that made you certain
of \(X\) (this is compatible with the above
constraints since \(X\) was
unconstrained). Seeing \(X\) land uppermost
now should be evidence that the coin has landed heads. The game set-up
makes it such that these apparently irrelevant instances of
evidence <em>can</em> carry information. Likewise, being very
confident of \(X\)
makes \(Y\) very good evidence
for \(H\). If instead you were
sure \(X\) was
false, \(Y\) would be solid gold evidence
of \(H\)&rsquo;s falsity. So it seems
that \(p(H\mid Y)\) is proportional to prior
belief in \(X\) (indeed, this can be proven
rather easily). Given the way the events are related, observing
whether \(X\) or \(\neg
X\) landed uppermost is a noisy channel to learn about whether
or not \(H\) landed uppermost.</p>

<p>So let&rsquo;s go back to the original imprecise case and consider what
it means to have an imprecise belief
in \(X\). Among other things, it means
considering possible that \(X\) could be very
likely. It is consistent with your belief state
that \(X\) is such that if you knew what
proposition \(X\) was, you would consider it
very likely. In this case, \(Y\) would be good
evidence for \(H\). Note that in this case
learning that the coin landed \(\neg X\)
uppermost&mdash;call
this \(Y'\)&mdash;would be just as good
evidence
against \(H\). Likewise, \(X\)
might be a proposition that you would have very low credence in, and
thus \(Y\) would be
evidence <em>against</em> \(H\).</p>

<p>Since you are in a state of ignorance with respect
to \(X\), your representor contains
probabilities that take \(Y\) to be good
evidence that \(H\) and probabilities that
take \(Y\) to be good evidence
that \(\neg H\). So, despite the fact
that \(P(H)=\{1/2\}\) we
have \(P(H\mid Y) = [0,1]\). This
phenomenon&mdash;posteriors being wider than their priors&mdash;is
known as dilation. The phenomenon has been thoroughly investigated in
the mathematical literature <span class="citation">(Walley 1991;
Seidenfeld and Wasserman 1993; Herron, Seidenfeld, and Wasserman 1994;
Pedersen and Wheeler 2014)</span>. Levi and Seidenfeld reported
an example of dilation to Good following <span class="citation">Good
(1967)</span>. Good mentioned this correspondence in his follow up
paper <span class="citation">(Good 1974)</span>. Recent interest in
dilation in the philosophical community has been generated by White&rsquo;s
paper <span class="citation">(White 2010)</span>.</p>

<p>White considers dilation to be a problem since learning \(Y\)
doesn&rsquo;t <em>seem</em> to be relevant to \(H\). That is, since
you are ignorant about \(X\), learning whether or not the coin landed
\(X\) up doesn&rsquo;t seem to tell you anything about whether the
coin landed heads up. It seems strange to argue that your belief in
\(H\) should <em>dilate</em> from \(1/2\) to \([0,1]\) upon learning
\(Y\). It feels as if this should just be irrelevant to
\(H\). However, \(Y\) is only really irrelevant to \(H\) when
\(p(X)=1/2\). Any other precise belief you might have in \(X\) is such
that \(Y\) now affects your posterior belief in
\(H\). <a href="#fig2">Figure 2</a> shows the situation for one
particular belief about how likely \(X\) is; for one particular \(p\in
P\). The horizontal line can shift up or down, depending on what the
committee member we focus on believes about \(X\). \(p(H\mid Y)\) is
a half only if the prior in \(X\) is also a half. However, the
imprecise probabilist takes into account <em>all the ways \(Y\) might
affect belief in \(H\)</em>.</p>

<div class="figure" id="fig2">
<img src="joyces-diagram.png" width="300" alt="[A square with two columns labeled 'H' and 'not H' and two rows, a narrow one labeled 'X' and a wide one labeled 'not X'.  The first quadrant (first column, first row) is shaded and has a 'Y' on it; second quadrant (second column, first row) is not shaded and has a 'not Y' on it; third quadrant (first column, second row) is unshaded with a 'not Y' on it and the fourth quadrant (second column, second row) is shaded and has a 'Y' on it.]" />

<p class="center"><span class="figlabel">Figure 2:</span> A member of the credal committee (after <span class="citation">Joyce (2011)</span>)</p>
</div>

<p>Consider a group of agents who each had precise credences in the
above coin case and differed in their priors
on \(X\). They would all start out with prior
of a half in \(H\). After
learning \(Y\), these agents would differ in
their posterior opinions about \(H\) based on
their differing dispositions to update. The group belief would
dilate. However, no agent in the group has acted in any way
unreasonably. If we take Levi&rsquo;s suggestion that individuals can be
conflicted just like groups can, then it seems that individual agents
can have their beliefs dilate just like groups can.</p>

<p>There are two apparent problems with dilation. First, the
belief-moving effect of apparently irrelevant evidence; and second,
the fact that learning some evidence can cause your belief-intervals
to widen. The above comments speak to the first of
these. <span class="citation">Pedersen and Wheeler
(2014)</span> also are focused on mitigating this worry. We
turn now to the second worry.</p>

<p>Even if we accept dilation as a fact of life for the imprecise
probabilist, it is still <em>weird</em>. Even if all of the above
argument is accepted, it still seems strange to say that your belief
in \(H\) is dilated, <em>whatever you
learn</em>. That is, whether you learn \(Y\)
or \(Y'\), your posterior belief
in \(H\) looks the
same: \([0,1]\). Or perhaps, what it shows to
be weird is that your initial credence was precise.</p>

<p>
<span class="citation">Hart and Titelbaum (2015)</span> suggest that dilation is strange because conditionalising on a biconditional (which is, after all, what you are doing in the above example) is unintuitive even in the precise case. Whether all cases of dilation can be explained away in this manner remains to be seen.
<span class="citation">Gong and Meng (2017)</span> likewise see dilation as a problem of mis-specified statistical inference, rather than a problem for IP per se.
</p>

<p>Beyond this seeming strangeness, White suggests a specific way that
being subject to dilation is an indicator of a defective
epistemology. White suggests that dilation examples show that
imprecise probabilities violate the <em>Reflection
Principle</em> <span class="citation">(van Fraassen 1984)</span>. The
argument goes as follows:
</p><blockquote><p> given that you know now that whether you
learn \(Y\) or you
learn \(Y'\) your credence
in \(H\) will
be \([0,1]\) (and you will certainly learn one
or the other), your current credence in \(H\)
should also be \([0,1]\).
</p></blockquote><p> The general idea is that you should set your
 credences to what you expect your credences to be in the future. More
 specifically, your credence in \(X\) should
 be the expectation of your future possible credences
 in \(X\) over the things you might
 learn. Given that, for all the things you might learn in this example
 your credence in \(H\) would be the same, you
 should have that as your prior credence also. Your prior should be
 such that \(P(H) = [0,1]\). So having a
 precise prior credence in \(H\) to start with
 is irrational. That&rsquo;s how the argument against dilation from
 reflection goes. Your prior \(P\) is not
 fully precise though. Consider \(P(H \cap
 Y)\). That is, the prior belief in the conjunction is
 imprecise. So the alleged problem with dilation and reflection is not
 as simple as &ldquo;your precise belief becomes imprecise&rdquo;. The
 problem is &ldquo;your precise
 belief <em>in \(H\)</em> becomes
 imprecise&rdquo;; or rather, your precise belief
 in \(H\) <em>as represented
 by \(P(H)\)</em> becomes imprecise.</p>

<p>The issue with reflection is more basic. What exactly does
reflection require of imprecise probabilists in this case? Now, it is
obviously the case that each credal committee member&rsquo;s prior credence
is its expectation over the possible future evidence (this is a
theorem of probability theory). But somehow, it is felt,
the <em>credal state as a whole</em> isn&rsquo;t sensitive to reflection in
the way the principle requires. Each \(p\in
P\) satisfies the principle, but the awkward symmetries of the
problem conspire to make \(P\) as a whole
violate the principle. This looks to be the case if we focus
on \(P(H)\) as an adequate representation of
that part of the belief state. But as noted earlier, this is not an
adequate way of understanding the credal state. Note that while
learning \(Y\) and
learning \(Y'\) both prompt revision to a
state where the posterior belief in \(H\) is
represented as an interval by \([0,1]\), the
credal states <em>as sets of probabilities</em> are not the same. Call
the state after
learning \(Y\), \(P'\)
and the state after
learning \(Y'\), \(P''\). So \(P'
= \{p(\cdot \mid Y), p\in P\}\)
and \(P'' = \{p(\cdot\mid Y'), p\in
P\}\). While it is true that \(P'(H) =
P''(H)\), \(P' \neq
P''\) as sets of probabilities, since
if \(p\in P'\)
then \(p(Y) = 1\) whereas
if \(p\in P''\)
then \(p(Y) = 0\). So one lesson we should
learn from dilation is that imprecise belief is represented
by <em>sets of functions</em> rather than by a set-valued
function <span class="citation">(see also, Joyce 2011; Topey 2012;
Bradley and Steele 2014b)</span>.</p>

<p>
So dilation can perhaps be tamed or rationalised, and the issue with reflection can be mitigated.
But there is still a puzzle that dilation raises:
in the precise context we have a nice result &ndash; due to <span class="citation">Good (1967)</span> &ndash;
that says roughly that learning new information has positive expected value.
Information has positive value.
This result is, to some extent, undermined by dilation.
<span class="citation">Bradley and Steele (2016)</span> suggest that there is 
some sense in which Good&rsquo;s result can be partially salvaged in the IP setting.
</p>

<p>It seems that examples of dilation undermine the earlier claim that
imprecise probabilities allow you to represent the difference between
the weight and balance of evidence
(see <a href="#WeiEviBalEvi">section 2.3</a>):
learning \(Y\) appears to give rise to a
belief which one would consider as representing <em>less evidence</em>
since it is more spread out. This is so because the prior credence in
the dilation case is precise, not through weight of evidence, but
through the symmetry discussed earlier. We cannot take narrowness of
the interval \([\underline{P}(X),
\overline{P}(X)]\) as a characterisation of weight of evidence
since the interval can be narrow for reasons other than because lots
of evidence has been accumulated. So my earlier remarks on
weight/balance should not be read as the claim that imprecise
probabilities can always represent the weight/balance
distinction. What is true is that <em>there are</em> cases where
imprecise probabilities can represent the distinction in a way that
impacts on decision making. This issue is far from settled and more
work needs to be done on this topic.</p>

<h3><a id="BelIne">3.2 Belief inertia</a></h3>

<p>Imagine there are two live
hypotheses \(H_1\)
and \(H_2\). You have no idea how likely they
are, but they are mutually exclusive and exhaustive. Then you acquire
some evidence \(E\). Some simple probability
theory shows that for every \(p\in P\) we have
the following relationship (using \(p_i = p(E\mid
H_i)\) for \(i=1,2\)).</p>

\[\begin{align}
p(H_1\mid E) &amp; = {{p(E\mid H_1)p(H_1)} \over {p_1 p(H_1) +p_2
p(H_2)}} \\ &amp; = {{p_1 p(H_1)} \over {p_2 + (p_1 - p_2) p(H_1)}}\\
\end{align}\]

<p>If your prior in \(H_1\) is <em>vacuous</em>&mdash;if \(P(H_1) =
[0,1]\)&mdash;then the above equation shows that your posterior is
vacuous as well. That is, if \(p(H_1) = 0\) then \(p(H_1\mid E) =
0\) and likewise for \(p(H_1) = 1 = p(H_1\mid E)\), and since the
right hand side of the above equation is a continuous function of
\(p(H_1)\), for every \(r\in [0,1]\) there is some \(p(H_1)\) such
that \(p(H_1\mid E) = r\). So \(P(H_1\mid E)=[0,1]\).</p>

<p>It seems like the imprecise probabilist cannot learn from vacuous
priors. This problem of belief inertia goes back at least as far as
<span class="citation">Levi (1980), chapter 13</span>.
Walley also discusses the issue, but appears unmoved by it: 
he says that vacuous posterior probabilities are just a
consequence of adopting a vacuous prior:</p>

<blockquote>
<p>The vacuous previsions really are rather trivial models. That seems
appropriate for models of &ldquo;complete ignorance&rdquo; which is a
rather trivial state of uncertainty. On the other hand, one cannot
expect such models to be very useful in practical problems,
notwithstanding their theoretical importance. If the vacuous
previsions are used to model prior beliefs about a statistical
parameter for instance, they give rise to vacuous posterior
previsions&hellip; However, prior previsions that are close to vacuous
and make nearly minimal claims about prior beliefs can lead to
reasonable posterior previsions. <span class="citation">(Walley 1991:
93)</span></p>
</blockquote>

<p><span class="citation">Joyce (2011)</span>
and <span class="citation">Rinard (2013)</span> have both discussed
this problem. Rinard&rsquo;s solution to it is to argue that this shows that
the vacuous prior is never a legitimate state of belief. Or rather,
that we only ever need to model your beliefs using non-vacuous priors,
even if these are incomplete descriptions of your belief state. This
is similar to Walley&rsquo;s &ldquo;non-exhaustive&rdquo; representation of
belief. 
<span class="citation">Vallinder (2018)</span> suggests that the problem of belief inertia is quite a general one.
<span class="citation">Castro and Hart (forthcoming)</span> use the looming danger of belief inertia to argue against what I have called an "objectivist" interpretation of IP.
</p>

<p>An alternative solution to this
problem, <span class="citation">(inspired by Wilson 2001; and Cattaneo
2008; 2014)</span>, would modify the update rule in such a way that
those extreme priors that give extremely small likelihoods to the
evidence are excised from the representor. More work would need to be
done to make this precise and show how exactly the response would
go.</p>

<h3><a id="DecMak">3.3 Decision making</a></h3>

<p>One important use that models of belief can be put to is as part of
a theory of rational decision. IP is no different. Decision making
with imprecise probabilities has some problems, however.</p>

<p>The problem for IP decision making, in short, is that your credal
committee can disagree on what the best course of action is, and when
they do, it is unclear how you should act (recall the definitions
in <a href="#SumTer">section 1.1</a>). Imagine betting on a coin of
unknown bias. Consider the indicator gambles on heads and tails. Both
bets have imprecise expectation \([0,1]\). How
are you supposed to compare these expectations? The bets are
incomparable. (If the coin case appears to have too much exploitable
symmetry, consider unit bets on Elga pulling toothpaste or jellyfish
from his bag.) This incomparability, argues Williamson, leads to
decision making paralysis, and this highlights a flaw in the
epistemology <span class="citation">(2010: 70)</span>. This argument
seems to be missing the point, however, if one of our motivations for
IP is precisely to be able to represent such incompatibility of
prospects (see <a href="#IncInc">section 2.2</a>)! The
incommensurability of options entailed by IP is not a bug, it&rsquo;s a
feature. Decision making with imprecise probabilities is discussed
by <span class="citation">Seidenfeld
(2004)</span>, <span class="citation">Troffaes
(2007)</span>, <span class="citation">Seidenfeld, Schervish, and
Kadane (2010)</span>, <span class="citation">Bradley
(2015)</span>, <span class="citation">Williams
(2014)</span>, <span class="citation">Huntley, Hable, and Troffaes
(2014)</span>.</p>

<p>A more serious worry confronts IP when you have to make sequences
of decisions. There is a rich literature in economics on sequences of
decisions for agents who fail to be orthodox expected utility
maximisers <span class="citation">(Seidenfeld 1988; 1994; Machina
1989; Al-Najjar and Weinstein 2009, and the references
therein)</span>. This topic was brought to the attention
of philosophers again after the publication of
Elga&rsquo;s <span class="citation">(2010)</span> paper <em>Subjective
Probabilities Should Be Sharp</em> which highlights the problem with a
simple decision example, although a very similar example appears
in <span class="citation">Hammond (1988)</span> in relation to
Seidenfeld&rsquo;s discussion of Levi&rsquo;s decision rule
&ldquo;E-admissibility&rdquo; <span class="citation">(Seidenfeld
1988)</span>.</p>

<p>A version of the problem is as follows. You are about to be offered
two bets on a coin of unknown bias, \(A\)
and \(B\), one after the other. The bets pay
out as follows:</p>
<ul>
<li>\(A\) loses 10 if the coin lands heads and wins 15 otherwise</li>
<li>\(B\) wins 15 if the coin lands heads and loses 10 otherwise</li>
</ul>

<p>If we assume you have beliefs represented
by \(P(H)=[0,1]\), these bets have
expectations of \([-10,15]\). Refusing each
bet has expectation of 0. So accepting and
refusing \(A\) are incomparable with respect
to your beliefs. Likewise for \(B\). The
problem is that refusing <em>both</em> bets seems to be irrational,
since accepting both bets gets you a guaranteed payoff of 5. Elga
argues that no decision rule for imprecise probabilities can rule out
refusing both bets. He then argues that this shows that imprecise
probabilities are bad epistemology. Neither argument
works. <span class="citation">Chandler (2014)</span>
and <span class="citation">Sahlin and Weirich (2014)</span> both point
out that a certain kind of imprecise decision rule does make refusing
both bets impermissible and Elga has acknowledged this in an erratum
to his paper. <span class="citation">Bradley and Steele (2014a)</span>
argue that decision rules that make refusing both bets merely
permissible are legitimate ways to make imprecise decisions. They also
point out that the rule that Chandler, and Sahlin and Weirich advocate
has counterintuitive consequences in other decision problems. 
</p>

<p>
<span class="citation">Moss (2015)</span> relates
Elga-style IP decision problems to moral dilemmas and uses the analogy
to explain the conflicting intuitions in Elga&rsquo;s problem.
<span class="citation">Sud (2014)</span> and <span class="citation">Rinard (2015)</span>
both also offer alternative decision theories for imprecise probabilities.
<span class="citation">Bradley (2019)</span> argues that all three struggle to accommodate a version of the Ellsberg decisions discussed above.
</p>

<p>
Even if
Elga&rsquo;s argument worked and there were no good imprecise decision
rules, that wouldn&rsquo;t show that IP was a faulty model of belief. We
want to be able to represent the suspension of judgement on various
things, including on the relative goodness of a number of
options. Such incommensurability inevitably brings with it some
problems for sequential decisions <span class="citation">(see, for
example, Broome 2000)</span>, but this is not an argument against the
epistemology. As Bradley and Steele note, Elga&rsquo;s argument&mdash;if it
were valid&mdash;could <em>mutatis mutandis</em> be used as an
argument that there are no incommensurable goods and this seems too
strong. </p>

<h3><a id="IntIP">3.4 Interpreting IP</a></h3>

<p>Imprecise probabilities aren&rsquo;t a radically new theory. They
are merely a slight modification of existing models of belief for
situations of ambiguity. Often your credences will be precise enough,
and your available actions will be such that you act more or
less <em>as if</em> you were a strict Bayesian. One might analogize imprecise
probabilities as the &ldquo;Theory of Relativity&rdquo; to the strict
Bayesian &ldquo;Newtonian Mechanics&rdquo;: all but indistinguishable
in all but the most extreme situations. This analogy goes deeper: in
both cases, the theories are &ldquo;empirically
indistinguishable&rdquo; in normal circumstances, but they both differ
radically in some conceptual respects. Namely, the role of absolute
space in Newtonian mechanics/GR; how to model ignorance in the
strict/imprecise probabilist case. <span class="citation">Howson
(2012)</span> makes a similar analogy between modelling belief and
models in science. Both involve some requirement to be somewhat
faithful to the target system, but in each case faithfulness must be
weighed up against various theoretical virtues like simplicity,
computational tractability and so
on. Likewise <span class="citation">Hosni (2014)</span> argues that
what model of belief is appropriate is somewhat dependent on
context. There is of course an important disanalogy in that models of
belief are supposed to be <em>normative</em> as well as descriptive,
whereas models in science typically only have to play a descriptive
role. <span class="citation">Walley (1991)</span> discusses a similar
view but is generally sceptical of such an interpretation.</p>

<h4><a id="WhaBel">3.4.1 What is a belief?</a></h4>

<p>One standard interpretation of the probability calculus is that
probabilities represent &ldquo;degrees of belief&rdquo; or
&ldquo;credences&rdquo;. This is more or less the concept that under
consideration so far. But what is a degree of belief? There are a
number of ways of cashing out what it is that a representation of
degree of belief is actually representing.</p>

<p>One of the most straightforward understandings of degree of belief
is that credences are interpreted in terms of an agent&rsquo;s limiting
willingness to bet. This is an idea which goes back
to <span class="citation">Ramsey (1926)</span>
and <span class="citation">de Finetti (1964, 1990 [1974])</span>. The
idea is that your credence in \(X\)
is \(\alpha\) just in
case \(\alpha\) is the value at which you are
indifferent between the gambles:</p>
<ul>
<li>Win \(1-\alpha\) if \(X\), lose \(\alpha\) otherwise</li>
<li>Lose \(1- \alpha \) if \(X\), win \(\alpha\) otherwise</li>
</ul>

<p>This is the &ldquo;betting interpretation&rdquo;. This is the
interpretation behind Dutch book arguments: this interpretation of
belief makes the link between betting quotients and belief strong
enough to sanction the Dutch book theorem&rsquo;s claim
that <em>beliefs</em> must be probabilistic. Williamson in fact takes
issue with IP because IP cannot be given this betting
interpretation <span class="citation">(2010: 68&ndash;72)</span>. He
argues that Smith&rsquo;s and Walley&rsquo;s contributions notwithstanding
(see <a href="supplement-formal.html">formal appendix</a>), the
single-value betting interpretation makes sense as a standard for
credence in a way that the one-sided betting interpretation
doesn&rsquo;t. The idea is that you may refuse all bets unless they are at
extremely favourable odds by your lights. Such behaviour doesn&rsquo;t speak
to your credences. However, <em>if you were to offer a single
value</em> then this tells us something about your epistemic
state. There is something to this idea, but it must be traded off
against the worry that <em>forcing</em> agents to have such single
numbers systematically misrepresents their epistemic states. As Kaplan
puts it</p>

<blockquote>
<p>The mere fact that you nominate \(0.8\)
under the <em>compulsion</em> to choose some determinate value for
[\(p(X)\)] hardly means that you have
a <em>reason</em> to choose \(0.8\). The
orthodox Bayesian is, in short, guilty of advocating false
precision. <span class="citation">(Kaplan 1983: 569, Kaplan&rsquo;s
emphasis)</span></p>
</blockquote>

<p>A related interpretation of credence is to understand credence as
being just a representation of an agent&rsquo;s dispositions to act. This
interpretation sees credence as that function such that your elicited
preferences and observed actions can be represented as those of an
expected utility maximiser with respect to that probability
function <span class="citation">(Briggs 2014: section 2.2)</span>. Your
credences <em>just are</em> that function that represents you as a
rational agent. For precise probabilism, &ldquo;rational agent&rdquo;
means &ldquo;expected utility maximiser&rdquo;. For imprecise
probabilism, rational agent must mean something slightly different. A
slightly more sophisticated version of this sort of idea is to
understand credence to be exactly that component of the preference
structure that the probability function represents in the
representation theorem. Recall the discussion of incompleteness
(<a href="#IncInc">section 2.2</a>). IP represents you as the agent
conflicted between all the \(p \in P\) such
that unless the \(p\) agree
that \(X\) is better
than \(Y\) or vice versa, you find them
incomparable. What a representation theorem actually proves is a
matter of some dispute <span class="citation">(see Zynda 2000; H&aacute;jek
2008; Meacham and Weisberg 2011)</span>.</p>

<p>One might take the view that credence is modelling some kind of
mental or psychological quantity in the head. Strength of belief is a
real psychological quantity and it is this that credence should
measure. Unlike the above views, this interpretation of credence isn&rsquo;t
easy to operationalise. It also seems like this understanding of
strength of belief distances credence from its role in understanding
decision making. The above <em>behaviourist</em> views take belief&rsquo;s
role in decision making to be central to or even definitional of what
belief is. This psychological interpretation seems to divorce belief
from decision. Whether there are such stable neurological structures
is also a matter of some controversy <span class="citation">(Fumagalli
2013; Smithson and Pushkarskaya 2015)</span>.</p>

<p>A compromise between the behaviourist views and the psychological
views is to say that belief is characterised <em>in part</em> by its
role in decision making. This leaves room for belief to play an
important role in other things, like assertion or reasoning and
inference. So the answer to the question &ldquo;What is degree of
belief?&rdquo; is: &ldquo;Degree of belief is whatever psychological
factors play the role imputed to belief in decision making contexts,
assertion behaviour, reasoning and inference&rdquo;. There is room in
this characterisation to understand credence as measuring some sort of
psychological quantity that causally relates to action, assertion and
so on. This is a sort of functionalist reading of what belief
is. <span class="citation">Eriksson and H&aacute;jek (2007)</span> argue that
&ldquo;degree of belief&rdquo; should just be taken as a primitive
concept in epistemology. The above attempts to characterise degree of
belief then fill in the picture of the role degree of belief
plays.</p>

<h4><a id="WhaBelX">3.4.2 What is a belief in \(X\)?</a></h4>

<p>So now we have a better idea of what it is that a model of belief
should do. But which part of our model of belief is representing which
part of the belief state? The first thing to say is
that \(P(X)\) is <em>not</em> an adequate
representation of the belief in \(X\). That
is, one of the values of the credal set approach is that it can
capture certain kinds of non-logical relationships between
propositions that are lost when focusing on, say, the associated set
of probability values. For example, consider tossing a coin of unknown
bias. \(P(H)=P(T)=[0,1]\), but this fails to
represent the important fact
that \(p(H)=1-p(T)\) for
all \(p\in P\). Or that getting a heads on the
first toss is at least as likely as heads on two consecutive
tosses. These facts that aren&rsquo;t captured by the sets-of-values view
can play an important role in reasoning and decision.</p>

<p>\(P(X)\) might be a <em>good enough</em>
representation of belief for some purposes. For example in the
Ellsberg game these sets of probability values (and their associated
sets of expectations) are enough to rationalise the non-probabilistic
preferences. How good the representation needs to be depends on what
it will be used for. Representing the sun as a point mass is a good
enough representation for basic orbital calculations, but obviously
inadequate if you are studying coronal mass ejections, solar flares or
other phenomena that depend on details of the internal dynamics of the
sun.</p>

<h3><a id="Reg">3.5 Regress</a></h3>

<p>Imprecise probabilities is a theory born of our limitations as
reasoning agents, and of limitations in our evidence base. If only we
had better evidence, a single probability function would do. But since
our evidence is weak, we must use a set. In a way, the same is true of
precise probabilism. If only we knew the truth, we could represent
belief with a truth-valuation function, or just a set of sentences
that are fully believed. But since there are truths we don&rsquo;t know, we
must use a probability to represent our intermediate confidence. And
indeed, the same problem arises for the imprecise probabilist. Is it
reasonable to assume that we know what set of probabilities best
represents the evidence? Perhaps we should have a set of sets of
probabilities&hellip; Similar problems arise for theories of
vagueness <span class="citation">(Sorensen 2012)</span>. We objected
to precise values for degrees of belief, so why be content with
sets-valued beliefs with precise boundaries? This is the problem of
&ldquo;higher-order vagueness&rdquo; recast as a problem for imprecise
probabilism. Why is sets of probabilities the right level to stop the
regress at? Why not sets of sets? Why not second-order probabilities?
Why not single probability
functions? <span class="citation">Williamson (2014)</span>
makes this point, and argues that a single precise probability is the
correct level at which to get off the &ldquo;uncertainty
escalator&rdquo;. Williamson advocates the betting interpretation of
belief, and his argument here presupposes that interpretation. But the
point is still worth addressing: for a particular interpretation of
what belief is, what sort of level of uncertainty is appropriate. For
the <em>functionalist</em> interpretation suggested above, this is
something of a pragmatic choice. The further we allow this regress to
continue, the harder it is to deal with these belief-representing
objects. So let&rsquo;s not go further than we need.</p>

<p>We have seen arguments above that IP does have some advantage over
precise probabilism, in the capacity to represent suspending
judgement, the difference between weight and balance of evidence and
so on. So we must go <em>at least</em> this far up the uncertainty
escalator. But for the sake of practicality we need not go any
further, even though there are hierarchical Bayes models that would
give us a well-defined theory of higher-order models of belief. This
is, ultimately, a pragmatic argument. Actual human belief states are
probably immensely complicated neurological patterns with all the
attendant complexity, interactivity, reflexivity and vagueness. We
are <em>modelling</em> belief, so it is about choosing a model at the
right level of complexity. If you are working out the trajectory of a
cannonball on earth, you can safely ignore the gravitational influence
of the moon on the cannonball. Likewise, there will be contexts where
simple models of belief are appropriate: perhaps your belief state is
just a set of sentences of a language, or perhaps just a single
probability function. If, however, you are modelling the tides, then
the gravitational influence of the moon needs to be involved: the
model needs to be more complex. This suggests that an adequate model of
belief under severe uncertainty may need to move beyond the single
probability paradigm. But a pragmatic argument says that we should
only move as far as we need to. So while you need to model the moon to
get the tides right, you can get away without having Venus in your
model. This relates to the contextual nature of appropriateness for
models of belief mentioned earlier. If one were attempting to
provide a <em>complete</em> formal characterisation of the ontology of
belief, then these regress worries would be significantly harder to
avoid.</p>

<p>Let&rsquo;s imagine that we had a second order
probability \(\mu\) defined over the set of
(first order) probabilities \(P\). We could
then reduce uncertainty to a single function
by \(p^*(X) = \sum_P \mu(p)p(X)\)
(if \(P\) is finite, in the interests of
keeping things simple I discuss only this case). Now
if \(p^*(X)\) is what is used in decision
making, then there is no real sense in which we have a genuine IP
model, and it cannot rationalise the Ellsberg choice, nor can it give
rise to incomparability. If there is some alternative use
that \(\mu\) is put to, a use that allows
incomparability and that rationalises Ellsberg choices, then it might
be a genuine rival to credal sets, but it represents just as much of a
departure from the orthodox theory as IP does.</p>

<p>G&auml;rdenfors and Sahlin&rsquo;s <em>Unreliable Probabilities</em> model
enriches a basic IP approach with a &ldquo;reliability index&rdquo;
(see the
 <a href="supplement-historical.html">historical appendix</a>).
 <span class="citation">Lyon (2017)</span>
enriches the standard IP picture in a different way: he adds a
privileged &ldquo;best guess&rdquo; probability. This modification
allows for better aggregation of elicited IP estimates. How best to
interpret such a model is still an open question. Other enriched IP
models are no doubt available.</p>

<h3><a id="WhaMakGooImpBel">3.6 What makes a good imprecise belief?</a></h3>

<p>There are, as we have seen, certain structural properties that are
necessary conditions on rational belief. What exactly these are
depends on your views. However, there are further ways of assessing
belief. Strongly believing true things and strongly believing the
negations of false things seem like good-making-features of
beliefs. For the case of precise credences, we can make this
precise. There is a large literature on &ldquo;scoring rules&rdquo;:
methods for measuring how good a probability is relative to the actual
state of the world <span class="citation">(Brier 1950; Savage 1971;
Joyce 2009; Pettigrew 2011)</span>. These are numerical methods of
measuring how good a probability is given the true state of the
world.</p>

<p>For the case of imprecise probabilities, however, the situation
looks bleak. No real valued scoring rule for imprecise probabilities
can have the desirable property of being <em>strictly
proper</em> <span class="citation">(Seidenfeld, Schervish, and Kadane
2012)</span>. <span class="citation">Schoenfield (2017)</span> presents a simple version of the result. Since strict propriety is a desirable property of a
scoring rule <span class="citation">(Br&ouml;cker and Smith 2007; Joyce
2009; Pettigrew 2011)</span>, this failing is serious. So further work
is needed to develop a well-grounded theory of how to assess imprecise
probabilities.
<span class="citation">Mayo-Wilson and Wheeler (2016)</span> provide a neater version of the proof, and offer a property weaker than strict propriety that an imprecise probability scoring rule can satisfy.
<span class="citation">Carr (2015)</span> and <span class="citation">Konek (forthcoming)</span> both present positive suggestions for moving forward with imprecise scoring rules.
<span class="citation">Levinstein (forthcoming)</span> suggests that the problem really only arises for determinately imprecise credences, but not for indeterminate credence.</p>

<h2><a id="Sum">4. Summary</a></h2>

<p>

Imprecise probabilities offer a model of rational belief that does
away with some of the idealisation required by the orthodox precise
probability approach. Many motivations for such a move have been put
forward, and many views on IP have been discussed. There are still
several open philosophical questions relating to IP, and this is
likely to be a rich field of research for years to come.</p>

</div>

<div id="bibliography">

<h2><a id="Bib">Bibliography</a></h2>

<ul class="hanging">

<li>Al-Najjar, Nabil I., and Jonathan Weinstein, 2009, &ldquo;The
Ambiguity Aversion Literature: A Critical
Assessment&rdquo;, <em>Economics and Philosophy</em>, 25:
249&ndash;284.</li>

<li>Augustin, Thomas, Frank P.A. Coolen, Gert de Cooman, and Matthias
C.M. Troffaes (eds), 2014, <em>Introduction to Imprecise
Probabilities</em>, John Wiley and Sons. New York.</li>

<li>Ben&eacute;treau-Dupin, Yann, 2015, &ldquo;The Bayesian who knew too much&rdquo;, <em>Synthese</em>, 192:5 1527&ndash;1542.</li>

<li>Binmore, Ken and Lisa Stewart and Alex Voorhoeve, 2012, &ldquo;How much ambiguity aversion? Finding indifferences between Ellsberg&rsquo;s risk and ambiguous bets&rdquo;, <em>Journal of Risk and Uncertainty</em>, 45: 215&ndash;238.</li>

<li>Blackwell, D., and M. A. Girschick, 1954, <em>Theory of Games and
Statistical Decisions</em>, Wiley. New York.</li>

<li>Boole, George. 1958 [1854], <em>The Laws of Thought</em>,
Dover. New York.</li>

<li>Bovens, Luc, and Stephan Hartmann, 2003, <em>Bayesian
epistemology</em>, Oxford University Press. Oxford.</li>

<li>Bradley, Richard, 2009, &ldquo;Revising Incomplete
Attitudes&rdquo;, <em>Synthese</em>, 171: 235&ndash;256.</li>

<li>&ndash;&ndash;&ndash;, 2017, <em> Decision theory with a human face</em> Cambridge University Press. Cambridge.</li>

<li>Bradley, Richard, and Mareile Drechsler, 2014,
&ldquo;Types of Uncertainty&rdquo;, <em>Erkenntnis</em>. 79: 1225&ndash;1248.</li>

<li>Bradley, Seamus, 2015, &ldquo;How to choose among choice functions&rdquo;, <em>Proceedings of the Ninth International Symposium on Imprecise Probability: Theories and Applications</em>, 57&ndash;66 URL =
&lt;<a href="http://www.sipta.org/isipta15/data/paper/9.pdf" target="other">http://www.sipta.org/isipta15/data/paper/9.pdf</a>&gt;.</li>

<li>&ndash;&ndash;&ndash;, 2016, 
&ldquo;Vague chance?&rdquo;, <em>Ergo</em>, 3:20</li>

<li>&ndash;&ndash;&ndash;, 2019, 
&ldquo;A counterexample to three imprecise decision theories&rdquo;, <em>Theoria</em>, 85:1 18&ndash;30</li>

<li>Bradley, Seamus, and Katie Steele, 2014a, &ldquo;Should Subjective
Probabilities be Sharp?&rdquo; <em>Episteme</em>, 11:
277&ndash;289.</li>

<li>&ndash;&ndash;&ndash;, 2014b, &ldquo;Uncertainty, Learning
and the &lsquo;Problem&rsquo; of
Dilation&rdquo;, <em>Erkenntnis</em>. 79: 1287&ndash;1303.</li>

<li>&ndash;&ndash;&ndash;, 2016, 
&ldquo;Can free evidence be bad? Value of information for the imprecise probabilist&rdquo;, <em>Philosophy of Science</em>, 83:1 1&ndash;28</li>

<li>Brady, Michael and Rog&eacute;rio Arthmar, 2012, &ldquo;Keynes, Boole and the interval approach to probability&rdquo;, <em>History of Economic Ideas</em>, 20:3
65&ndash;84.</li>

<li>Brier, Glenn, 1950, &ldquo;Verification of Forecasts Expressed in
Terms of Probability&rdquo;, <em>Monthly Weather Review</em>, 78:
1&ndash;3.</li>

<li>Briggs, R.A., 2014, &ldquo;Normative Theories of Rational
Choice: Expected Utility&rdquo;, <em>The Stanford Encyclopedia of
Philosophy</em>, (Fall 2014 Edition), Edward N. Zalta (ed.), URL =
&lt;<a href="https://plato.stanford.edu/archives/fall2014/entries/rationality-normative-utility/" target="other">https://plato.stanford.edu/archives/fall2014/entries/rationality-normative-utility/</a>&gt;.</li>

<li>Broome, John, 2000, &ldquo;Incommensurable Values&rdquo;,
in <em>Well-Being and Morality: Essays in Honour of James
Griffin</em>, R. Crisp and B. Hooker (eds), 21&ndash;38, Clarendon
Press. Oxford.</li>

<li>Br&ouml;cker, Jochen, and Leonard A. Smith, 2007, &ldquo;Scoring
Probabilistic Forecasts; On the Importance of Being
Proper&rdquo;, <em>Weather and Forecasting</em>, 22:
382&ndash;388.</li>

<li>Camerer, Colin, and Martin Weber, 1992, &ldquo;Recent Developments
in Modeling Preferences: Uncertainty and Ambiguity&rdquo;, <em>Journal
of Risk and Uncertainty</em>, 5: 325&ndash;370.</li>

<li>Carr, Jennifer, 2015 &ldquo;Chancy accuracy and imprecise credence&rdquo;, <em>Philosophical Topics</em> 29 67&ndash;81.</li>

<li>Castro, Clinton and Casey Hart, forthcoming, &ldquo;The imprecise impermissivists dilemma&rdquo;, <em>Synthese</em>.</li>

<li>Cattaneo, Marco, 2008, &ldquo;Fuzzy Probabilities based on the
Likelihood Function&rdquo;, in <em>Soft Methods for Handling
Variability and Imprecision</em>, D. Dubois, M. A. Lubiano, H. Prade,
M. A. Gil, P. Grzegorzewski, and O. Hryniewicz (eds), 43&ndash;50,
Springer.</li>

<li>&ndash;&ndash;&ndash;, 2014, &ldquo;A Continuous Updating Rule for
Imprecise Probabilities&rdquo;, in <em>Information Processing and
Management of Uncertainty in Knowledge Based Systems</em>, Anne
Laurent, Oliver Strauss, Bernadette Bouchon-Meunier, and Ronald
R. Yager (eds), 426&ndash;435, Springer.</li>

<li>Chandler, Jacob, 2014, &ldquo;Subjective Probabilities Need
Not Be Sharp&rdquo;, <em>Erkenntnis</em>. 79: 1273&ndash;1286.</li>

<li>Chu, Francis, and Joseph Y. Halpern, 2004, &ldquo;Great
expectations. Part II: Generalized expected utility as a universal
decision rule&rdquo;, <em>Artificial Intelligence</em>, 159:
207&ndash;230.</li>

<li>&ndash;&ndash;&ndash;, 2008, &ldquo;Great expectations. Part I: On
the customizability of General Expected Utility&rdquo;, <em>Theory and
Decision</em>, 64: 1&ndash;36.</li>

<li>Clifford, William Kingdom, 1901, &ldquo;The Ethics of
Belief&rdquo;, in <em>Lectures and Essays</em>, Leslie Stephen and
Frederick Pollock (eds), 2:161&ndash;205, 3<sup>rd</sup> Edition,
Macmillan. London.</li>

<li>de Cooman, Gert, and Enrique Miranda, 2007, &ldquo;Symmetry of
models versus models of symmetry&rdquo;, in <em>Probability and
Inference: Essays in Honor of Henry E. Kyburg Jnr.</em>, William
Harper and Gregory Wheeler (eds), 67&ndash;149, Kings College
Publications.</li>

<li>Cozman, Fabio, 2000, &ldquo;Credal Networks&rdquo;, <em>Artificial
Intelligence</em>, 120: 199&ndash;233.</li>

<li>&ndash;&ndash;&ndash;, 2012, &ldquo;Sets of probability
distributions, independence and convexity&rdquo;, <em>Synthese</em>,
186: 577&ndash;600.</li>

<li>Cozman, Fabio, and Peter Walley, 2005, &ldquo;Graphoid properties
of epistemic irrelevance and independence&rdquo;, <em>Annals of
Mathematics and Artificial Intelligence</em>, 45: 173&ndash;195.</li>

<li>Dardashti, Radin, Luke Glynn, Karim Th&eacute;bault, and Mathias Frisch,
2014, &ldquo;Unsharp Humean chances in statistical physics: a reply to
Beisbart&rdquo;, in <em>New Directions in the Philosophy of
Science</em>, Maria Carla Galavotti, Dennis Dieks, Wenceslao
J. Gonzalez, Stephan Hartmann, Thomas Uebel, and Marcel Weber (eds),
531&ndash;542, Springer. Dordrecht.</li>

<li>Elga, Adam, 2010, &ldquo;Subjective Probabilities should be
Sharp&rdquo;, <em>Philosophers&rsquo; Imprint</em>, 10.</li>

<li>Elkin, Lee and Gregory Wheeler, 2016 &ldquo;Resolving peer disagreements through imprecise probabilities&rdquo;, <em>No&ucirc;s</em>, 52:2 260&ndash;278.</li>

<li>Ellsberg, Daniel, 1961, &ldquo;Risk, ambiguity and the Savage
axioms&rdquo;, <em>Quarterly Journal of Economics</em>, 75:
643&ndash;696.</li>

<li>Eriksson, Lena, and Alan H&aacute;jek, 2007, &ldquo;What Are Degrees of
Belief?&rdquo; <em>Studia Logica</em>, 86: 183&ndash;213.</li>

<li>Evren, &Ouml;zg&uuml;r, and Efe Ok, 2011, &ldquo;On the multi-utility
representation of preference relations&rdquo;, <em>Journal of
Mathematical Economics</em>, 47: 554&ndash;563.</li>

<li>Ferson, Scott and Lev R. Ginzburg,1996, &ldquo;Different methods are needed to propagate ignorance and variability&rdquo;, <em>Reliability Engineering and System Safety</em>, 54 133&ndash;144.</li>

<li>Ferson, Scott and Janos G. Hajagos, 2004, &ldquo;Arithmetic with uncertain numbers: Rigorous and (often) best possible answers&rdquo;, <em>Reliability Engineering and System Safety</em>, 85 135&ndash;152.</li>

<li>Fine, Terrence L., 1973, <em>Theories of Probability: An
Examination of Foundations</em>, Academic Press. New York.</li>

<li>&ndash;&ndash;&ndash;, 1988, &ldquo;Lower Probability Models for
Uncertainty and Nondeterministic Processes&rdquo;, <em>Journal of
Statistical Planning and Inference</em>, 20: 389&ndash;411.</li>

<li>de Finetti, Bruno, 1964, &ldquo;Foresight: Its Logical Laws, Its
Subjective Sources&rdquo;, in <em>Studies in Subjective Probability
Studies in Subjective Probability</em>, Henry E. Kyburg and Howard
E. Smokler (eds), 97&ndash;158, Wiley. New York.</li>

<li>&ndash;&ndash;&ndash;, 1990 [1974], <em>Theory of
Probability</em>, Wiley Classics Library, Vol. 1, Wiley. New York.</li>

<li>Fox, Craig R., and Amos Tversky, 1995, &ldquo;Ambiguity aversion
and comparative ignorance&rdquo;, <em>Quarterly Journal of
Economics</em>, 110: 585&ndash;603.</li>

<li>van Fraassen, Bas, 1984, &ldquo;Belief and the
Will&rdquo;, <em>Journal of Philosophy</em>, 81: 235&ndash;256.</li>

<li>&ndash;&ndash;&ndash;, 1990, &ldquo;Figures in a Probability
Landscape&rdquo;, in <em>Truth or Consequences</em>, Michael Dunn and
Anil Gupta (eds), 345&ndash;356, Springer. Dordrecht.</li>

<li>Frigg, Roman, 2008, &ldquo;Humean chance in Boltzmannian
statistical mechanics&rdquo;, <em>Philosophy of Science</em>, 75:
670&ndash;681.</li>

<li>Frigg, Roman, Seamus Bradley, Hailiang Du, and Leonard A. Smith,
2014, &ldquo;Laplace&rsquo;s Demon and the Adventures of his
Apprentices&rdquo;, <em>Philosophy of Science</em>, 81:
31&ndash;59.</li>

<li>Fumagalli, Roberto, 2013, &ldquo;The Futile Search for True
Utility&rdquo;, <em>Economics and Philosophy</em>, 29:
325&ndash;347.</li>

<li>G&auml;rdenfors, Peter, 1979, &ldquo;Forecasts, Decisions and Uncertain
Probabilities&rdquo;, <em>Erkenntnis</em>, 14: 159&ndash;181.</li>

<li>G&auml;rdenfors, Peter, and Nils-Eric Sahlin, 1982, &ldquo;Unreliable
probabilities, risk taking and decision
making&rdquo;, <em>Synthese</em>, 53: 361&ndash;386.</li>

<li>Genest, Christian, and James V. Zidek, 1986, &ldquo;Combining
Probability Distributions: A Critique and Annotated
Bibliography&rdquo;, <em>Statistical Science</em>, 1:
114&ndash;135.</li>

<li>Gilboa, Itzhak, 1987, &ldquo;Expected Utility with Purely
Subjective Non-additive Probabilities&rdquo;, <em>Journal of
Mathematical Economics</em>, 16: 65&ndash;88.</li>

<li>Glymour, Clark, 1980, &ldquo;Why I am not a Bayesian&rdquo;,
in <em>Theory and Evidence</em>, 63&ndash;93. Princeton University
Press. Princeton.</li>

<li>Gong, Ruobin and Xiao-Li Meng, 2017 &ldquo;Judicious judgment meets unsettling update: dilation, sure loss and Simpson&rsquo;s paradox&rdquo;, URL =
&lt;<a href="https://arxiv.org/abs/1712.08946" target="other">https://arxiv.org/abs/1712.08946</a>&gt;.</li>

<li>Good, Irving John, 1962, &ldquo;Subjective probability as the
measure of a non-measurable set&rdquo;, in <em>Logic, Methodology and
Philosophy of Science: Proceedings of the 1960 International
Congress</em>, 319&ndash;329.</li>

<li>&ndash;&ndash;&ndash;, 1967, &ldquo;On the principle of total
evidence&rdquo;, <em>British Journal for the Philosophy of
Science</em>, 17: 319&ndash;321.</li>

<li>&ndash;&ndash;&ndash;, 1974, &ldquo;A little learning can be
dangerous&rdquo;, <em>British Journal for the Philosophy of
Science</em>, 25: 340&ndash;342.</li>

<li>&ndash;&ndash;&ndash;, 1983 [1971], &ldquo;Twenty-Seven principles
of rationality&rdquo;, in <em>Good Thinking: The Foundations of
Probability and its Applications Good Thinking: The Foundations of
Probability and its Applications</em>, 15&ndash;19. University of
Minnesota Press. Minnesota.</li>

<li>Grize, Yves L., and Terrence L. Fine, 1987, &ldquo;Continuous
Lower Probability-Based Models for Stationary Processes with Bounded
and Divergent Time Averages&rdquo;, <em>The Annals of
Probability</em>, 15: 783&ndash;803.</li>

<li>Haenni, Rolf, 2009, &ldquo;Non-additive degrees of belief&rdquo;,
in Huber and Schmidt-Petri 2009: 121&ndash;160.</li>


<li>Haenni, Rolf, Jan-Willem Romeijn, Gregory Wheeler, and Jon
Williamson, 2011, <em>Probabilistic Logic and Probabilistic
Networks</em>, Synthese Library. Dordrecht.</li>


<li>H&aacute;jek, Alan, 2003, &ldquo;What conditional probabilities could not
be&rdquo;, <em>Synthese</em>, 137: 273&ndash;323.</li>

<li>&ndash;&ndash;&ndash;, 2008, &ldquo;Arguments for&mdash;or
against&mdash;probabilism?&rdquo; <em>British Journal for the Philosophy of
Science</em>, 59: 793&ndash;819.</li>

<li>&ndash;&ndash;&ndash;, 2011, &ldquo;Interpretations of
Probability&rdquo;, <em>The Stanford Encyclopedia of Philosophy</em>
(Winter 2012 Edition), Edward N. Zalta (ed.), URL =
&lt;<a href="https://plato.stanford.edu/archives/win2012/entries/probability-interpret/" target="other">https://plato.stanford.edu/archives/win2012/entries/probability-interpret/</a>&gt;. </li>

<li>H&aacute;jek, Alan, and Michael Smithson, 2012, &ldquo;Rationality and
Indeterminate Probabilities&rdquo;, <em>Synthese</em>, 187:
33&ndash;48.</li>

<li>Halpern, Joseph Y., 2003, <em>Reasoning about uncertainty</em>,
MIT press. Cambridge.</li>

<li>Hammond, Peter, 1988, &ldquo;Orderly Decision
Theory&rdquo;, <em>Economics and Philosophy</em>, 4:
292&ndash;297.</li>

<li>Harsanyi, John, 1955, &ldquo;Cardinal welfare, individualistic
ethics and interpersonal comparisons of utility&rdquo;, <em>Journal of
Political Economy</em>, 63: 309&ndash;321.</li>

<li>Hart. Casey and Michael Titelbaum, 2015 &ldquo;Intuitive dilation?&rdquo;, <em>Thought</em>, 4 252&ndash;262.</li>

<li>Hartmann, Stephan, and Patrick Suppes, 2010, &ldquo;Entanglement,
Upper Probabilities and Decoherence in Quantum Mechanics&rdquo;,
in <em>EPSA Philosophical Issues in the Sciences: Launch of the
European Philosophy of Science Association</em>, Mauricio Su&aacute;rez,
Mauro Dorato, and Mikl&oacute;s R&eacute;dei (eds), 93&ndash;103, Springer.</li>

<li>Hawthorne, James, 2009, &ldquo;The Lockean Thesis and the Logic of
Belief&rdquo;, in Huber and Schmidt-Petri 2009: 49&ndash;74.</li>

<li>Herron, Timothy, Teddy Seidenfeld, and Larry Wasserman, 1994,
&ldquo;The Extent of Dilation of Sets of Probabilities and the
Asymptotics of Robust Bayesian Inference&rdquo;, in <em>PSA:
Proceedings of the Biennial Meeting of the Philosophy of Science
Association</em>, 250&ndash;259.</li>

<li>Hill, Brian,  2013, &ldquo;Confidence and decision&rdquo;, <em>Games and Economic Behavior</em>, 82
675&ndash;692.</li>

<li>Hosni, Hykel, 2014, &ldquo;Towards a Bayesian theory of second
order uncertainty: Lessons from non-standard logics&rdquo;,
in <em>David Makinson on classical methods for non-classical
problems</em>, Sven Ove Hansson (ed.), 195&ndash;221, Springer. Dordrecht.</li>

<li>Howson, Colin, 2012, &ldquo;Modelling Uncertain
Inference&rdquo;, <em>Synthese</em>, 186: 475&ndash;492.</li>

<li>Howson, Colin, and Peter Urbach, 2006, <em>Scientific Reasoning:
the Bayesian Approach</em>, 3<sup>rd</sup> edition, Open Court. Chicago.</li>

<li>Huber, Franz, 2009, &ldquo;Belief and Degrees of Belief&rdquo;, in
Huber and Schmidt-Petri 2009: 1&ndash;33.</li>

<li>&ndash;&ndash;&ndash;, 2014, &ldquo;Formal Representations of
Belief&rdquo;, <em>Stanford Encyclopedia of Philosophy</em> (Spring
2014 Edition), E. N. Zalta (ed.), URL =
&lt;<a href="https://plato.stanford.edu/archives/spr2014/entries/formal-belief/" target="other">https://plato.stanford.edu/archives/spr2014/entries/formal-belief/</a>&gt;.</li>

<li>Huber, Franz and Cristoph Schmidt-Petri (eds), 2009, <em>Degrees
of Belief</em>, Springer. Dordrecht.</li>

<li>Huntley, Nathan, Robert Hable, and Matthias Troffaes, 2014,
&ldquo;Decision making&rdquo;, in Augustin et al. 2014:
190&ndash;206.</li>

<li>James, William, 1897, &ldquo;The Will to Believe&rdquo;,
in <em>The Will to Believe and other essays in popular
philosophy</em>, 1&ndash;31. Longmans, Green and Co. New York.</li>

<li>Jaynes, Edwin T., 2003, <em>Probability Theory: The Logic of
Science</em>, Cambridge University Press. Cambridge.</li>

<li>Jeffrey, Richard, 1983, <em>The Logic of Decision</em>,
2<sup>nd</sup> edition. University of Chicago Press. Chicago.</li>

<li>&ndash;&ndash;&ndash;, 1984, &ldquo;Bayesianism with a Human

Face&rdquo;, in <em>Testing Scientific Theories</em>, John Earman

(ed.), 133&ndash;156, University of Minnesota Press. Minnesota.</li>

<li>&ndash;&ndash;&ndash;, 1987, &ldquo;Indefinite Probability
Judgment: A Reply to Levi&rdquo;, <em>Philosophy of Science</em>, 54:
586&ndash;591.</li>

<li>Joyce, James M., 1999, <em>The Foundations of Causal Decision
Theory</em>, <em>Cambridge studies in probability, induction and

decision theory</em>, Cambridge University Press. Cambridge.</li>

<li>&ndash;&ndash;&ndash;, 2005, &ldquo;How Probabilities Reflect
Evidence&rdquo;, <em>Philosophical Perspectives</em>, 19:
153&ndash;178.</li>

<li>&ndash;&ndash;&ndash;, 2009, &ldquo;Accuracy and Coherence:

Prospects for an Alethic Epistemology of Partial Belief&rdquo;, in

Huber and Schmidt-Petri 2009: 263&ndash;297.</li>

<li>&ndash;&ndash;&ndash;, 2011, &ldquo;A Defense of Imprecise

Credence in Inference and Decision&rdquo;, <em>Philosophical
Perspectives</em>, 24: 281&ndash;323.</li>

<li>Kadane, Joseph B., Mark J. Schervish, and Teddy Seidenfeld,
1999, <em>Rethinking the Foundations of Statistics</em>, Cambridge
University Press. Cambridge.</li>

<li>Kaplan, Mark, 1983, &ldquo;Decision theory as
philosophy&rdquo;, <em>Philosophy of Science</em>, 50:
549&ndash;577.</li>

<li>&ndash;&ndash;&ndash;, 1996, <em>Decision Theory as
Philosophy</em>, Cambridge University Press. Cambridge.</li>

<li>Keynes, J. M., 1921, <em>A Treatise on Probability</em>,
Macmillan. London.</li>

<li>Konek, Jason, forthcoming &ldquo;Epistemic conservativity and imprecise credence&rdquo;, <em>Philosophy and Phenomenological Research</em></li>

<li>Koopman, B. O., 1940, &ldquo;The Bases of
Probability&rdquo;, <em>Bulletin of the American Mathematical
Society</em>, 46: 763&ndash;774.</li>

<li>Kumar, Anurag, and Terrence L. Fine, 1985, &ldquo;Stationary Lower
Probabilities and Unstable Averages&rdquo;, <em>Zeitschrift f&uuml;r
Wahrscheinlichkeitstheorie und verwandte Gebiete</em>, 69:
1&ndash;17.</li>

<li>Kyburg, Henry E., 1983, &ldquo;Rational belief&rdquo;, <em>The
Brain and Behavioural Sciences</em>, 6: 231&ndash;273.</li>

<li>&ndash;&ndash;&ndash;, 1987, &ldquo;Bayesian and non-Bayesian
evidential updating&rdquo;, <em>Artificial Intelligence</em>, 31:
271&ndash;293.</li>

<li>&ndash;&ndash;&ndash;, 2003, &ldquo;Are there degrees of
belief?&rdquo; <em>Journal of Applied Logic</em>: 139&ndash;149.</li>

<li>Kyburg, Henry E., and Michael Pittarelli, 1992, <em>Set-based
Bayesianism</em>.</li>

<li>Kyburg, Henry E., and Choh Man Teng, 2001, <em>Uncertain
Inference</em>, Cambridge University Press. Cambridge.</li>

<li>Leitgeb, Hannes, 2014, &ldquo;The stability theory of
belief&rdquo;, <em>The Philosophical Review</em>, 123:
131&ndash;171.</li>

<li>Levi, Isaac, 1974, &ldquo;On Indeterminate
probabilities&rdquo;, <em>Journal of Philosophy</em>, 71:
391&ndash;418.</li>

<li>&ndash;&ndash;&ndash;, 1980, <em>The Enterprise of Knowledge</em>,
The MIT Press. Cambridge.</li>

<li>&ndash;&ndash;&ndash;, 1982, &ldquo;Ignorance, Probability and
Rational Choice&rdquo;, <em>Synthese</em>, 53: 387&ndash;417.</li>

<li>&ndash;&ndash;&ndash;, 1985, &ldquo;Imprecision and Indeterminacy
in Probability Judgment&rdquo;, <em>Philosophy of Science</em>, 52:
390&ndash;409.</li>

<li>&ndash;&ndash;&ndash;, 1986, <em>Hard Choices: decision making
under unresolved conflict</em>, Cambridge University Press. Cambridge.</li>

<li>&ndash;&ndash;&ndash;, 1999, &ldquo;Value commitments, value
conflict and the separability of belief and
value&rdquo;, <em>Philosophy of Science</em>, 66: 509&ndash;533.</li>

<li>Levinstein, Ben, forthcoming, &ldquo;Imprecise epistemic values and imprecise credences&rdquo;, <em>Australasian Journal of Philosophy</em>.</li>

<li>Lewis, David, 1986, &ldquo;A Subjectivist&rsquo;s Guide to Objective
Chance (and postscript)&rdquo;, in <em>Philosophical Papers II</em>,
83&ndash;132. Oxford University Press. Oxford.</li>

<li>&ndash;&ndash;&ndash;, 1994, &ldquo;Humean Supervenience
Debugged&rdquo;, <em>Mind</em>, 103: 473&ndash;490.</li>

<li>List, Christian, and Philip Pettit, 2011, <em>Group Agency</em>,
Oxford University Press. Oxford.</li>

<li>Loewer, B., 2001, &ldquo;Determinism and
chance&rdquo;, <em>Studies in the History and Philosophy of Modern
Physics</em>, 32: 609&ndash;620.</li>

<li>Lyon, Aidan, 2017, &ldquo;Vague Credences&rdquo;, <em>Synthese</em>, 194:10 3931&ndash;3954.</li>

<li>Machina, Mark J., 1989, &ldquo;Dynamic Consistency and
Non-Expected Utility Models of Choice Under
Uncertainty&rdquo;, <em>Journal of Economic Literature</em>, 27:
1622&ndash;1668.</li>

<li>Mayo-Wilson, Conor and Gregory Wheeler, 2016, &ldquo;Scoring imprecise credences: a mildly immodest proposal&rdquo;, <em>Philosophy and Phenomenological Research</em>, 93:1 55&ndash;78.</li>

<li>Meacham, Christopher, and Jonathan Weisberg, 2011,
&ldquo;Representation Theorems and the Foundations of Decision
Theory&rdquo;, <em>Australasian Journal of Philosophy</em>, 89:
641&ndash;663.</li>

<li>Miranda, Enrique, 2008, &ldquo;A survey of the theory of coherent
lower previsions&rdquo;, <em>International Journal of Approcimate
Reasoning</em>, 48: 628&ndash;658.</li>

<li>Miranda, Enrique, and Gert de Cooman, 2014, &ldquo;Lower
previsions&rdquo;, in Augustin et al. 2014, pp. 28&ndash;55.</li>

<li>Moss, Sarah, 2015, &ldquo;Credal
Dilemmas&rdquo;, <em>No&ucirc;s</em>, 49:4 665&ndash;683.</li>

<li>Norton, John, 2007, &ldquo;Probability
disassembled&rdquo;, <em>British Journal for the Philosophy of
Science</em>, 58: 141&ndash;171.</li>

<li>&ndash;&ndash;&ndash;, 2008a, &ldquo;Ignorance and
Indifference&rdquo;, <em>Philosophy of Science</em>, 75:
45&ndash;68.</li>

<li>&ndash;&ndash;&ndash;, 2008b, &ldquo;The dome: An Unexpectedly
Simple Failure of Determinism&rdquo;, <em>Philosophy of Science</em>,
75: 786&ndash;798.</li>

<li>Oberguggenberger, Michael, 2014, &ldquo;Engineering&rdquo;, in
Augustin et al. 2014: 291&ndash;304. </li>

<li>Oberkampf, William and Christopher Roy, 2010 <em>Verification and Validation in Scientific Computing</em>, Cambridge University Press. Cambridge.</li>

<li>Pedersen, Arthur Paul, 2014, &ldquo;Comparative
Expectations&rdquo;, <em>Studia Logica</em>. 102: 811&ndash;848.</li>

<li>Pedersen, Arthur Paul, and Gregory Wheeler, 2014,
&ldquo;Demystifying Dilation&rdquo;, <em>Erkenntnis</em>.79: 1305&ndash;1342.</li>

<li>Pettigrew, Richard, 2011, &ldquo;Epistemic Utility Arguments for
Probabilism&rdquo;, <em>The Stanford Encyclopedia of Philosophy</em>
(Winter 2011 Edition), Edward N. Zalta (ed.), URL =
&lt;<a href="https://plato.stanford.edu/archives/win2011/entries/epistemic-utility/" target="other">https://plato.stanford.edu/archives/win2011/entries/epistemic-utility/</a>&gt;.</li>

<li>Pfeifer, Niki, and Gernot D. Kleiter, 2007, &ldquo;Human reasoning
with imprecise probabilities: Modus ponens and denying the
antecedent&rdquo;, <em>Proceedings of the 5th International Symposium
on Imprecise Probability: Theory and Application</em>:
347&ndash;356.</li>

<li>Quaeghebeur, Erik, 2014, &ldquo;Desirability&rdquo;, in Augustin
et al. 2014: 1&ndash;27.</li>

<li>Ramsey, F. P., 1926, &ldquo;Truth and Probability&rdquo;,
in <em>The Foundations of Mathematics and other Logical Essays</em>,
156&ndash;198. Routledge. London.</li>

<li>Rinard, Susanna, 2013, &ldquo;Against Radical Credal
Imprecision&rdquo;, <em>Thought</em>, 2: 157&ndash;165.</li>

<li>&ndash;&ndash;&ndash;, 2015, &ldquo;A decision theory for imprecise probabilities&rdquo;, <em>Philosophers&rsquo; Imprint</em>, 15 1&ndash;16.</li>

<li>Ruggeri, Fabrizio, David R&iacute;os and Jacinto Mart&iacute;n, 2005, &ldquo;Robust Bayesian analysis&rdquo;, <em>Handbook of Statistics</em>, 25 623&ndash;667, Elsevier. Amsterdam</li>

<li>Sahlin, Nils-Eric, and Paul Weirich, 2014, &ldquo;Unsharp
Sharpness&rdquo;, <em>Theoria</em>, 80: 100&ndash;103.</li>

<li>Savage, Leonard J., 1972 [1954], <em>The Foundations of
Statistics</em>, 2nd edition, Dover. New York.</li>

<li>&ndash;&ndash;&ndash;, 1971, &ldquo;Elicitation of Personal
Probabilities and Expectation&rdquo;, <em>Journal of the American
Statistical Association</em>, 66: 783&ndash;801.</li>

<li>Schaffer, Jonathan, 2007, &ldquo;Deterministic
Chance?&rdquo; <em>British Journal for the Philosophy of Science</em>,
58: 114&ndash;140.</li>

<li>Schervish, Mark J., Teddy Seidenfeld, and Joseph B. Kadane, 2008,
&ldquo;The fundamental theorems of prevision and asset
pricing&rdquo;, <em>International Journal of Approximate
Reasoning</em>, 49: 148&ndash;158.</li>

<li>Schoenfield, Miriam, 2012, &ldquo;Chilling out on epistemic
rationality&rdquo;, <em>Philosophical Studies</em>, 158:
197&ndash;219.</li>

<li>&ndash;&ndash;&ndash;, 2017, &ldquo;The accuracy and rationality of imprecise credence&rdquo;, <em>No&ucirc;s</em>, 51:4 667&ndash;685.</li>

<li>Seidenfeld, Teddy, 1988, &ldquo;Decision theory without
&lsquo;independence&rsquo; or without &lsquo;ordering&rsquo;. What&rsquo;s
the difference?&rdquo; <em>Economics and Philosophy</em>:
267&ndash;290.</li>

<li>&ndash;&ndash;&ndash;, 1994, &ldquo;When normal and extensive form
decisions differ&rdquo;, <em>Logic, Methodology and Philosophy of
Science</em>, IX: 451&ndash;463.</li>

<li>&ndash;&ndash;&ndash;, 2004, &ldquo;A contrast between two
decision rules for use with (convex) sets of probabilities:
Gamma-maximin versus E-admissibility&rdquo;, <em>Synthese</em>, 140:
69&ndash;88.</li>

<li>Seidenfeld, Teddy, Joseph B. Kadane, and Mark J. Schervish, 1989,
&ldquo;On the shared preferences of two Bayesian decision
makers&rdquo;, <em>The Journal of Philosophy</em>, 86:
225&ndash;244.</li>

<li>Seidenfeld, Teddy, Mark J. Schervish, and Joseph B. Kadane, 1995,
&ldquo;A Representation of Partially Ordered
Preferences&rdquo;, <em>Annals of Statistics</em>, 23:
2168&ndash;2217.</li>

<li>&ndash;&ndash;&ndash;, 2010, &ldquo;Coherent choice functions
under uncertainty&rdquo;, <em>Synthese</em>, 172: 157&ndash;176.</li>

<li>&ndash;&ndash;&ndash;, 2012, &ldquo;Forecasting with imprecise
probabilities&rdquo;, <em>International Journal of Approximate
Reasoning</em>, 53: 1248&ndash;1261.</li>

<li>Seidenfeld, Teddy, and Larry Wasserman, 1993, &ldquo;Dilation for
sets of probabilities&rdquo;, <em>Annals of Statistics</em>, 21:
1139&ndash;1154.</li>

<li>Skyrms, Brian, 2011, &ldquo;Resiliency, Propensities and Causal
Necessity&rdquo;, in <em>Philosophy of Probability: Contemporary
Readings</em>, Antony Eagle (ed.), 529&ndash;536, Routledge. London.</li>

<li>Smith, Cedric A.B, 1961, &ldquo;Consistency in Statistical
Inference and Decision&rdquo;, <em>Journal of the Royal Statistical
Society. Series B (Methodological)</em>, 23: 1&ndash;37.</li>

<li>Smithson, Michael, and Paul D. Campbell, 2009, &ldquo;Buying and
Selling Prices under Risk, Ambiguity and
Conflict&rdquo;, <em>Proceedings of the 6th International Symposium on
Imprecise Probability: Theory and Application</em>.</li>

<li>Smithson, Michael, and Helen Pushkarskaya, 2015,
&ldquo;Ignorance and the Brain: Are there Distinct Kinds of
Unknowns?&rdquo; in <em>Routledge International Handbook of Ignorance
Studies</em>, Matthias Gross and Linsey McGoey (eds), Routledge.</li>

<li>Sorensen, Roy, 2012, &ldquo;Vagueness&rdquo;, <em>The Stanford
Encyclopedia of Philosophy</em> (Winter 2013 Edition), Edward N. Zalta
(ed.), URL =
&lt;<a href="https://plato.stanford.edu/archives/win2013/entries/vagueness/" target="other">https://plato.stanford.edu/archives/win2013/entries/vagueness/</a>&gt;.</li>

<li>Stainforth, David A., Miles R. Allen, E. R. Tredger, and Leonard
A. Smith, 2007, &ldquo;Confidence uncertainty and decision-support
relevance in climate models&rdquo;, <em>Philosophical Transactions of
the Royal Society</em>, 365: 2145&ndash;2161.</li>

<li>Steele, Katie, 2007, &ldquo;Distinguishing indeterminate belief
from &lsquo;risk averse&rsquo; preference&rdquo;, <em>Synthese</em>,
158: 189&ndash;205.</li>

<li>Stewart, Rush T. and Ignacio Ojea Quintana, 2018 &ldquo;Probabilistic opinion pooling with imprecise probabilities&rdquo;, <em>Journal of Philosophical Logic</em>,
47:1 17&ndash;45.</li>

<li>Sturgeon, Scott, 2008, &ldquo;Reason and the grain of
belief&rdquo;, <em>No&ucirc;s</em>, 42: 139&ndash;165.</li>

<li>Sud, Rohan, 2014, &ldquo;A forward looking decision rule for imprecise credences&rdquo;, <em>Philosophical Studies</em>, 167 119&ndash;139.</li>

<li>Suppes, Patrick, 1974, &ldquo;The Measurement of
Belief&rdquo;, <em>Journal of the Royal Statistical Society B</em>,
36: 160&ndash;191.</li>

<li>Suppes, Patrick, and Mario Zanotti, 1991, &ldquo;Existence of
Hidden Variables Having Only Upper Probability&rdquo;, <em>Foundations
of Physics</em>, 21: 1479&ndash;1499.</li>

<li>Talbott, William, 2008, &ldquo;Bayesian
Epistemology&rdquo;, <em>The Stanford Encyclopedia of Philosophy</em>
(Fall 2013 Edition), Edward N. Zalta (ed.), URL =
&lt;<a href="https://plato.stanford.edu/archives/fall2013/entries/epistemology-bayesian/" target="other">https://plato.stanford.edu/archives/fall2013/entries/epistemology-bayesian/</a>&gt;. </li>

<li>Topey, Brett, 2012, &ldquo;Coin flips, credences and the
Reflection Principle&rdquo;, <em>Analysis</em>, 72:
478&ndash;488.</li>

<li>Trautmann, Stefan and Guijs van der Kuilen, 2016 &ldquo;Ambiguity Attitudes&rdquo;, <em>Blackwell Handbook of Judgement and Decision-Making</em>,  89&ndash;116.</li>

<li>Troffaes, Matthias, 2007, &ldquo;Decision Making under Uncertainty
using Imprecise Probabilities&rdquo;, <em>International Journal of
Approximate Reasoning</em>, 45: 17&ndash;29.</li>

<li>Troffaes, Matthias, and Gert de Cooman, 2014, <em>Lower
Previsions</em>, Wiley. New York.</li>

<li>Vallinder, Aron, 2018
&ldquo;Imprecise Bayesianism and global belief inertia&rdquo;, <em>British Journal for the Philosophy of Science</em>, 69:4 1205&ndash;1230.</li>

<li>Vicig, Paolo, Marco Zaffalon, and Fabio G. Cozman, 2007,
&ldquo;Notes on &lsquo;Notes on conditional
previsions&rsquo;&rdquo;, <em>International Journal of Approximate
Reasoning</em>, 44: 358&ndash;365.</li>

<li>Vicig, Paolo, and Teddy Seidenfeld, 2012, &ldquo;Bruno de Finetti
and imprecision: Imprecise Probability Does not
Exist!&rdquo; <em>International Journal of Approximate Reasoning</em>,
53: 1115&ndash;1123.</li>

<li>Voorhoeve, Alex, Ken Binmore, Arnaldur Stefansson and Lisa Stewart,2016
&ldquo;Ambiguity attitudes, framing and consistency&rdquo;, <em>Theory and Decision</em>, 81:3 313&ndash;337.</li>

<li>Walley, Peter, 1991, <em>Statistical Reasoning with Imprecise
Probabilities</em>, <em>Monographs on Statistics and Applied
Probability</em>, Vol. 42. Chapman and Hall. London.</li>

<li>Walley, Peter, and Terrence L. Fine, 1982, &ldquo;Towards a
frequentist theory of upper and lower probability&rdquo;, <em>The
Annals of Statistics</em>, 10: 741&ndash;761.</li>

<li>Wallsten, Thomas, and David V. Budescu, 1995, &ldquo;A review of
human linguistic probability processing: General principles and
empirical evidence&rdquo;, <em>The Knowledge Engineering Review</em>,
10: 43&ndash;62.</li>

<li>Weatherson, Brian, 2002, &ldquo;Keynes, uncertainty and interest
rates&rdquo;, <em>Cambridge Journal of Economics</em>:
47&ndash;62.</li>

<li>Weichselberger, Kurt, 2000, &ldquo;The theory of
interval-probability as a unifying concept for
uncertainty&rdquo;, <em>International Journal of Approximate
Reasoning</em>, 24: 149&ndash;170.</li>

<li>Wheeler, Gregory, 2014, &ldquo;Character matching and the Locke
pocket of belief&rdquo;, in <em>Epistemology, Context and
Formalism</em>, Franck Lihoreau and Manuel Rebuschi (eds),
185&ndash;194, Synthese Library. Dordrecht.</li>

<li>Wheeler, Gregory, and Jon Williamson, 2011, &ldquo;Evidential
Probability and Objective Bayesian Epistemology&rdquo;,
in <em>Philosophy of Statistics</em>, Prasanta S. Bandyopadhyay and
Malcom Forster (eds), 307&ndash;332, North-Holland. Amsterdam.</li>

<li>White, Roger, 2010, &ldquo;Evidential Symmetry and Mushy
Credence&rdquo;, in <em>Oxford Studies in Epistemology</em>, T. Szabo
Gendler and J. Hawthorne (eds), 161&ndash;186, Oxford University
Press.</li>

<li>Williams, J. Robert G., 2014, &ldquo;Decision-making under
indeterminacy&rdquo;, <em>Philosophers&rsquo; Imprint</em>, 14:
1&ndash;34.</li>

<li>Williams, P. M., 1976, &ldquo;Indeterminate Probabilities&rdquo;,
in <em>Formal Methods in the Methodology of Empirical Sciences</em>,
Marian Przel&#281;cki, Klemens Szaniawski, and Ryszard W&oacute;jcicki (eds),
229&ndash;246, D. Reidel Publishing Company.</li>

<li>&ndash;&ndash;&ndash;, 2007, &ldquo;Notes on conditional
previsions&rdquo;, <em>International Journal of Approximate
Reasoning</em>, 44: 366&ndash;383.</li>

<li>Williamson, Jon, 2010, <em>In Defense of Objective
Bayesianism</em>, Oxford University Press. Oxford.</li>

<li>&ndash;&ndash;&ndash;, 2014, &ldquo;How uncertain do we
need to be?&rdquo; <em>Erkenntnis</em>. 79: 1249&ndash;1271.</li>

<li>Wilson, Nic, 2001, &ldquo;Modified upper and lower probabilities
based on imprecise likelihoods&rdquo;, in <em>Proceedings of the 2nd
International Symposium on Imprecise Probabilities and their
Applications</em>.</li>

<li>Zynda, Lyle, 2000, &ldquo;Representation Theorems and Realism
about Degrees of Belief&rdquo;, <em>Philosophy of Science</em>, 67:
45&ndash;69.</li>

</ul>

</div>

<div id="academic-tools">

<h2 id="Aca">Academic Tools</h2>

<blockquote>
<table class="vert-top">
<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=imprecise-probabilities" target="other">How to cite this entry</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://leibniz.stanford.edu/friends/preview/imprecise-probabilities/" target="other">Preview the PDF version of this entry</a> at the
 <a href="https://leibniz.stanford.edu/friends/" target="other">Friends of the SEP Society</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/inpho.png" alt="inpho icon" /></td>
<td><a href="https://www.inphoproject.org/entity?sep=imprecise-probabilities&amp;redirect=True" target="other">Look up topics and thinkers related to this entry</a>
 at the Internet Philosophy Ontology Project (InPhO).</td>
</tr>

<tr>
<td><img src="../../symbols/pp.gif" alt="phil papers icon" /></td>
<td><a href="http://philpapers.org/sep/imprecise-probabilities/" target="other">Enhanced bibliography for this entry</a>
at <a href="http://philpapers.org/" target="other">PhilPapers</a>, with links to its database.</td>
</tr>

</table>
</blockquote>

</div>

<div id="other-internet-resources">

<h2><a id="OthIntResSec">Other Internet Resources</a></h2>

<ul>
<li>Bradley, Seamus, 2013,
 &ldquo;<a href="http://www.lse.ac.uk/CPNSS/research/currentResearchProjects/ChoiceGroup/PDF_files/bradleyS-ImpreciseChoice.pdf" target="other">Weak rationality and imprecise choice</a>,&rdquo;
  LSE Choice Group Working Paper.</li>

<li>Cozman, Fabio,
<a href="http://www.cs.cmu.edu/~qbayes/Tutorial/" target="other">An Informal Introduction to the Theory of Sets of Probabilities</a>
</li>

<li><a href="http://www.sipta.org/" target="other">SIPTA: the Society for Imprecise Probability, Theory and Applications</a></li>

<li>
<a href="http://philpapers.org/browse/imprecise-credences" target="other">PhilPapers category Imprecise Credences</a>
</li>

</ul>

</div>

<div id="related-entries">

<h2><a id="Rel">Related Entries</a></h2>

<p>

 <a href="../formal-belief/index.html">belief, formal representations of</a> |
 <a href="../epistemic-utility/index.html">epistemic utility arguments for probabilism</a> |
 <a href="../epistemology-bayesian/index.html">epistemology: Bayesian</a> |
 <a href="../probability-interpret/index.html">probability, interpretations of</a> |
 <a href="../rationality-normative-utility/index.html">rational choice, normative: expected utility</a> |
 <a href="../statistics/index.html">statistics, philosophy of</a> |
 <a href="../vagueness/index.html">vagueness</a>

</p>

</div>

<div id="acknowledgments">

<h3>Acknowledgments</h3>

<p>Many thanks to Teddy Seidenfeld, Greg Wheeler, Paul Pedersen, Aidan
Lyon, Catrin Campbell-Moore, Stephan Hartmann, the ANU Philosophy of
Probability Reading Group, and an anonymous referee for helpful
comments on drafts of this article.</p>

</div>

<script type="text/javascript" src="local.js"></script>
<script type="text/javascript" src="../../MathJax/MathJaxb198.js?config=TeX-MML-AM_CHTML"></script>
 
</div><!-- #aueditable --><!--DO NOT MODIFY THIS LINE AND BELOW-->

<!-- END ARTICLE HTML -->

</div> <!-- End article-content -->

  <div id="article-copyright">
    <p>
 <a href="../../info.html#c">Copyright &copy; 2019</a> by

<br />
<a href="https://www.seamusbradley.net/" target="other">Seamus Bradley</a>
&lt;<a href="m&#97;ilto:scmbradley&#37;40gmail&#37;2ecom"><em>scmbradley<abbr title=" at ">&#64;</abbr>gmail<abbr title=" dot ">&#46;</abbr>com</em></a>&gt;
    </p>
  </div>

</div> <!-- End article -->

<!-- NOTE: article banner is outside of the id="article" div. -->
<div id="article-banner" class="scroll-block">
  <div id="article-banner-content">
    <a href="../../fundraising/index.html">
    Open access to the SEP is made possible by a world-wide funding initiative.<br />
    The Encyclopedia Now Needs Your Support<br />
    Please Read How You Can Help Keep the Encyclopedia Free</a>
  </div>
</div> <!-- End article-banner -->

    </div> <!-- End content -->

    <div id="footer">

      <div id="footer-menu">
        <div class="menu-block">
          <h4><i class="icon-book"></i> Browse</h4>
          <ul role="menu">
            <li><a href="../../contents.html">Table of Contents</a></li>
            <li><a href="../../new.html">What's New</a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
            <li><a href="../../published.html">Chronological</a></li>
            <li><a href="../../archives/index.html">Archives</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-info-sign"></i> About</h4>
          <ul role="menu">
            <li><a href="../../info.html">Editorial Information</a></li>
            <li><a href="../../about.html">About the SEP</a></li>
            <li><a href="../../board.html">Editorial Board</a></li>
            <li><a href="../../cite.html">How to Cite the SEP</a></li>
            <li><a href="../../special-characters.html">Special Characters</a></li>
            <li><a href="../../tools/index.html">Advanced Tools</a></li>
            <li><a href="../../contact.html">Contact</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-leaf"></i> Support SEP</h4>
          <ul role="menu">
            <li><a href="../../support/index.html">Support the SEP</a></li>
            <li><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
            <li><a href="../../support/donate.html">Make a Donation</a></li>
            <li><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
          </ul>
        </div>
      </div> <!-- End footer menu -->

      <div id="mirrors">
        <div id="mirror-info">
          <h4><i class="icon-globe"></i> Mirror Sites</h4>
          <p>View this site from another server:</p>
        </div>
        <div class="btn-group open">
          <a class="btn dropdown-toggle" data-toggle="dropdown" href="https://plato.stanford.edu/">
            <span class="flag flag-usa"></span> USA (Main Site) <span class="caret"></span>
            <span class="mirror-source">Philosophy, Stanford University</span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="../../mirrors.html">Info about mirror sites</a></li>
          </ul>
        </div>
      </div> <!-- End mirrors -->
      
      <div id="site-credits">
        <p>The Stanford Encyclopedia of Philosophy is <a href="../../info.html#c">copyright &copy; 2022</a> by <a href="http://mally.stanford.edu/">The Metaphysics Research Lab</a>, Department of Philosophy, Stanford University</p>
        <p>Library of Congress Catalog Data: ISSN 1095-5054</p>
      </div> <!-- End site credits -->

    </div> <!-- End footer -->

  </div> <!-- End container -->

   <!-- NOTE: Script required for drop-down button to work (mirrors). -->
  <script>
    $('.dropdown-toggle').dropdown();
  </script>

</body>

<!-- Mirrored from seop.illc.uva.nl/entries/imprecise-probabilities/index.html by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 22 Jun 2022 20:05:40 GMT -->
</html>
