<!DOCTYPE html>
<!--[if lt IE 7]> <html class="ie6 ie"> <![endif]-->
<!--[if IE 7]>    <html class="ie7 ie"> <![endif]-->
<!--[if IE 8]>    <html class="ie8 ie"> <![endif]-->
<!--[if IE 9]>    <html class="ie9 ie"> <![endif]-->
<!--[if !IE]> --> <html> <!-- <![endif]-->

<!-- Mirrored from seop.illc.uva.nl/entries/perception-auditory/supplement.html by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 22 Jun 2022 20:05:44 GMT -->
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>
Auditory Perception &gt; Speech Perception: Empirical and Theoretical Considerations (Stanford Encyclopedia of Philosophy)
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="robots" content="noarchive, noodp" />
<meta name="DCTERMS.ispartof" content="https://plato.stanford.edu/entries/perception-auditory/" />

<!-- NOTE: Import webfonts using this link: -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300,600,200&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css" />

<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap-responsive.min.css" />
<link rel="stylesheet" type="text/css" href="../../css/font-awesome.min.css" />
<!--[if IE 7]> <link rel="stylesheet" type="text/css" href="../../css/font-awesome-ie7.min.css"> <![endif]-->
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/style.css" />
<link rel="stylesheet" type="text/css" media="print" href="../../css/print.css" />
<link rel="stylesheet" type="text/css" href="../../css/entry.css" />
<!--[if IE]> <link rel="stylesheet" type="text/css" href="../../css/ie.css" /> <![endif]-->
<script type="text/javascript" src="../../js/jquery-1.9.1.min.js"></script>
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>

<!-- NOTE: Javascript for sticky behavior needed on article and ToC pages -->
<script type="text/javascript" src="../../js/jquery-scrolltofixed-min.js"></script>
<script type="text/javascript" src="../../js/entry.js"></script>

<!-- SEP custom script -->
<script type="text/javascript" src="../../js/sep.js"></script>
</head>

<!-- NOTE: The nojs class is removed from the page if javascript is enabled. Otherwise, it drives the display when there is no javascript. -->
<body class="nojs article" id="pagetopright">
<div id="container">
<div id="header-wrapper">
  <div id="header">
    <div id="branding">
      <div id="site-logo"><a href="../../index.html"><img src="../../symbols/sep-man-red.png" alt="SEP logo" /></a></div>
      <div id="site-title"><a href="../../index.html">Stanford Encyclopedia of Philosophy</a></div>
    </div>
    <div id="navigation">
      <div class="navbar">
        <div class="navbar-inner">
          <div class="container">
            <button class="btn btn-navbar collapsed" data-target=".collapse-main-menu" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Menu </button>
            <div class="nav-collapse collapse-main-menu in collapse">
              <ul class="nav">
                <li class="dropdown open"><a id="drop1" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-book"></i> Browse</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop1">
                    <li><a href="../../contents.html">Table of Contents</a></li>
                    <li><a href="../../new.html">What's New</a></li>
                    <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
                    <li><a href="../../published.html">Chronological</a></li>
                    <li><a href="../../archives/index.html">Archives</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop2" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-info-sign"></i> About</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop2">
                    <li><a href="../../info.html">Editorial Information</a></li>
                    <li><a href="../../about.html">About the SEP</a></li>
                    <li><a href="../../board.html">Editorial Board</a></li>
                    <li><a href="../../cite.html">How to Cite the SEP</a></li>
                    <li><a href="../../special-characters.html">Special Characters</a></li>
                    <li><a href="../../tools/index.html">Advanced Tools</a></li>
                    <li><a href="../../contact.html">Contact</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop3" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-leaf"></i> Support SEP</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop3">
                    <li><a href="../../support/index.html">Support the SEP</a></li>
                    <li><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
                    <li><a href="../../support/donate.html">Make a Donation</a></li>
                    <li><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
                  </ul>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End navigation -->
    
    <div id="search">
      <form id="search-form" method="get" action="https://seop.illc.uva.nl/search/searcher.py">
        <input type="search" name="query" placeholder="Search SEP" />
        <div class="search-btn-wrapper"><button class="btn search-btn" type="submit"><i class="icon-search"></i></button></div>
      </form>
    </div>
    <!-- End search --> 
    
  </div>
  <!-- End header --> 
</div>
<!-- End header wrapper -->

<div id="content">

<!-- Begin article sidebar -->
<div id="article-sidebar" class="sticky">
  <div class="navbar">
    <div class="navbar-inner">
      <div class="container">
        <button class="btn btn-navbar" data-target=".collapse-sidebar" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Entry Navigation </button>
        <div id="article-nav" class="nav-collapse collapse-sidebar in collapse">
          <ul class="nav">
            <li><a href="index.html">Back to Entry <i class="icon-external-link"></i></a></li>
            <li><a href="index.html#toc">Entry Contents <i class="icon-external-link"></i></a></li>
            <li><a href="index.html#Bib">Entry Bibliography <i class="icon-external-link"></i></a></li>
            <li><a href="index.html#Aca">Academic Tools <i class="icon-external-link"></i></a></li>
            <li><a href="https://leibniz.stanford.edu/friends/preview/perception-auditory/">Friends PDF Preview <i class="icon-external-link"></i></a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=perception-auditory">Author and Citation Info <i class="icon-external-link"></i></a> </li>
            <li><a href="#pagetopright" class="back-to-top">Back to Top <i class="icon-angle-up icon2x"></i></a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</div>
<!-- End article sidebar --> 

<!-- NOTE: Article content must have two wrapper divs: id="article" and id="article-content" -->
<div id="article">
<div id="article-content">

<!-- BEGIN ARTICLE HTML -->
<h4 id="supphead">Supplement to <a href="index-2.html">Auditory Perception</a></h4>

<div id="aueditable">
<!--DO NOT MODIFY THIS LINE AND ABOVE--> 
<h2><a name="SpeechSupp">Speech Perception: Empirical and Theoretical Considerations</a></h2>

<p>What are the objects of speech perception? Speaking involves the
production of meaningful streams of sounds. At the physical level,
a <i>spectrogram</i> reveals the patterns of frequency and amplitude
that ground audible features. The stream sounds like a complex
acoustic structure involving patterns of audible qualities over
time. The stream, however, auditorily appears to be segmented (speech
in an unfamiliar language often seems like an unsegmented stream). The
most salient segments are words, the meaningful units. Also
discernible in the stream are segments that correspond to something
like syllables. These units or segments are not ascribed meaning, but
instead combine to form words in a way loosely analogous to the way
words combine to form sentences. Even syllables, however, comprise
perceptually distinguishable sound types. For instance, though
&lsquo;dough&rsquo; has one syllable, it includes the sounds of /d/
and /O/ (or /o&#650;/). The sound of the one-syllable spoken word
&lsquo;bad&rsquo; includes /b/, /&#230;/, and /d/. Those of
&lsquo;bat&rsquo; and &lsquo;bash&rsquo; differ because the former
contains /t/ and the latter contains /&#643;/. Such perceptible units,
or <i>phonemes</i>, whose patterns form the basis for recognizing and
distinguishing words, have been one primary focus of research into
speech perception. Phonemes form a sort of &ldquo;sound alphabet&rdquo; from which
audible words are built (Appelbaum 1999 critiques the
&ldquo;alphabetic&rdquo; conception).</p>

<p>What is a phoneme? First, consider the universal class
of <i>phones</i>, which contains all of the possibly distinguishable
types of speech sounds that may mark a semantic difference in some
world language.  In contrast, phonemes are specific to a particular
language.  Phonemes also may be understood in terms of equivalence
classes of sounds.  Phonemes are semantically significant sound types
that constitute the spoken words in a given language.  The boundaries
between phonemes in a language mark sound differences that may be
semantically significant for that language.
</p>

<p>
Phonemes thus may differ across languages.  For instance, though
certain phonemes are shared, the class of English phonemes differs
from that of Japanese.  English, for example, distinguishes the [l]
and [r] sounds (phones) as distinct phonemes, while Japanese does
not. Instead, Japanese treats them as <i>allophones</i>, or variants
of a common phoneme. Standard Chinese distinguishes distinct phonemes
that correspond to allophones of the single English phoneme /p/ (the
aspirated /p&#688;/ and unaspirated /p/). It is noteworthy that
infants prior to language learning distinguish phones that are later
subsumed to a single phonemic equivalence class (see, e.g., Werker
1995, Kuhl 2000 for review and commentary). In addition, certain
languages make use of novel sounds, such as clicks, that others do
not. So, when compared with each other, distinct languages may differ
in which sounds they include or omit among their respective phonemes,
and they may differ in which sound pairs they treat as distinct
phonemes or as allophonic.</p>

<p>
The central puzzle of speech perception is that there is no obvious
direct, consistent correspondence between the surface properties of a
physical acoustic signal and the phonemes perceived when listening to
speech.</p>

<p>
This is manifested in a number of ways. 
 Pioneers into speech perception research aimed initially
to develop an automated reading machine for the blind that worked by
replacing individual letters with specific sounds. The project failed
miserably&mdash;listeners were unable at the rates of normal speech to
resolve the sequence of individual sounds required to detect words
(see Liberman 1996).</p>

<p>
Most importantly, there is no clear <i>invariant</i> property of a
sound signal that corresponds to a given phoneme. What sounds like a
single phoneme might have very different acoustic correlates depending
not just upon the speaker or the speaker&rsquo;s mood, but also upon
the phonemic context. For instance, /di/ and /du/ audibly share the
/d/ phoneme. However, the acoustic signal corresponding to /d/ differs
greatly in these cases (see Liberman et al. 1967, 435, fig. 1). While
/di/ includes a formant that begins at a higher frequency and rises,
/du/ includes a formant that begins at a lower frequency and
drops. Acoustically, nothing straightforward in the signal corresponds
to the /d/ sound one auditorily experiences in both cases. Two
different audible phonemes also might share acoustic correlates, again
depending on context. The acoustic signal that corresponds to /p/ is
nearly identical to that of /k/ in the contexts /pi/ and /ka/ (Cooper
et al. 1952). Prima facie, phonemes thus are not identical with
distinctive invariant acoustic structures.</p>

<p>Lack of invariance stems in large part
from <i>coarticulation</i>. In contrast to how things seem auditorily,
how a speaker articulates a given phoneme depends upon what precedes
or follows that phoneme. Being followed by /i/ rather than /u/ impacts
how one pronounces /d/, and being preceded by /d/ impacts the
vowel. When pronouncing &lsquo;dab&rsquo;, the effects of pronouncing
both /d/ and /b/ are evident in the acoustic signature of /a/. The
articulatory consequences of phonemic context change the acoustic
features of the signal and confound attempts to map phonemes to
signals (which presents the difficulty for artificial speech
production and recognition). Furthermore, due to coarticulation, the
signal lacks the clear <i>segmentation</i> of categorically perceived
phonemes, which have been likened to beads on a string (Bloomfield
1933). In effect, speakers pronounce two or more phonemes at a time,
and transitions are fluid rather than discrete (see, e.g., Liberman
1970, 309, fig. 5, Diehl et al. 2004).</p>

<p>One response to this, compatible with realism about perceptible
phonological features, is to search for more complex acoustic
structures or to higher-order acoustical properties that correspond to
apparent phonemes (see, e.g., Blumstein and Stevens 1981, Diehl et
al. 2004, Holt and Lotto 2008 for the <i>general auditory</i>
approach). On the other hand, some philosophers instead conclude that
phonological features are mere intentional objects, or
&lsquo;intentional inexistents&rsquo; (see Rey 2012). Pautz (2017,
27&ndash;28), for instance, maintains that differences in acoustical
features cannot account for apparent categorical differences between
phonemes.</p>

<p>Another type of realist approach appeals to aspects of
the <i>gestures</i> used to pronounce phonemes&mdash;ways of moving
one&rsquo;s throat and mouth and tongue&mdash;which are reasonably
invariant across contexts.  For instance, pronouncing /d/ involves
placing the tip of the tongue on the alveolar ridge directly behind
the teeth. The alveolar consonants /d/ and /t/ differ from each other
in being <i>voiced</i>, or accompanied by vocal fold movement. Whether
you say /di/ or /du/, your tongue touches the alveolar ridge and you
voice the consonant. But, while you articulate the gestures associated
with /d/, you anticipate and begin to articulate those associated with
/i/ or /u/. This alters the overall acoustic signature of the gestures
associated with /d/. Gestures, rather than the complex acoustic
signals they produce, on this view make intelligible the perceptual
individuation of phonemes. Some therefore hold that perceiving
phonemes involves recovering information about articulatory gestures
from the acoustic signal. The <i>motor theory</i> (Liberman et
al. 1967, Liberman and Mattingly 1985) and <i>direct realism</i>
(Fowler 1986) are very different versions of this
approach. Articulatory gestures thus make plausible candidates for
objects of phoneme perception. They are, however, imperfect
candidates, since they do not entirely escape worries about the
context dependence and lack of discrete segmentation stemming from
fluid coarticulation (Appelbaum 1996, Remez and Trout 2009).</p>

<p>Nonetheless, the claim is supported by the surprising finding that
visual processes impact the auditory experience of speech. For
instance, the McGurk effect includes one instance in which seeing
video of a speaker pronouncing /ga/ dubbed with audio of /ba/ leads to
hearing as of the /da/ phoneme (McGurk and Macdonald 1976). If
perceiving speech involves perceiving gestures, it is not surprising
that the visual evidence for articulatory gestures should be weighed
against auditory evidence.</p>

<p>Some researchers who hold that intended or actual gestures are the
best candidates for the objects of phoneme perception argue that
speech perception therefore is special. That is, speech
perception&rsquo;s <i>objects</i> differ in kind from the sounds and
acoustic structures we hear in general audition (Liberman et al. 1967,
Liberman and Mattingly 1985). Liberman and Mattingly (1985),
furthermore, use the claim that audition has distinctive objects to
motivate the claim that speech perception therefore involves
distinctive perceptual <i>processes</i>. They even argue that although
speech perception shares an end organ with auditory perception, it
constitutes a functionally distinct <i>modular</i> perceptual system
(Liberman and Mattingly 1985, 7&ndash;10, 27&ndash;30, see also
1989). Part of the motivation for their <i>motor theory</i> of speech
perception, against auditory theories, is to integrate explanations of
speech perception and speech production (1985, 23&ndash;5, 30&ndash;1,
see also Matthen 2005, ch 9, which uses the Motor Theory to support a
Codependency Thesis linking the capacities to perceive and produce
phonemes, 221). On this account, a single modular system is
responsible for both the production and perception of speech.  This
purported link between capacities for production and perception
suggests that humans are unique in possessing a speech perception
system. Humans, but not other creatures, are capable of discerning
speech for many of the same reasons they are capable of producing the
articulatory gestures that correspond to perceived phonemes. Other
animals presumably hear just sounds (Liberman et al. 1967, Liberman
and Mattingly 1985).</p>

<p>One might accept that perceived phonemes should be identified with
articulatory gestures but reject that this makes speech special (see,
e.g., Fowler 1986, Mole 2009). If auditory perception generally
implicates environmental happenings or sound sources, then the
gestures and activities associated with speech production are not
entirely distinctive among objects of audition. If hearing
even <i>sounds</i> is not merely a matter of hearing features of
acoustic signals or structures, and if it is part of the function of
auditory perception to furnish information about distal events on the
basis of their audible characteristics, then speech is not entirely
unique among things we hear (see also Rosenbaum 2004, O&rsquo;Callaghan 2015).</p>

<p>The <i>processes</i> associated with speech perception therefore
need not be understood as entirely distinct in function or in kind
from those devoted to general audition, as Liberman and Mattingly
contend. Given this, it is not surprising to learn that good evidence
suggests humans are not special in possessing the capacity to
perceptually individuate the sounds of speech (see, e.g., Lotto et
al. 1997 for details).</p>

<p>The processes associated with speech need not be <i>entirely</i>
continuous with those of general audition. The overall claim is
compatible with higher acuity or sensitivity for speech sounds, and it
allows for special selectivity for speech sounds. Even if hearing
speech marshals perceptual resources continuous with those devoted to
hearing other sounds and events in one&rsquo;s environment, it would
be very surprising to discover that there were <i>not</i> processes
and resources devoted to the perception of speech. Research in fact
supports a special status for speech among the things we auditorily
perceive. First, evidence suggests that human neonates prefer sounds
of speech to non-speech (Vouloumanos and Werker 2007). Second, adults
are able to distinguish speech from non-speech based on visual cues
alone (Soto-Faraco et al. 2007). Third, infants can detect and
distinguish different languages auditorily (Mehler et al. 1988, Bosch
et al. 1997). Finally, infants aged approximately 4&ndash;6 months can
detect, based on visual cues alone, when a speaker changes from one
language to another, though all but those in bilingual households lose
that ability by roughly 8 months (Weikum et al. 2007).</p>

<p>To review, no obvious acoustic correlates exist for phonetic
segments heard in speech. Complex acoustic cues therefore must trigger
perceptual experiences of phonemes. Articulatory gestures, however,
are good (though imperfect) candidates for objects of speech
perception. This does not imply that speech perception involves
entirely different kinds of objects or processes from ordinary
non-linguistic audition, nor does it imply that speech perception is a
uniquely human capacity. Nevertheless, speech clearly is special for
humans, in that we have special sensitivity for speech sounds. Speech
perception promises to reward additional philosophical attention (see
O&rsquo;Callaghan 2015 for further development).</p>


</div><!-- #aueditable --><!--DO NOT MODIFY THIS LINE AND BELOW-->

<!-- END ARTICLE HTML -->

</div> <!-- End article-content -->

  <div id="article-copyright">
    <p>
 <a href="../../info.html#c">Copyright &copy; 2020</a> by

<br />
<a href="http://caseyocallaghan.com/" target="other">Casey O&rsquo;Callaghan</a>
&lt;<a href="m&#97;ilto:casey&#37;2eocallaghan&#37;40wustl&#37;2eedu"><em>casey<abbr title=" dot ">&#46;</abbr>ocallaghan<abbr title=" at ">&#64;</abbr>wustl<abbr title=" dot ">&#46;</abbr>edu</em></a>&gt;
    </p>
  </div>

</div> <!-- End article -->

<!-- NOTE: article banner is outside of the id="article" div. -->
<div id="article-banner" class="scroll-block">
  <div id="article-banner-content">
    <a href="../../fundraising/index.html">
    Open access to the SEP is made possible by a world-wide funding initiative.<br />
    The Encyclopedia Now Needs Your Support<br />
    Please Read How You Can Help Keep the Encyclopedia Free</a>
  </div>
</div> <!-- End article-banner -->

    </div> <!-- End content -->

    <div id="footer">

      <div id="footer-menu">
        <div class="menu-block">
          <h4><i class="icon-book"></i> Browse</h4>
          <ul role="menu">
            <li><a href="../../contents.html">Table of Contents</a></li>
            <li><a href="../../new.html">What's New</a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
            <li><a href="../../published.html">Chronological</a></li>
            <li><a href="../../archives/index.html">Archives</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-info-sign"></i> About</h4>
          <ul role="menu">
            <li><a href="../../info.html">Editorial Information</a></li>
            <li><a href="../../about.html">About the SEP</a></li>
            <li><a href="../../board.html">Editorial Board</a></li>
            <li><a href="../../cite.html">How to Cite the SEP</a></li>
            <li><a href="../../special-characters.html">Special Characters</a></li>
            <li><a href="../../tools/index.html">Advanced Tools</a></li>
            <li><a href="../../contact.html">Contact</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-leaf"></i> Support SEP</h4>
          <ul role="menu">
            <li><a href="../../support/index.html">Support the SEP</a></li>
            <li><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
            <li><a href="../../support/donate.html">Make a Donation</a></li>
            <li><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
          </ul>
        </div>
      </div> <!-- End footer menu -->

      <div id="mirrors">
        <div id="mirror-info">
          <h4><i class="icon-globe"></i> Mirror Sites</h4>
          <p>View this site from another server:</p>
        </div>
        <div class="btn-group open">
          <a class="btn dropdown-toggle" data-toggle="dropdown" href="https://plato.stanford.edu/">
            <span class="flag flag-usa"></span> USA (Main Site) <span class="caret"></span>
            <span class="mirror-source">Philosophy, Stanford University</span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="../../mirrors.html">Info about mirror sites</a></li>
          </ul>
        </div>
      </div> <!-- End mirrors -->
      
      <div id="site-credits">
        <p>The Stanford Encyclopedia of Philosophy is <a href="../../info.html#c">copyright &copy; 2021</a> by <a href="http://mally.stanford.edu/">The Metaphysics Research Lab</a>, Department of Philosophy, Stanford University</p>
        <p>Library of Congress Catalog Data: ISSN 1095-5054</p>
      </div> <!-- End site credits -->

    </div> <!-- End footer -->

  </div> <!-- End container -->

   <!-- NOTE: Script required for drop-down button to work (mirrors). -->
  <script>
    $('.dropdown-toggle').dropdown();
  </script>

</body>

<!-- Mirrored from seop.illc.uva.nl/entries/perception-auditory/supplement.html by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 22 Jun 2022 20:05:44 GMT -->
</html>
