<!DOCTYPE html>
<!--[if lt IE 7]> <html class="ie6 ie"> <![endif]-->
<!--[if IE 7]>    <html class="ie7 ie"> <![endif]-->
<!--[if IE 8]>    <html class="ie8 ie"> <![endif]-->
<!--[if IE 9]>    <html class="ie9 ie"> <![endif]-->
<!--[if !IE]> --> <html> <!-- <![endif]-->

<!-- Mirrored from seop.illc.uva.nl/entries/information-semantic/ by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 22 Jun 2022 19:50:10 GMT -->
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>
Semantic Conceptions of Information (Stanford Encyclopedia of Philosophy)
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="robots" content="noarchive, noodp" />
<meta property="citation_title" content="Semantic Conceptions of Information" />
<meta property="citation_author" content="Sequoiah-Grayson, Sebastian" />
<meta property="citation_author" content="Floridi, Luciano" />
<meta property="citation_publication_date" content="2005/10/05" />
<meta name="DC.title" content="Semantic Conceptions of Information" />
<meta name="DC.creator" content="Sequoiah-Grayson, Sebastian" />
<meta name="DC.creator" content="Floridi, Luciano" />
<meta name="DCTERMS.issued" content="2005-10-05" />
<meta name="DCTERMS.modified" content="2022-01-14" />

<!-- NOTE: Import webfonts using this link: -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300,600,200&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css" />

<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap.min.css" />
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/bootstrap-responsive.min.css" />
<link rel="stylesheet" type="text/css" href="../../css/font-awesome.min.css" />
<!--[if IE 7]> <link rel="stylesheet" type="text/css" href="../../css/font-awesome-ie7.min.css"> <![endif]-->
<link rel="stylesheet" type="text/css" media="screen,handheld" href="../../css/style.css" />
<link rel="stylesheet" type="text/css" media="print" href="../../css/print.css" />
<link rel="stylesheet" type="text/css" href="../../css/entry.css" />
<!--[if IE]> <link rel="stylesheet" type="text/css" href="../../css/ie.css" /> <![endif]-->
<script type="text/javascript" src="../../js/jquery-1.9.1.min.js"></script>
<script type="text/javascript" src="../../js/bootstrap.min.js"></script>

<!-- NOTE: Javascript for sticky behavior needed on article and ToC pages -->
<script type="text/javascript" src="../../js/jquery-scrolltofixed-min.js"></script>
<script type="text/javascript" src="../../js/entry.js"></script>

<!-- SEP custom script -->
<script type="text/javascript" src="../../js/sep.js"></script>
</head>

<!-- NOTE: The nojs class is removed from the page if javascript is enabled. Otherwise, it drives the display when there is no javascript. -->
<body class="nojs article" id="pagetopright">
<div id="container">
<div id="header-wrapper">
  <div id="header">
    <div id="branding">
      <div id="site-logo"><a href="../../index.html"><img src="../../symbols/sep-man-red.png" alt="SEP home page" /></a></div>
      <div id="site-title"><a href="../../index.html">Stanford Encyclopedia of Philosophy</a></div>
    </div>
    <div id="navigation">
      <div class="navbar">
        <div class="navbar-inner">
          <div class="container">
            <button class="btn btn-navbar collapsed" data-target=".collapse-main-menu" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Menu </button>
            <div class="nav-collapse collapse-main-menu in collapse">
              <ul class="nav">
                <li class="dropdown open"><a id="drop1" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-book"></i> Browse</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop1">
                    <li><a href="../../contents.html">Table of Contents</a></li>
                    <li><a href="../../new.html">What's New</a></li>
                    <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
                    <li><a href="../../published.html">Chronological</a></li>
                    <li><a href="../../archives/index.html">Archives</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop2" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-info-sign"></i> About</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop2">
                    <li><a href="../../info.html">Editorial Information</a></li>
                    <li><a href="../../about.html">About the SEP</a></li>
                    <li><a href="../../board.html">Editorial Board</a></li>
                    <li><a href="../../cite.html">How to Cite the SEP</a></li>
                    <li><a href="../../special-characters.html">Special Characters</a></li>
                    <li><a href="../../tools/index.html">Advanced Tools</a></li>
                    <li><a href="../../contact.html">Contact</a></li>
                  </ul>
                </li>
                <li class="dropdown open"><a id="drop3" href="#" class="dropdown-toggle" data-toggle="dropdown" role="button"><i class="icon-leaf"></i> Support SEP</a>
                  <ul class="dropdown-menu" role="menu" aria-labelledby="drop3">
                    <li><a href="../../support/index.html">Support the SEP</a></li>
                    <li><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
                    <li><a href="../../support/donate.html">Make a Donation</a></li>
                    <li><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
                  </ul>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End navigation -->
    
    <div id="search">
      <form id="search-form" method="get" action="https://seop.illc.uva.nl/search/searcher.py">
        <input type="search" name="query" placeholder="Search SEP" />
        <div class="search-btn-wrapper"><button class="btn search-btn" type="submit"><i class="icon-search"></i></button></div>
      </form>
    </div>
    <!-- End search --> 
    
  </div>
  <!-- End header --> 
</div>
<!-- End header wrapper -->

<div id="content">

<!-- Begin article sidebar -->
<div id="article-sidebar" class="sticky">
  <div class="navbar">
    <div class="navbar-inner">
      <div class="container">
        <button class="btn btn-navbar" data-target=".collapse-sidebar" data-toggle="collapse" type="button"> <i class="icon-reorder"></i> Entry Navigation </button>
        <div id="article-nav" class="nav-collapse collapse-sidebar in collapse">
          <ul class="nav">
            <li><a href="#toc">Entry Contents</a></li>
            <li><a href="#Bib">Bibliography</a></li>
            <li><a href="#Aca">Academic Tools</a></li>
            <li><a href="https://leibniz.stanford.edu/friends/preview/information-semantic/">Friends PDF Preview <i class="icon-external-link"></i></a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=information-semantic">Author and Citation Info <i class="icon-external-link"></i></a> </li>
            <li><a href="#pagetopright" class="back-to-top">Back to Top <i class="icon-angle-up icon2x"></i></a></li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</div>
<!-- End article sidebar --> 

<!-- NOTE: Article content must have two wrapper divs: id="article" and id="article-content" -->
<div id="article">
<div id="article-content">

<!-- BEGIN ARTICLE HTML -->


<div id="aueditable"><!--DO NOT MODIFY THIS LINE AND ABOVE-->

<h1>Semantic Conceptions of Information</h1><div id="pubinfo"><em>First published Wed Oct 5, 2005; substantive revision Fri Jan 14, 2022</em></div>

<div id="preamble">

<p>
Information is a rich commodity. It is perhaps the richest of them
all. It is by now almost post-truistic that interested parties, from
individuals, to large-scale globalised private actors and nation
states, will go to extraordinary lengths to protect and to acquire
information. If we allow ourselves to engage in a little armchair
etymology, then something&rsquo;s being in <em>formation</em> is just
for it to to be non-random. Indeed this armchair etymology gets
straight to the heart of how it is that we often speak of information,
as a contrast to randomisation. Indeed &ldquo;signal-to-noise
ratio&rdquo; may be restated as &ldquo;information-to-noise
ratio&rdquo;, hence the ubiquity of the refrain &ldquo;That is not
information, it is just noise&rdquo;.</p>

<p>
Another way of putting this is to say that information is a non-random
distinction with a difference. Information supervenes on patterned
deviations, and getting information from one&rsquo;s environment
depends on one being able to recognise and exploit these patterns.
That one part of our environment may carry information about another
part of our environment depends on our environment behaving in
reliable, non-randomised way. Our ability to use information at one
part of our environment to access information at another part of it
depends not only on our ability to recognise patterned regularities,
but on our ability to recognise connections between such
regularities.</p>

<p>
For example, for us to access the distal information that there is
fire from the proximal information that there is smoke, we must be
aware of the correlation between smoke and fire. That we do is marked
by maxims such as &rdquo;smoke means fire&rdquo;. Such correlations of
natural meaning between events in the world is studied by the
Mathematical Theory of Communication (MTC) due to Shannon and Weaver
(see the entry on
 <a href="../information/index.html">information</a>).
 This is the &ldquo;industry standard&rdquo; account of information,
as analysed by communications engineers. What of the information
encoded by natural language however? What of information in this more
concretely semantic sense? A semantic conception of information will
not investigate the mere surprise value of a signal given the
likelihood of the signal itself (as does MTC). Instead, it will
investigate the nature of the information carried by, or encoded by
the messages transmitted themselves. Another way of approaching the
issue is to understand such semantic conceptions of information to be
concerned with the investigation of the <em>informational content
of</em> a piece of language of some sort. Such pieces of language
might be propositions, singular terms, descriptions, and so on.</p>

<p>
The first theory of such a semantic conception of information that was
articulated in detail was Bar-Hillel and Carnap&rsquo;s <em>theory of
semantic information</em>. It is fitting then that we start here in
 <a href="#BarHillCarnTheoSemaInfo">section 1</a>.
 In
 <a href="#ProbForTheoSemaInfo">section 1.1</a>
 we will see how the theory of semantic information has some
consequences for certain types of statements, namely logical truths
and contradictions, that many have found counterintuitive. In
 <a href="#FlorTheoStroSemaInfo">section 1.2</a>
 we will see how Floridi responds to these problems, as well how
others have extended and responded to Floridi&rsquo;s proposal that we
accept the <em>veridicality thesis</em>&mdash;that information be
factive. A number of such responses are motivated by realist and
naturalist considerations with regard to the ontology of semantic
information. In
 <a href="#InfoModePresMean">section 2</a>
 we will look at semantic conceptions of information that take
naturalist metaphysics seriously. Here we will see how Fodor, Dretske,
Evans, Lewis, Recanati, and Jackson propose and defend semantic
conceptions of information that turn on modes of presentation, and
that are broadly naturalist insofar as they attempt to subsume
information within causal relations of various sorts. Any causal
theory of meaning or knowledge will face the problem of accounting for
the meaning of, and our epistemic access to, abstract objects.</p>

<p>
Most of the theories of broadly semantic information outlined in
 <a href="#InfoModePresMean">section 2</a>
 appeal to information channels in one sense or another (with a
channel being a relation allowing information the flow from one state
to another). In
 <a href="#TheoInfoChan">section 3</a>
 we will look at theories of information channels directly. (For a
detailed examination of the formal properties of the theory of
information channels, as well as how it is that the theory connects
with other issues in logic and information more generally, see the
entry on
 <a href="../logic-information/index.html">logic and information</a>.)
 We will see how the original theory of information channels due to
Barwise and Perry emerged from Austin&rsquo;s theory of semantic
content, as well as how it is that the theory of information channels
has been developed and extended in order to give a semantics for
information flow itself, as well as informational semantics for
natural language fragments and epistemic phenomena.</p>
</div> 

<div id="toc"><!--Entry Contents-->
<ul>
	<li><a href="#BarHillCarnTheoSemaInfo">1. Bar-Hillel and Carnap&rsquo;s Theory of Semantic Information</a>

	<ul>
		<li><a href="#ProbForTheoSemaInfo">1.1 Problems for the theory of semantic information</a></li>
		<li><a href="#FlorTheoStroSemaInfo">1.2 Floridi&rsquo;s theory of strongly semantic information</a></li>
	</ul>
	</li>
	<li><a href="#InfoModePresMean">2. Information, Modes of Presentation, and Meaning</a></li>
	<li><a href="#TheoInfoChan">3. The Theory of Information Channels</a>
	<ul>
		<li><a href="#SemaInfoFlow">3.1 The semantics of information flow</a></li>
		<li><a href="#ModeEpisPhenInfo">3.2 Modelling epistemic phenomena informationally</a></li>
	</ul>
	</li>
	<li><a href="#Summ">4. Summary</a></li>
	<li><a href="#Bib">Bibliography</a></li>
	<li><a href="#Aca">Academic Tools</a></li>
	<li><a href="#Oth">Other Internet Resources</a></li>
	<li><a href="#Rel">Related Entries</a></li>
</ul>
<!--Entry Contents-->

<hr /></div>

<div id="main-text">

<h2 id="BarHillCarnTheoSemaInfo">1. Bar-Hillel and Carnap&rsquo;s Theory of Semantic Information</h2>

<p>
The most natural starting point for any overview of semantic
conceptions of information is Carnap and Bar-Hillel&rsquo;s &ldquo;An
Outline of a Theory of Semantic Information&rdquo; (1952). Bar Hillel
and Carnap&rsquo;s theory of semantic information is a quantitative
theory that emerged from more general theories of information (see 
section 4.2 on Shannon in the entry on
 <a href="../information/index.html#ShanInfoDefiTermProb">information</a>).
 Their theory was designed with the goal of giving us a usable
framework for calculating the <em>amount</em> of semantic information
encoded by a sentence in a particular language. In their case the
language in question is monadic predicate logic. The philosophical
details are grounded on an idea that has come to be known as the
<em>inverse range principle</em> (IRP). Loosely, IRP states that the
amount of information carried or encoded by a sentence is inversely
proportional to something else, where this something else is something
to which one can attach a precise numerical value. Once this has been
done, one can use this numerical value to calculate the measure of
semantic information as understood by the theory of semantic
information.</p>

<p>
For Bar-Hillel and Carnap, <em>the amount of semantic information
encoded by a sentence is inversely proportional to the likelihood of
the truth of that sentence</em>. So for them, the likelihood of truth
is the &ldquo;something else&rdquo; to which we can attach a precise
numerical value. To illustrate, we start with their method of
calculating a qualitative individuation of semantic information,
<em>content</em> or &ldquo;Cont&rdquo;.</p>

<p>
Where \(s\) is a sentence and \(W\) is
the set of all possible worlds, content is defined as follows:</p>

<div id="ex1">\[\tCont(s) := \{x\in W : x\vdash \neg s\}\]</div>

<p>
Given that the <em>intension</em> of a sentence \(s\)
is the set of all worlds in which the sentence if <em>true</em>, and
that the content of a sentence is the set of all worlds in which \(s\)
is <em>false</em>, the intension and the content of a
sentence \(s\) form a partition on the set of all
worlds \(W\).</p>

<p>
Bar-Hillel and Carnap define two distinct methods for
<em>quantitative</em> calculations of semantic information&mdash;a
<em>content measure</em> (cont), and an <em>information measure</em>
(inf). They start with an <em>a priori</em> probability measure of a
sentence \(s\), \(p(s)\), which is gotten from an <em>a
priori distribution</em>. The <em>a priori</em> distribution onto \(W\)
sums to 1, and we assume that all assignments are
equiprobable, hence the <em>a priori</em> probability measure is the
value of \(p(s)\) that results from this distribution. In this case,
<em>cont</em> and <em>inf</em> can be defined as follows:</p>

<div id="ex2_3">\[\begin{align} \tcont(s) &amp;:= 1 - p(s)\\ \inf(s) &amp;:= \log_2\frac{1}{1 - \tcont(s)} \end{align}\]</div>

<p>
The two measures are required for technical reasons&mdash;in order to
capture additivity on <em>content independence</em> and <em>inductive
independence</em> respectively. Two sentences \(s\) and
\(s'\) are content independent when they do not have any worlds in
common. Two sentences \(s\) and \(s'\) are inductively
independent when the conditional probability of each sentence given
the other is identical to their initial unconditional probability.
Additivity on inductive independence fails for cont, since it might be
the case that \(\tcont(s\wedge s') \neq \tcont(s) + \tcont(s')\) on
account of \(p(s)\) and \(p(s')\) having worlds in common&mdash;that
is, on account of them not being content independent in spite of their
being inductively independent. For additivity to hold on cont, it is
<em>content independence</em>, as opposed to inductive independence
that is required. By contrast, additivity on inductive independence
does not fail for inf. Bar-Hillel and Carnap&rsquo;s proof is
non-trivial (found on their 1952: 244&ndash;5).</p>

<h3 id="ProbForTheoSemaInfo">1.1 Problems for the theory of semantic information</h3>

<p>
Technical matters aside, some philosophical issues are immediate.
Firstly, how do we know in practice how many possible words there are?
If we are talking about the number of possible worlds with respect to
all possible sentences in English, then there will be infinitely many
of them. Bar-Hillel and Carnap avoided this issue by talking
exclusively about the semantic information encoded by sentences
formulated in monadic predicate logic with a finite number of
predicate letters. Where there are \(n\) predicate
letters, there will be \(2^n\) possible objects, exhausting all
possible predicate combinations. There will then be \(2^{2^n}\)
possible worlds (&ldquo;state descriptions&rdquo; in Bar-Hillel and
Carnap&rsquo;s parlance), corresponding to all possible combinations
of possible objects. Hintikka (1970, 1973), extended Bar-Hillel and
Carnap&rsquo;s theory of semantic information from monadic predicate
logic to fully general predicate logic.</p>

<p>
Thirdly and more generally, Bar-Hillel and Carnap&rsquo;s theory of
semantic information has give rise to two problems of strong
significance philosophically.</p>

<ul>

<li>The Bar-Hillel and Carnap Semantic Paradox (BCP)</li>

<li>The Scandal of Deduction (SOD)</li>
</ul>

<p>
BCP refers to the fact that Bar-Hillel and Carnap&rsquo;s theory of
semantic information assigns maximal information to contradictory
sentences. Where \(\perp\) is an arbitrary contradiction, given that
\(\perp\) will be false in all possible worlds, we have the following
via
 <a href="#ex1">(1)</a>,
 <a href="#ex2_3">(2)</a>, and
 <a href="#ex2_3">(3)</a>
 respectively:</p> 

\[\begin{align} \tCont(\perp) &amp;= W\; (\text{i.e., maximal content})\\ \tcont(\perp) &amp;= 1\\ \inf(\perp) &amp;= \infty\end{align}\]

<p>
Bar-Hillel and Carnap (1952: 229) responded to this situation as
follows:</p>

<blockquote>

<p>
It might perhaps, at first, seem strange that a self-contradictory
sentence, hence one which no ideal receiver would accept, is regarded
as carrying with it the most inclusive information. It should,
however, be emphasised that semantic information is here not meant as
implying truth. A false sentence which happens to say much is thereby
highly informative in our sense. Whether the information it carries is
true or false, scientifically valuable or not, and so forth, does not
concern us. A self-contradictory sentence asserts too much; it is too
informative to be true.</p>
</blockquote>

<p>
There are two dimensions to this response that have caused concern in
philosophical circles. The first is that their notion of semantic
information is <em>non-factive</em>&mdash;semantic information does
not need to be true. The second is that they are taking their notion
of semantic information to underpin informativeness in some
non-trivial sense.</p>

<p>
SOD refers to the fact philosophical accounts of information are yet
to give an account of the informativeness of logical truths and
deductive inferences. Bar-Hillel and Carnap&rsquo;s theory of semantic
information assigns minimal information to logical truths (and valid
deductive inferences can be transformed into logical truths by
conjoining the premises into an antecedent of a conditional that takes
the conclusion as its consequent). Where \(\top\) is an arbitrary
tautology, given that \(\top\) will be false in all possible worlds,
we have the following via
 <a href="#ex1">(1)</a>,
 <a href="#ex2_3">(2)</a>, and
 <a href="#ex2_3">(3)</a>
 respectively:</p> 

\[\begin{align} \tCont(\top) &amp;= \varnothing\\ \tcont(\top) &amp;= 0\\ \inf(\top) &amp;= 0\end{align}\]

<p>
With respect to logically true sentences returning a minimal
information value, Bar-Hillel and Carnap (1952: 223) respond as
follows:</p>

<blockquote>

<p>
This, however, is by no means to be understood as implying that there
is no good sense of &lsquo;amount of information&rsquo; in which the
amount of information of these sentences will not be zero at all, and
for some people, might even be rather high. To avoid ambiguities, we
shall use the adjective &lsquo;semantic&rsquo; to differentiate both
the presystematic sense of &lsquo;information&rsquo; in which we are
interested at the moment and their systematic explicata from other
senses (such as &ldquo;amount of psychological information for the
person P&rdquo;) and their explicata.</p>
</blockquote>

<p>
We will return to SOD briefly in
 <a href="#ModeEpisPhenInfo">section 3.2</a>
 below. Note here however that Hintikka (1970, 1973) mounted a
technically heroic if ultimately unsuccessful attempt to solve it (see
Sequoiah-Grayson (2008)), and for a properly detailed investigation,
see the entry on
 <a href="../logic-information/index.html">logic and information</a>.
 For now we must recognise that the response of Bar-Hillel and Carnap
above brings with it some noteworthy philosophical claims of its own.
Firstly, Bar-Hillel and Carnap are suggesting that the type of
information which is encoded by logical truths and for which the
amount encoded is non-zero, is psychological in some sense or other.
Furthermore, it may vary for one person from the other even with
respect to the same logical truth. Secondly, they are heeding the
following advice of Claude Shannon, the originator of the mathematical
theory of communication, given just two years earlier.</p>

<blockquote>

<p>
The word &lsquo;information&rsquo; has been given different meanings
by various writers in the general field of information theory. It is
likely that at least a number of these will prove to be useful in
certain applications to deserve further study and permanent
recognition. It is hardly to be expected that a single concept of
information would satisfactorily account for the numerous possible
applications of the general field. (1950 [1993: 180]).</p>
</blockquote>

<p>
Shannon is advocating a rich <em>informational pluralism</em>, for a
detailed development of which see Allo (2007). Shannon&rsquo;s advice
on this point was, as we are about to see, nothing if not
prescient.</p>

<h3 id="FlorTheoStroSemaInfo">1.2 Floridi&rsquo;s theory of strongly semantic information</h3>

<p>
Luciano Floridi&rsquo;s theory of <em>strongly semantic
information</em> (2004, 2005), is a response to BCP motivated by the
belief that something has gone essentially amiss with Bar-Hillel and
Carnap&rsquo;s theory. The suspicion is that their theory is based on
a semantic principle that is too weak, <em>namely that truth-values
are independent of semantic information</em>. Floridi&rsquo;s proposal
is that an approach according to which semantic information is factive
can avoid the paradox, and that the resulting theory is more in line
with our ordinary conception of what generally counts as information.
The line of argument is that a theory of strongly semantic
information, based on alethic and discrepancy values rather than
probabilities, can successfully avoid BCP. Relatedly, see Bremer and
Cohnitz (2004: chap. 2) for an overview of Floridi&rsquo;s theory to
be described below, and Sequoiah-Grayson (2007) for a defence of the
theory of strongly semantic information against independent objections
from Fetzer (2004) and Dodig-Crnkovic (2005).</p>

<p>
Before we expound Floridi&rsquo;s approach, note that some have
proposed a different alethic approach, one that uses truthlikeness, or
verisimilitude, to explicate the notion of semantic
information&mdash;Frick&eacute; (1997), Cevolani (2011, 2014), and
D&rsquo;Alfonso (2011). Typically these seek to identify factual
information with likeness to the complete truth about all empirical
matters or about some restricted relevant domain of factual interest.
These also avoid the BCP, and also do not use probabilities. However,
truthlikeness is different from truth itself in as much as a truth
bearer can be truth-like without actually being true, i.e., while
being false, so that verisimilitude accounts of information can permit
that false claims may possess information. Indeed false statements can
sometimes carry more information than their true negations on this
account, see Frick&eacute; (1997).</p>

<p>
By contrast, Floridi&rsquo;s strongly semantic factive information is
defined as well-formed, meaningful, and <em>truthful</em> data. This
latter factivity constraint on semantic information has come to be
known commonly as the <em>veridicality thesis</em> (VT) (prefigured in
Mingers (1995, 1996a, 1996b)). Importantly, versions of VT arise in
debates about the ontological status of information in general, not
merely with regard to semantic information in particular&mdash;see
Dretske (1981) for a classic example. Once the content is so defined,
the quantity of strongly semantic information in a proposition \(p\)
is calculated in terms of the distance of \(p\)
from a situation \(z\) (where
situations are partial or incomplete worlds) that \(p\)
is supposed to model.</p>

<p>
When \(p\) is true in all cases, but also when \(p\)
is false in all cases, there is maximal distance (as
opposed to maximal closeness) between \(p\) and the
situation \(z\) that \(p\) is supposed
to model. By contrast, <em>maximum closeness</em> is equivalent to the
precise modelling of \(z\) at the agreed level of
abstraction or descriptive adequacy. <em>Maximal distance</em> in the
direction of truth will result in \(p\) being true in
all cases in which case \(p = \mathord{\top}\) and is minimally
informative. Similarly, maximal distance in the direction of falsity
results in \(p\) being false in all cases (all possible
situations or probability 0) in which case \(p = \mathord{\perp}\) and
is minimally informative also. The important difference here is that
any distance in this direction is distance <em>bereft of strongly
semantic information entirely</em>. This is on account of distance in
the direction of &ldquo;the false&rdquo; violating the factivity
condition on strongly semantic information.</p>

<p>
Floridi distinguishes <em>informativeness</em> from strongly semantic
information itself. This is welcome, since strongly semantic
information is factive, whereas false statements can still be
informative. Indeed a false statement \(s\) may be
<em>more informative</em> than a true statement \(s'\), in spite of
the fact that \(s'\) carries strongly semantic information whereas \(s\)
does not. By way of example, suppose that you are
running a catering contract for an event, and that there will in fact
be exactly 200 people in attendance. Suppose that \(s\)
is <em>there will be 201 people in attendance</em>, and \(s'\) is
<em>there will be between 100 and 200 people in attendance</em>.
\(s'\) is true whilst \(s\) is false, but \(s\)
is more informative than \(s'\) on any natural
understanding of the concept <span class="sc">informative</span>.</p>

<p>
Where \(\sigma\) is a piece of strongly semantic (hence true)
information, and \(z\) is the target situation that it
describes with total accuracy, the more distant \(\sigma\) gets from
\(z\), the larger the number of situations to which it
applies and the lower its degree of informativeness. Floridi uses
&lsquo;\(\Theta\)&rsquo; to refer to the distance between a true
\(\sigma\) and \(z\) (recall that Floridi is not
interested in non-factive information, and might well deny that there
is any sensible such commodity). \(\Theta\) indicates the degree of
support offered by \(z\) to \(\sigma\). Given a
specific \(\sigma\) and a corresponding target \(z\),
Floridi maps the values of \(\Theta\) onto the x-axis of a Cartesian
diagram. We now need a formula to calculate the degree of
informativeness \(\iota\) of
\(\sigma\)&mdash;\(\iota(\sigma)\)&mdash;in relation to
\(\Theta(\sigma)\). Floridi&rsquo;s proposal is that we calculate the
value of \(\iota(\sigma)\) via the complement of the distance of
\(\Theta(\sigma)\) squared:</p> 

<div id="ex10">\[\iota(\sigma) = 1 - \Theta(\sigma)^2\]</div>

<p>
Values of \(\iota\) range from 0 to 1 and are mapped along the y-axis
of the Cartesian diagram.
 <a href="#figure">Figure 1</a>
 shows the graph generated by
 <a href="#ex10">(10)</a>
 when we include negative values for false \(\sigma\). \(\Theta\)
ranges from \(-1 = \mathord{\perp}\) to \(1 = \mathord{\top}\):</p>

<div class="figure" id="figure">
<img alt="A graph with a curve starting at (-1,0), labelled necessarily false, rising to (0,1) then falling to (1,0), labelled necessarily true; the quadrant for x less than 0 is labelled inaccuracy; the quadrant for x greater than 0 is labelled vacuity. The rest is explained above" src="informativeness.svg" />

<p class="center">
<span class="figlabel">Figure 1</span></p>
</div>

<h4 id="RespExte">Responses and extensions</h4>

<p>
Floridi (2012) extends the theory of strongly semantic information
into matters of traditional epistemology. His <em>network theory of
account</em> involves an argument for the claim that should strongly
semantic information be embedded within a network of questions and
answers that account for it correctly, then this is necessary and
sufficient for the strongly semantic information to count as
<em>knowledge</em>. Floridi (2008) develops a theory of
<em>relevant</em> semantic information in order to articulate a theory
of <em>epistemic relevance</em>. Here he argues that the nature of
relevant semantic information is an additional vindication of the
veridicality thesis. In Floridi (2011) he further explores just what
it might, or should mean for semantic information to be true. Rather
than accept a correspondence, coherence, or pragmatic theory of truth,
he develops what he calls a <em>correctness theory of truth</em> for
the veridicality thesis, one which connects directly with his network
theory of account described above.</p>

<p>
Floridi (2006) argues that the modal logic KTB is well placed to play
the role of a logic of <em>being informed</em> (KTB is system
<strong>B</strong> described in the entry on
 <a href="../logic-modal/index.html">modal logic</a>.)
 KTB itself licenses a version of the veridicality thesis within the
context of being informed, \(I_\alpha p\to p\) (where \(I\)
is a universal modal operator, on account of the
axiom \(\square p\to p\) being an axiom of KTB). &ldquo;Being
informed&rdquo; is understood as a cognitive state distinct from both
knowledge and belief. Allo (2011) provides a formal semantics for the
logic of being informed, in both pure and applied versions. Primiero
(2009) rejects the veridicality thesis for a logic of
<em>becoming</em> informed. Primiero&rsquo;s logic of becoming
informed is a logic of epistemic constructive information, within
which the definition of information requires it to be kept distinct
from truth. Epistemic constructive information understands information
for propositional content in terms of proof-conditions as opposed to
truth-conditions.</p>

<p>
More broadly, Dinnen and Brauner (2015) search for a single definition
of information (be it semantic or not) and find the veridicality
thesis to be obstructive. By contrast, Mingers and Standing (2018)
argue for a single definition of information that supports the
veridicality thesis. Allo (2007) preempts such concerns with an
argument for an <em>informational pluralism</em> (analogous to a
 <a href="../logical-pluralism/index.html">logical pluralism</a>,
 see the entry) via a <em>realist</em> interpretation of semantic
information itself. A realist interpretation of semantic information
leads naturally to the question of how it is that semantic information
can emerge from and be a part of the natural world&mdash;a question
that is addressed in detail in Vakarelov (2010). The question of how
it might be that information could be accounted for naturalistically
has a rich history in philosophy, most notably in <em>informational
semantics</em> covered in the following section.</p>

<p>
Although Floridi&rsquo;s, and Bar-Hillel and Carnap&rsquo;s stance on
semantic information is not uncontroversial ( <em>sans</em> an
informational pluralism that is), Floridi&rsquo;s motivating intuition
has some philosophical precedent. Firstly, it is unlikely that many
are satisfied with Bar-Hillel and Carnap&rsquo;s claim that &ldquo;A
self-contradictory sentence asserts too much; it is too informative to
be true&rdquo;. Secondly, with regard to logical truths having zero
semantic information on Floridi&rsquo;s account, recall that as
Wittgenstein put it with typical bluntness&mdash;&ldquo;All the
propositions of logic say the same thing, viz nothing. They are
tautologies&rdquo; (<em>Tractatus</em>, 4.46, 6.1). One way to
understand Floridi&rsquo;s theory of strongly semantic information is
as a theory of the information we get from and about our particular
objective physical environment, as our physical environment is one
about which contradictions and logical truths are typically maximally
uninformative. Semantic conceptions of information designed to tell a
naturalistic story about the content of our putatively referring terms
have a rich history of their own in philosophy, and this is the topic
to which we now turn.</p>

<h2 id="InfoModePresMean">2. Information, Modes of Presentation, and Meaning</h2>

<p>
Theories of meaning that turn on modes of presentation have been
common in philosophy in one way or another since Frege&rsquo;s
<em>Sense and Reference</em>. The story is as follows. There must be
more to the meaning of a referring term than its referent, since terms
can co-refer. For example, <em>James Newell Osterberg Jr.</em> and
<em>Iggy Pop</em> both refer to the same individual. In spite of this
the intuition that they do not mean the same thing is strong.
&ldquo;Iggy Pop is Iggy Pop&rdquo; and &ldquo;Iggy Pop is James Newell
Osterberg Jr.&rdquo; do not <em>seem</em> to mean the same thing.
Similarly, it seems to be the case that &ldquo;Alice believes that
Iggy Pop was the singer in The Stooges&rdquo; might true, whilst
&ldquo;Alice believes that James Newell Osterberg Jr. was the singer
in The Stooges&rdquo; might be false, at least on one natural reading
that is in line with our intuitions with regard to meanings.</p>

<p>
Frege&rsquo;s well known response is that both the referent and the
sense of a referring term play a role in specifying its semantic
content. But what is the sense of a term? Frege&rsquo;s own way of
cashing out the notion of sense is in terms of a <em>mode of
presentation</em> (MOP), an idea used by many later philosophers. The
MOP of a referring term is the way in which the putative referent of
the term is presented to us by our phenomenology. MOPs are what we
would use in an attempt to identify or locate the referent of a
referring term whose meaning we grasp. Many contemporary theories of
meaning that turn on information, incorporate MOPs in one way or
another. The reason for this is that although reducing the meaning of
a term to the information carried or transmitted by it alone is
attractive, it has proven to be fraught.</p>

<p>
The temptation to take meaning and information to amount to pretty
much the same thing is a result of the following idea. The idea is
that the word &lsquo;cat&rsquo; denotes the property of being a cat,
and that it means <em>cat</em> because it expresses the concept <span class="sc">cat</span>,
and the concept <span class="sc">cat</span>
means <em>cat</em>. The concept <span class="sc">cat</span> means
<em>cat</em> because it carries the information
\(\langle\)cat\(\rangle\), and <span class="sc">cat</span> carries the
information \(\langle\)cat\(\rangle\) because its instances or
tokenings are caused, by and large, by cats. This is a nice idea. By
tying meaning and information together and telling a <em>causal</em>
story about them, we have a naturalistic story to tell about the
information that we get from our environment, and hence a naturalistic
story to tell about meaning. Such information-transmitting causal
relationships are <em>information channels</em>&mdash;causal
connections that facilitate the flow of information between the source
of information and the receiver. We should take care to note this
story is telling an informationally semantic story about
sub-propositionally located pieces of information such as the
predicate &lsquo;cat&rsquo; and paradigmatic uses of singular terms.
As such it sits outside of the domain described by the theories of
semantic and strongly semantic information described above. In spite
of this, we will see that a refinement of this story turns on tracking
<em>accuracy</em>, if not truth itself.</p>

<p>
In a series of influential works on this area of informational
semantics, Jerry Fodor (1990), and Fred Dretske (1981) proposed a
theory of semantics very much like the one outlined above (see the
entry on
 <a href="../content-causal/index.html">causal theories of mental content</a>).
 A noted problem for such an informational semantics has come to be
known commonly as the <em>disjunction problem</em>. The disjunction
problem is as follows. <span class="sc">cat</span> tokens are not
always caused by cats, they are sometimes caused by other things like
small dogs for example (or by thoughts about balls of yarn or
whatever). Given this fact, if the story above is correct, then why
does <span class="sc">cat</span> mean <em>cat</em> and not <em>cat or
dog</em>? Fodor&rsquo;s (1990) response is in two stages.</p>

<p>
Firstly, Fodor&rsquo;s initial proposal is that non cat-caused tokens
of <span class="sc">cat</span> are asymmetrically dependent on
cat-caused tokens of <span class="sc">cat</span>. That is, there would
not be any non cat-caused tokens of <span class="sc">cat</span> had
there not been any cat-caused tokens of <span class="sc">cat</span>.
Secondly, on Fodor&rsquo;s picture, meaning and information <em>come
apart</em>. The <em>information</em> carried by a token of a concept
covaries with its cause, whereas the <em>meaning</em> of a token is
what all of the concept&rsquo;s tokenings have in common&mdash;the
inner vehicles of our <span class="sc">cat</span> tokenings, or their
MOPs. Note that Fodor is not, strictly speaking, subsuming information
as part of meaning, but rather teasing them apart. Our failure to
appreciate that meaning and information come apart is, according to
Fodor, a consequence of the fact that firstly, they are very often
coextensive, and that secondly, &lsquo;means&rsquo; is a homonym for
both semantic content (meaning) and information-carrying. Consider the
following two uses of &ldquo;means&rdquo;:</p>

<ul>

<li>Smoke means fire.</li>

<li>&lsquo;Smoke&rsquo; means <em>smoke</em>.</li>
</ul>

<p>
On Fodor&rsquo;s view, the first use is in the sense of
information-carrying only. Smoke carries the information that there is
fire, but that is not what it means semantically. What
&lsquo;smoke&rsquo; means, in the semantic sense, is <em>smoke</em>,
and this is captured by the latter use of &ldquo;means&rdquo; above.
On Fodor&rsquo;s story, just as with &lsquo;cat&rsquo; above,
&lsquo;smoke&rsquo; means <em>smoke</em> because it expresses <span class="sc">smoke</span>,
and tokenings of <span class="sc">smoke</span>
are caused paradigmatically (but not always!)
by smoke itself. The &ldquo;not always&rdquo; qualification is covered
by the asymmetric dependence condition above. So far so good, but what
about non-existent objects such as bunyips? Non bunyip-caused bunyip
tokens of <span class="sc">bunyip</span> cannot be asymmetrically
dependent on bunyip-caused bunyip tokens of <span class="sc">bunyip</span>
because there are no bunyips around to cause
anything at all.</p>

<p>
In light of non-existent objects such as bunyips, and the
meaningfulness of <span class="sc">bunyip</span> tokens in spite of
there being no bunyips, Fodor adjusts his proposal so that meaning now
rests on <em>asymmetrical dependences among nomological relations
among properties</em>&mdash;the property of being a bunyip for
example&mdash;as opposed to <em>actual</em> causal relations between
individuals. Nomological relations are cashed out in terms of
counterfactuals, so what we have now is an informational semantics
along the lines of the following&mdash;<span class="sc">bunyip</span>
means <em>bunyip</em> because if there <em>were</em> bunyips, bunyips
<em>would be</em> the cause of <span class="sc">bunyip</span> tokens
on which all other causes would depend asymmetrically.</p>

<p>
Recall again that Fodor is teasing meaning and information apart.
Gareth Evans (1982) formulates a similar informational theory of
meaning, but one where information and MOPs are both subsumed within
the semantic story itself. For Evans, a full story about the meaning
of thoughts about particular objects that are&mdash;putatively at
least&mdash;in the world, needs to take account of both the causal
origins of the thought, as well as the MOP engendered by it. Evans
calls such thoughts <em>information based particular thoughts</em>,
and such thoughts will be <em>well grounded</em> if and only if the
object satisfying the MOP and the object at the source-end of the
causal route are one and the same thing.</p>

<p>
What the Fodor/Dretske and Evans theories of informational semantics
have in common, is that they recognise that the meaning or
content/object of a thought is robust across causal variation:</p>

<blockquote>

<p>
We want to be able to say that two informational states (states of
different persons) embody the same information, provided that they
result from the same initial informational event. (Evans 1982:
128&ndash;129)</p>

<p>
Informational theories&hellip;appeal to reliable covariances while
quantifying over the causal mechanisms by which these covariances are
sustained. By doing so, they explain why information (indeed, why the
very same information ) can be transmitted over so many different
kinds of channels. (Fodor 1990: 100)</p>
</blockquote>

<p>
Moreover, although Evans did not put things in quite these terms,
Fodor, Dretske, and Evans all recognise <em>information channels</em>
as robust entities in their own right.</p>

<p>
Fran&ccedil;ois Recanati (2012, 2016), has proposed a detailed version
of informational semantics, his <em>mental files</em> theory, within
which information channels play a central role. Recanati&rsquo;s
mental files are cognitive counterparts to singular terms, and as such
are referring concepts. Recanati&rsquo;s view looks very similar to
Evan&rsquo;s information based particular thoughts at first glance.
However, on Recanati&rsquo;s view, metal files contain information
<em>in the form of MOPs of an object</em>&mdash;be they given directly
and experientially, or indirectly via descriptions&mdash;their
reference is not fixed by the information that they contain/their
modes of presentation. Rather, the reference of a metal file is fixed
by the relations <em>on which</em> this file is based, and the
referent of a mental file will be the entity or object with which we
are acquainted correctly in virtue of such relations obtaining. So
Recanati is allowing that MOPs contain information themselves, rather
than restrict the role of information to the reference fixing relation
itself (as do Evans and Fodor). The feature that identifies these
relations is that they are <em>epistemically rewarding</em> (ER)
relations. For Recanati, a relation is an ER relation in virtue of the
fact that it is the sort of relation that makes the <em>flow of
information possible</em>. In other words, ER relations are
<em>information channels</em>.</p>

<p>
Recanati&rsquo;s ER relations draw heavily on Lewis&rsquo;s (1983)
relations of &ldquo;epistemic rapport&rdquo;&mdash;causal chains that
would permit information flow, or information channels under another
name. Both Recanati and Lewis recognise the disjunction problem by
allowing that both information and misinformation may be transmitted
along information channels. Recanati&rsquo;s take is that the
reference of a mental file is fixed by the object that sits at the
distal end of the information channel that contributes to the
information that the mental file contains, irrespectively of the
&ldquo;fit&rdquo;. Fit may of course be bad on account of noisy
channels and/or misidentification on the agent&rsquo;s behalf. As
Recanati puts it:</p>

<blockquote>

<p>
The role of a mental file based on a certain acquaintance relation is
to store information acquired in virtue of that relation. The
information in question need not be veridical; we can think of it in
terms, simply, of a list of predicates which the subject takes the
referent to satisfy. The referent need not actually satisfy the
predicates in question, since the subject may be mistaken. Such
mistakes are possible because what determines the reference is not the
content of the file but the relevant relation to the object. The file
corresponds to an information channel, and the reference is the object
from which the information derives, whether that information is
genuine information or misinformation. (2012: 37&ndash;38)</p>
</blockquote>

<p>
It reads here as though Recanati is conflating a mental file on the
one hand, with the information channel that carries its informational
payload. Indeed Recanati goes on to argue that there are two sensible
and &ldquo;distinct notions of file&rdquo; (p. 82). The first notion
is simply a repository of evolving information that appears to be and
may be about a single distinct object. The second notion of file, what
Recanati calls the &ldquo;proper notion&rdquo;, involves <em>both</em>
a specific relevant information channel, <em>and</em> the repository
of information acquired via that channel.</p>

<p>
Along with Fodor, Dretske, Evans, Recanati, and Lewis, Frank Jackson
(2010) also articulates a semantic theory based upon information
channels that supervene on causal relations, along with MOPs.
Jackson&rsquo;s MOPs are identified with <em>descriptions</em>.
Jackson&rsquo;s description theory of reference for proper names turns
on information channels, which are articulated in terms of causal
links that underpin information flow. Jackson&rsquo;s motivating idea
is that names are by and large sources of information about the
entities that they name. The descriptive dimension is a function of
their (the descriptions) being specified in terms of
information-carrying causal connections&mdash;information
channels.</p>

<p>
For Jackson, language is, in general, a representational system that
transmits information about the way that things are taken to be to
those who comprehend the language. When names are used in declarative
sentences, speakers are representing things as being a certain way.
The use of names in such contexts is to deliver putative information
about the way things are to other speakers in the language community.
According to Jackson, names do this as a function of their being parts
of information channels that exist between users of the language, and
the world. In order for us to track the information channel itself for
the purposes of getting information from it, we must understand the
structured connection between linguistic items (words and sentences),
and ways that the world might be. Names themselves facilitate this
practice in virtue of their being elements in the information channels
that exist between us and the world. These channels are created by
conventions of language use and established practices of baptism.</p>

<p>
Given the ubiquity of information channels in the theories above, it
is no surprise that information channels have become a topic of study
on their own terms. The theory of information channels has made
contributions to information-based analysis of natural language and
formal semantics.</p>

<h2 id="TheoInfoChan">3. The Theory of Information Channels</h2>

<p>
The theory of information channels, <em>channel theory</em>, emerged
from
 <a href="../situations-semantics/index.html">situation semantics</a>
 (see the entry), with the latter being motivated by the observation
that meaning depends on systematic regularities in the world, and that
such regularities are a necessary condition on our grasping any
meanings at all (Barwise 1993). Jon Barwise and John Perry (1983)
appealed to this observation in order to justify and motivate a
naturalistic theory of meaning. Early work in situation theory
concentrated on situations themselves, thought of best as <em>partial
worlds</em> in modal parlance. Importantly, situation theory itself
dealt with the formal side of things in terms of set theory as opposed
to modally, although as we will see below, modal interpretations have
come to dominate.</p>

<p>
Situation theory focused on <em>constraints</em> early on, with
constraints thought of most usefully as <em>conditionals</em>.
Situation theory builds its semantic theory on an Austinian theory of
truth&mdash;where an utterance \(u_s\) of a declarative sentence \(s\)
is putting forward a claim that is about some type of
situation \(x\), such that \(x\) is of
some type \(\phi\) (Barwise 1993: 4). Austin (1950) calls the type
\(\phi\) the descriptive content of \(s,\) with \(\phi\) specifying
the type of situation (or event or thing etc.) in the world that is
being described. He calls the situation \(x\) itself
the <em>demonstrative content of \(s\)</em>. In other
words, \(\phi\) describes the content of \(s\), and \(x\)
is the content <em>demonstrated</em> by \(s\)&mdash;which
is just to say that it is the part of the
world about which the utterer of \(u_s\) is speaking when they utter
\(s\).</p>

<p>
According to Barwise, for any conditional statement <em>if \(s_1\)
then \(s_2\)</em>, such that the descriptive content of \(s_1\) is of
type \(\phi\), and the descriptive content of \(s_2\) is of type
\(\psi\), the descriptive content of <em>if \(s_1\) then \(s_2\)</em>
is the constraint \(\phi\to\psi\). Constraints are connections between
types. The <em>demonstrative content</em> of <em>if \(s_1\) then
\(s_2\)</em> will be a connection between the demonstrative contents
of \(s_1\) and \(s_2\). Supposing that \(x\) is the
demonstrative content of \(\phi\), and \(y\) is the
demonstrative content of \(\psi\), the demonstrative content of <em>if
\(s_1\) then \(s_2\)</em> will be a connection between \(x\)
and \(y\), with this connection being
an <em>information channel</em> \(c\) between \(x\)
and \(y\), written \(x\cmapsto y\). As
Barwise puts it succinctly:</p>

<blockquote>

<p>
By an <em>information channel</em>, let us mean one of these relations
between situations, since it is these relations which allow
information about one situation to be gleaned from another situation.
(1993: 5)</p>
</blockquote>

<p>
The proposal in sum is that when we express a constraint
\(\phi\to\psi\) by way of uttering <em>if \(s_1\) then \(s_2\)</em>,
we are making a claim to the effect that <em>there is an information
channel supporting the constraint</em>. For an information channel to
support a constraint, Barwise&rsquo; proposal is the following:</p>

<div id="ex11">\[\text{If }x\vDash\phi, \:x\cmapsto y,\text{ and } \phi\to\psi, \text{ then } y\vDash \psi\]</div>

<p>

 <a href="#ex11">(11)</a>
 states that if information of type \(\phi\) is true at the situation
\(x\), and there is an information channel \(c\)
from the situation \(x\) to the
situation \(y\), and there is a constraint from
information of type \(\phi\) to information of type \(\psi\), then
information of type \(\psi\) is true at the situation \(y\).</p>

<p>
Barwise refines the notion of a situation to that of
&ldquo;site&rdquo;&mdash;a structured object that contains
information. We now have sites \(x,\) \(y,\) \(z,\)&hellip; and types
\(\phi,\) \(\psi,\)&hellip;, where \(x:\phi\) is read as <em>the
information site \(x\) is of type</em> \(\phi\). With
the qualification that the channels may or may not be among the sites,
and that \(x\cmapsto y\) is a three-place (ternary) relation between
information sites and channels. Barwise formulates the <em>Soundness
Axiom</em> for channel theory as follows:</p> 

<div id="ex12">\[c: \phi\to\psi \text{ iff } \text{ for all sites } x, y, \text{ if } x\cmapsto y\text{ and } x:\phi, \text{ then } y:\psi.\]</div>

<p>
At this stage, things are starting to look decidedly <em>modal</em> in
spirit, if not in practice.</p>

<p>
Barwise and Perry&rsquo;s <em>situations</em> and Austin&rsquo;s
<em>demonstrative contents</em>, are simply partial worlds under a
different name. That is, they are <em>incomplete possible worlds</em>.
Austin&rsquo;s types, the <em>descriptive contents</em> of statements,
are looking very much like propositions&mdash;in particular the
proposition that describes the claim being made by an utterance. With
a little bit of license, we might think of Austin&rsquo;s
demonstrative content of a statement as that statement&rsquo;s
<em>truthmaker</em> in a fine-grained sense. Barwise&rsquo; notation
in
 <a href="#ex11"> (11)</a>
 above with respect to \(x\vDash\phi\) betrays this reading. Moreover,
given that \(x\cmapsto y\) is a ternary relation,
 <a href="#ex12">(12)</a>
 is starting to look very much like a semantic clause for the
conditional that turns on a three-place accessibility relation in
something like a Kripke frame.</p>

<p>
The semantics from Routley <em>et al.</em>&rsquo;s (1982) relevance logic
gives the evaluation conditions on a three-place accessibility
relation, where the notion of an accessibility relation is familiar
from their role in Kripke frames, used to specify the semantics of
modal logic. Barwise notes the connection explicitly:</p>

<blockquote>

<p>
The work presented here work also suggests a way to think about the
three-place accessibility relation semantics for relevance logic of
Routley and Meyer. (I have discussed this with both Gabbay and Dunn
off and on over the past year. More recently, Greg Restall has
observed this connection, and has begun to work out the connection in
some detail.) (1993: 26)</p>
</blockquote>

<p>
Restall (1996) along with Mares (1996) work out this connection as
follows. Restall assumes that channels <em>are</em> amongst the
information sites (Mares does not). Instead of information
<em>sites</em>, common terminology speaks of information
<em>states</em>. Information states may be incomplete and/or
inconsistent, indeed they may be sub-propositional entirely (as will
be the case below when we look at fine-grained information-based
semantics for natural languages based on informationalised versions of
the Lambek Calculi). In Kripke/frame semantics terms, we have
<em>ternary information frame</em> \(\mathbf{F}:\langle S,
\sqsubseteq, R\rangle\), where \(S\) is a set of
information states, \(\sqsubseteq\) is a partial order on \(S\),
and \(R\) is a ternary accessibility
relation on members of \(S\). An <em>information
model</em> is an information frame \(\mathbf{F}\) along with an
evaluation/supports relation \(\Vdash\) between members of \(S\)
and types/propositions \(\phi, \psi\ldots\). How
exactly we read \(x\Vdash\phi\) is going to depend on what sort of
information state \(x\) happens to be, and what type of
thing \(\phi\) is. The simplest case will be when \(x\)
is a situation and \(\phi\) is a proposition. In this case we may read
\(x\Vdash\phi\) as <em>\(\phi\) is true at \(x\)</em>.
Given this much,
 <a href="#ex12">(12)</a>
 is translated as follows:</p> 

<div id="ex13">\[x\Vdash\phi\to\psi\text{ iff }\forall y, z \text{ s.t. } Rxyz, \text{ if }y\Vdash\phi\text{ then }z\Vdash\psi.\]</div>

<p>
In this context, the way that \(Rxyz\) is read is&mdash;if you take
the information that is true at \(x\), and you put it
together with the information that is true at \(y\),
then you get the information that is true at \(z\).
However, \(Rxyz\) is not read so strictly in general. Although the
\(\Vdash\) relation <em>can</em> be read as a straightforward semantic
relation in line with \(\vDash\), it is considerably more flexible.
Other readings \(x\Vdash\phi\) include <em>\(x\)
carries the information that \(\psi\)</em>, <em>\(x\)
carries the information of type \(\psi\)</em>, <em>\(x\)
supports the information that/of type \(\psi\)</em>,
<em>\(x\) is a record of the information that/of type
\(\psi\)</em>, and so on. As a consequence of this, the way that
\(Rxyz\) is read in practice will depend the applications to which the
resulting information-based semantic models are being put&mdash;that
is, on the domain of the information channels in question.</p>

<p>
The domain of information channels might be anything from channels for
propositionally structured environmental information along the lines
Floridi is interested in (be it veridical or not), or
sub-propositionally structured environmental information along the
lines Fodor and Evans are interested in. Moreover, it might be
linguistically focused sub-propositionally structured information from
natural language semantics, or concern semantic informational
phenomena familiar from issues in the philosophy of language such as
attitude reports and the semantic analysis of epistemic and other
attitudinal states. We will examine such approaches in some detail in
the section below.</p>

<p>
For now, note that semantic models of different information channel
types will be individuated in terms of how it is that the
&ldquo;putting together&rdquo; of \(x\) and \(y\)
in \(Rxyz\) is understood <em>precisely</em>. For
example, putting \(x\) together with \(y\)
might mean the same thing as putting \(y\)
together with \(x\), or it might not,
depending on whether or not one wants the ternary relation \(R\)
to be a <em>commutative</em> relation. That is, on
whether or not one wants it to be the case the \(\forall x\;\forall
y\;\forall z(Rxyz\to Ryxz)\) Whether or not one <em>does</em> want \(R\)
to be a commutative relation will depend on
properties of the information channels that one is trying to model
(for which see the paragraph above).</p>

<p>
By analogy, recall modal logic, where different properties of the
two-place accessibility relation \(R^2xy\) will generate different
modal logics (for example, to get the modal logic \(T\)
one makes \(R^2xy\) reflexive, to get the modal logic \(S4\) one makes
\(R^2xy\) reflexive and transitive and so on). Similar decisions can
be made with regard to the ternary relation \(Rxyz\). For example, one
might want \(Rxyz\) to have the properties of <em>commutativity</em>,
<em>associativity</em>, <em>contraction</em>, <em>monotonicity</em>,
and others, or none at all, or subtle combinations of these and more.
These decisions will generate <em>different logics of information
channels</em> in the same way as do the choices on \(R^2\) with regard
to different modal logics. These logics are known in general as
substructural logics on account of the way the properties of the
ternary accessibility relation (commutation etc.), correspond to the
structural rules that individuate the logics themselves. (One may
think of structural rules as the syntactic/proof-theoretic
counterparts to the semantic conditions being discussed presently.) As
a part of the growing field of
 <a href="../logic-information/index.html">logic and information</a>
 more generally, we will see in the following section that clusters of
such logics have found utility across a range of
informational-semantic phenomena.</p>

<h3 id="SemaInfoFlow">3.1 The semantics of information flow</h3>

<p>
A group of weak substructural logics known as the <em>Lambek
calculi</em> reject <em>all</em> structural rules, or else one or the
other of either commutation or association, or possess both of these
rules only. Designed by and named after Joachim Lambek, these logics
were designed originally to model the syntax, or formal grammar, of
natural languages (see the entry on
 <a href="../typelogical-grammar/index.html">typelogical grammar</a>).</p>
 
<p>
That they have found a home modelling&mdash;providing a semantics
for&mdash;information flow across information channels is not as
surprising as it might seem initially. Firstly, with some license we
may think of a natural language lexicon as a database, and a grammar
as a specification of the processing constraints on that database such
that the processing constraints guarantee well-formed outputs.
Secondly, one of situation and channel theory&rsquo;s targets
originally was natural language semantics itself, so the convergence
is far from totally surprising. For example, Massimo Poesio (1993)
appeals to the formal nomenclature of situation theory in order to
build a theory of definite descriptions. Ginzburg (1993) uses the
naturally fine-grained structures of situation theory to give a
semantics for propositional attitudes. Hwang and Schubert (1993)
implement natural Language Processing (NLP) controls via a situation
theoretic framework. Westerh&aring;ll, Haglund and Lager (1993) appeal
to situation theory to give a theory of <em>text meaning</em> where
texts are treated as abstract states coding readers&rsquo; cognitive
states.</p>

<p>
Barwise, Gabbay, and Hartonas (1995, 1996), appeal to the
<em>associative</em> Lambek calculus in order to model, that is to
give a semantics for, <em>information flow itself</em>. They define an
<em>information network</em> \(\mathbf{N}\) as a quadruple such that
\(\mathbf{N} := \langle S, C, \mapsto, \circ\rangle\), where \(S\)
is a set of information states (called
&ldquo;sites&rdquo; by the authors), \(C\) is a set of
information channels, \(\mapsto\) is a ternary accessibility relation
on \(S \times C \times S\), and \(\circ\) is an associative binary
composition operator on \(C\). For information to flow,
there must be some way in which channels compose so that information
can flow from one channel to another. The authors specify the
following constraint on serial channel composition. For all channels
\(a\) and \(b\):</p> 

<div id="ex14">\[\forall x\; \forall y (x \overset{a\circ b}{\longmapsto} y\: \text{ iff }\ \exists z(x\overset{a}{\mapsto} z \text{ and } z\overset{b}{\mapsto} y))\]</div>

<p>
The author&rsquo;s argue for channels associating, hence the binary
composition operator on channels being associative, i.e., for all
channels \(a,\) \(b\), and \(c\), if
\(a\circ(b\circ c)\), then \((a\circ b)\circ c\)). Those familiar with
<em>category theory</em> will know the refrain &ldquo;channels
associate!&rdquo;.</p>

<p>
Care is needed so as to not conflate channel <em>composition</em> as
specified above in
 <a href="#ex14">(14)</a>,
 with the channel <em>application</em> specified above in
 <a href="#ex12">(12)</a>
 and
 <a href="#ex13">(13)</a>.
 The latter involves feeding a channel its input, whereas the former
involves the compositions of channels themselves. Tedder (2017) argues
elegantly for the composition and application of information channels
to be treated separately, and that we should not expect the properties
of both (specified via structural rules on the ternary relation
\(\mapsto\) to be the same. For arguments with regard to just what
properties it is that we <em>should</em> expect channel composition
and application to possess, see Tedder (2021) and Sequoiah-Grayson
(2021). Sequoiah-Grayson (2010) argues that a basic theory of
information flow with a semantics given by the Lambek calculi gives us
an informational interpretation of the <em>dynamic predicate
logic</em> (DPL) of Groenendijk and Stokhof (1991).</p>

<p>
Van Benthem (2010), by contrast, argues against the temptation to
understand Lambek calculi in such foundational informational terms.
This is not to suggest that van Benthem is opposed to extended
applications of the Lambek calculi. For example, van Benthem (1996)
argues for an application of the Lambek calculi for the purpose of
giving a dynamic semantics for <em>cognitive procedures</em>. Van
Benthem&rsquo;s use of the Lambek calculi for a dynamic semantics of
cognitive procedures, <em>in combination with</em> the use of
substructurally interpreted Lambek calculi as a foundational model of
information flow, leads naturally to the idea that models for
<em>dynamic epistemic phenomena</em> might be given in information
channel-theoretic terms. We examine such information models in the
following section.</p>

<h3 id="ModeEpisPhenInfo">3.2 Modelling epistemic phenomena informationally</h3>

<p>
Sedl&aacute;r and Pun&#269;och&aacute;&#345; (2019) extend
<em>propositional dynamic logic</em> (PDL) into the non-associative
Lambek Calculus, which they call <em>Lambek PDL</em>. They give Lambek
PDL three informal interpretations, one in terms of actions that
modify linguistic resources, another in terms of actions that modify
bodies of information, and another in terms of actions that modify the
epistemic states of agents (see 2019: 358&ndash;539). In their
semantics, specific readings of the ternary relation \(R\)
from
 <a href="#ex13">(13)</a>
 above will depend on the interpretation of the information states in
their models. In particular, they are interested in threshold cases
where commutation, \(x\circ y = y\circ x\), breaks down for channel
application. Sedl&aacute;r (2020) extends the non-associative and
non-commutative Lambek calculus with <em>iterative</em> channel
operations (both applicational and compositional) under an
informational interpretation.</p>

<p>
Sedl&aacute;r (2016) designs and explores substructural epistemic
logics under an informational interpretation with the explicit goal of
attending to the <em>Scandal of Deduction</em> (SoD) from
 <a href="#ProbForTheoSemaInfo">section 1.1</a>
 above. The motivating idea here is that there are channels from one
epistemic state of an agent to another epistemic state of that agent,
and that certain epistemic actions (namely acts of reasoning) that
facilitate information flow along such channels can be captured by the
ternary relation \(R\) that marks channel application
in
 <a href="#ex13">(13)</a>
 above. Pun&#269;och&aacute;&#345; and Sedl&aacute;r (2017) introduce
a substructural epistemic logic for <em>pooling information</em> in a
group of agents via structured communication (viz. structured
information flow) between them. In this context the binary combination
operator \(\circ\) (&lsquo;\(\cdot\)&rsquo; in Sedl&aacute;r and
Pun&#269;och&aacute;&#345;&rsquo;s notation) is a pooling operator
<em>between</em> the different epistemic states of agents in a
communicative group. The authors&rsquo; have several examples to
suggest that both association and commutation are misguided in this
context. Sedl&aacute;r, Pun&#269;och&aacute;&#345;, and Tedder (2019)
provide a semantics for universal and common knowledge operators via
the now-familiar informational reading of the non-associative Lambek
Calculus under an informational interpretation.</p>

<h2 id="Summ">4. Summary</h2>

<p>
At this point it is clear that semantic conceptions of information
cover a large amount of territory, but not one without structure of
cohesion.</p>

<p>
Carnap and Bar-Hillel&rsquo;s (1952) theory of semantic information
for formal languages has an intuitive starting point, one that takes
intensions and semantic information to be very closely related.
Whatever the shortcomings of their theory, it has motivated an entire
field of research into the nature of semantic information via the
systematic informational approach to semantic and related phenomena of
Luciano Floridi along with an increasingly large number of closely
related research programs.</p>

<p>
The information-based semantics for natural languages and content
bearing mental states due largely to Dretske, Evans, Fodor, Lewis,
Jackson, Recanati, and Zalta has led to refined theories of meaning
and content in terms of informational relations. Such
relations&mdash;information channels that allow information to flow
from one part of a system to another&mdash;have proved to be so
indispensable that they are in turn an object of research in their own
right.</p>

<p>
The semantic theory of information channels due largely to Barwise has
been refined in such a way as to permit its adaptation for modelling a
rich range of philosophical phenomena. Logics designed originally to
model linguistic artefacts on their own terms have been used to
capture the properties of information flow. This has lead quickly to
rigorously defined semantic models for such linguistic artefacts, as
well as to models for epistemic phenomena that are given in terms of
information flow itself.</p>
</div>

<div id="bibliography">

<h2 id="Bib">Bibliography</h2>

<ul class="hanging">

<li>Aczel, Peter, David Israel, Yasuhiro Katagin, and Stanley Peters
(eds.), 1993, <em>Situation Theory and Its Applications, Volume 3:
Proceedings of the First-Third Conference on Situation Theory and Its
Applications. Third Conference Held in Kanagawa, Japan, November
1991</em>, (CSLI Lecture Notes 37), Stanford, CA: CSLI
Publications.</li>

<li>Allo, Patrick, 2007, &ldquo;Logical Pluralism and Semantic
Information&rdquo;, <em>Journal of Philosophical Logic</em>, 36(6):
659&ndash;694. doi:10.1007/s10992-007-9054-2</li>

<li>&ndash;&ndash;&ndash;, 2011, &ldquo;The Logic of &lsquo;Being
Informed&rsquo; Revisited and Revised&rdquo;, <em>Philosophical
Studies</em>, 153(3): 417&ndash;434.
doi:10.1007/s11098-010-9516-1</li>

<li>Austin, J. L., 1950, &ldquo;Truth&rdquo;, <em>Aristotelian Society
Supplementary Volume</em>, 24: 111&ndash;128.
doi:10.1093/aristoteliansupp/24.1.111</li>

<li>Barwise, Jon, 1993, &ldquo;Constraints, Channels, and the Flow of
Information&rdquo;, in Aczel et al. 1993: 3&ndash;28.</li>

<li>Barwise, Jon and John Perry, 1983, <em>Situations and
Attitudes</em>, Cambridge, MA: MIT Press.</li>

<li>Barwise, Jon, Dov Gabbay, and Chrysafis Hartonas, 1995, &ldquo;On
the Logic of Information Flow&rdquo;, <em>Logic Journal of IGPL</em>,
3(1): 7&ndash;49. doi:10.1093/jigpal/3.1.7</li>

<li>&ndash;&ndash;&ndash;, 1996, &ldquo;Information Flow and the
Lambek Calculus&rdquo;, in Seligman and Westerst&aring;hl 1996:
47-62.</li>

<li>van Benthem, Johan, 1996, <em>Exploring Logical Dynamics</em>,
Stanford, CA: CSLI Publications.</li>

<li>&ndash;&ndash;&ndash;, 2010, &ldquo;Categorial Versus Modal
Information Theory&rdquo;, in van Benthem and Moortgat 2010:
533&ndash;543.</li>

<li>van Benthem, Johan and Michael Moortgat (eds), 2010,
<em>Festschrift for Joachim Lambek</em>, issue of <em>Linguistic
Analysis</em>, 36(1-4).</li>

<li>Bremer, Manual and Daniel Cohnitz, 2004, <em>Information and
Information Flow: An Introduction</em>, Frankfurt, Lancaster: Ontos
Verlag.</li>

<li>Carnap, Rudolf and Yehoshua Bar-Hillel, 1952, &ldquo;An Outline of
a Theory of Semantic Information&rdquo;, Technical report 247,
Cambridge, MA: Research Laboratory of Electronics, Massachusetts
Institute of Technology. Reprinted in <em>Language and Information:
Selected Essays on their Theory and Application</em>, Y. Bar-Hillel,
Addison-Wesley Series in Logic, Israel: Jerusalem Academic Press and
Addison-Wesley, 1964, pp. 221&ndash;274.
 [<a href="https://dspace.mit.edu/handle/1721.1/4821" target="other">Carnap and Bar-Hillel 1952 available online</a>]</li>
 
<li>Cevolani, Gustavo, 2011, &ldquo;Verisimilitude and Strongly
Semantic Information&rdquo;, <em>Etica &amp; Politica/Ethics &amp;
Politics</em>, 13(2): 159&ndash;179.</li>

<li>&ndash;&ndash;&ndash;, 2014, &ldquo;Strongly Semantic Information
as Information About the Truth&rdquo;, in <em>Recent Trends in
Philosophical Logic</em>, Roberto Ciuni, Heinrich Wansing, and
Caroline Willkommen (eds.), (Trends in Logic 41), Cham: Springer
International Publishing, 59&ndash;74.
doi:10.1007/978-3-319-06080-4_5</li>

<li>D&rsquo;Alfonso, Simon, 2011, &ldquo;On Quantifying Semantic
Information&rdquo;, <em>Information</em>, 2(1): 61&ndash;101.
doi:10.3390/info2010061</li>

<li>Dinneen, Jesse David and Christian Brauner, 2015, &ldquo;Practical
and Philosophical Considerations for Defining Information as
Well-Formed, Meaningful Data in the Information Sciences&rdquo;,
<em>Library Trends</em>, 63(3): 378&ndash;400.
doi:10.1353/lib.2015.0012</li>

<li>Dodig-Crnkovic, Gordana, 2005, &ldquo;System Modeling and
Information Semantics&rdquo;, in <em>Proceedings of the Fifth Promote
IT Conference</em> (Borl&auml;nge, Sweden), Janis Bubenko, Owen
Eriksson, Hans Fernlund, and Mikael Lind (eds.), Lund:
Studentlitteratur.</li>

<li>Dretske, Fred I., 1981, <em>Knowledge and the Flow of
Information</em>, Cambridge, MA: The MIT Press.</li>

<li>Evans, Gareth, 1982, <em>The Varieties of Reference</em>, John
Henry McDowell (ed.), Oxford: Clarendon Press.</li>

<li>Floridi, Luciano, 2004, &ldquo;Outline of a Theory of Strongly
Semantic Information&rdquo;, <em>Minds and Machines</em>, 14(2):
197&ndash;221. doi:10.1023/B:MIND.0000021684.50925.c9</li>

<li>&ndash;&ndash;&ndash;, 2005, &ldquo;Is Semantic Information
Meaningful Data?&rdquo;, <em>Philosophy and Phenomenological
Research</em>, 70(2): 351&ndash;370.
doi:10.1111/j.1933-1592.2005.tb00531.x</li>

<li>&ndash;&ndash;&ndash;, 2006, &ldquo;The Logic of Being
Informed&rdquo;, <em>Logique et Analyse</em>, 49(196):
433&ndash;460.</li>

<li>&ndash;&ndash;&ndash;, 2008, &ldquo;Understanding Epistemic
Relevance&rdquo;, <em>Erkenntnis</em>, 69(1): 69&ndash;92.
doi:10.1007/s10670-007-9087-5</li>

<li>&ndash;&ndash;&ndash;, 2011, &ldquo;Semantic Information and the
Correctness Theory of Truth&rdquo;, <em>Erkenntnis</em>, 74(2):
147&ndash;175. doi:10.1007/s10670-010-9249-8</li>

<li>&ndash;&ndash;&ndash;, 2012, &ldquo;Semantic Information and the
Network Theory of Account&rdquo;, <em>Synthese</em>, 184(3):
431&ndash;454. doi:10.1007/s11229-010-9821-4</li>

<li>Fetzer, James H., 2004, &ldquo;Information: Does It Have To Be
True?&rdquo;, <em>Minds and Machines</em>, 14(2): 223&ndash;229.
doi:10.1023/B:MIND.0000021682.61365.56</li>

<li>Fodor, Jerry A., 1990, <em>A Theory of Content and Other
Essays</em>, Cambridge, MA: MIT Press.</li>

<li>Frick&eacute;, Martin, 1997, &ldquo;Information Using Likeness
Measures&rdquo;, <em>Journal of the American Society for Information
Science</em>, 48(10): 882&ndash;892.
doi:10.1002/(SICI)1097-4571(199710)48:10&lt;882::AID-ASI4&gt;3.0.CO;2-Y</li>

<li>Ginzburg, Jonathan, 1993, &ldquo;Propositional and
Non-Propositional Attitudes&rdquo;, in Aczel et al. 1993:
265&ndash;302.</li>

<li>Groenendijk, Jeroen and Martin Stokhof, 1991, &ldquo;Dynamic
Predicate Logic&rdquo;, <em>Linguistics and Philosophy</em>, 14(1):
39&ndash;100. doi:10.1007/BF00628304</li>

<li>Hintikka, Jaakko, 1970, &ldquo;Surface Information and Depth
Information&rdquo;, in <em>Information and Inference</em>, Jaakko
Hintikka and Patrick Suppes (eds.), Dordrecht: Reidel, 263&ndash;297.
doi:10.1007/978-94-010-3296-4_8</li>

<li>&ndash;&ndash;&ndash;, 1973, <em>Logic, Language Games, and
Information</em>, Oxford: Clarendon Press.</li>

<li>Hwang, Chung Hee and Lenhart K. Schubert, 1993, &ldquo;Episodic
Logic: A Situational Logic for Natural Language Processing&rdquo;, in
Aczel et al. 1993: 303&ndash;338.</li>

<li>Jackson, Frank, 2010, <em>Language, Names, and Information</em>,
Oxford, UK: Wiley-Blackwell. doi:10.1002/9781444325362</li>

<li>Lewis, David, 1983, &ldquo;Individuation by Acquaintance and by
Stipulation&rdquo;, <em>The Philosophical Review</em>, 92(1):
3&ndash;32. doi:10.2307/2184519</li>

<li>Mares, Edwin D., 1996, &ldquo;Relevant Logic and the Theory of
Information&rdquo;, <em>Synthese</em>, 109(3): 345&ndash;360.
doi:10.1007/BF00413865</li>

<li>Mingers, John C., 1995, &ldquo;Information and Meaning:
Foundations for an Intersubjective Account&rdquo;, <em>Information
Systems Journal</em>, 5(4): 285&ndash;306.
doi:10.1111/j.1365-2575.1995.tb00100.x</li>

<li>&ndash;&ndash;&ndash;, 1996a, &ldquo;Embodying Information
Systems&rdquo;, in <em>Information Technology and Changes in
Organisational Work</em>, Wanda Orlikowski, Geoff Walsham, Matthew
Jones, and Janice DeGross (eds), London: Chapman Hall,
272&ndash;292.</li>

<li>&ndash;&ndash;&ndash;, 1996b, &ldquo;An Evaluation of Theories of
Information with Regard to the Semantic and Pragmatic Aspects of
Information Systems&rdquo;, <em>Systems Practice</em>, 9(3):
187&ndash;209. doi:10.1007/BF02169014</li>

<li>Mingers, John C. and Craig Standing, 2018, &ldquo;What Is
Information? Toward a Theory of Information as Objective and
Veridical&rdquo;, <em>Journal of Information Technology</em>, 33(2):
85&ndash;104. doi:10.1057/s41265-017-0038-6</li>

<li>Poesio, Massimo, 1993, &ldquo;A Situation-Theoretic Formalization
of Definite Description Interpretation in Plan Elaboration
Dialogues&rdquo;, in Aczel et al. 1993: 339&ndash;374.</li>

<li>Primiero, Giuseppe, 2009, &ldquo;An Epistemic Logic for Becoming
Informed&rdquo;, <em>Synthese</em>, 167(2): 363&ndash;389.
doi:10.1007/s11229-008-9413-8</li>

<li>Pun&#269;och&aacute;&#345;, V&iacute;t and Igor Sedl&aacute;r,
2017, &ldquo;Substructural Logics for Pooling Information&rdquo;, in
<em>Logic, Rationality, and Interaction: 6th International Workshop,
LORI 2017</em>, Alexandru Baltag, Jeremy Seligman, and Tomoyuki Yamada
(eds.), (Lecture Notes in Computer Science 10455), Berlin, Heidelberg:
Springer Berlin Heidelberg, 407&ndash;421.
doi:10.1007/978-3-662-55665-8_28</li>

<li>Recanati, Fran&ccedil;ois, 2012, <em>Mental Files</em>, Oxford:
Oxford University Press.
doi:10.1093/acprof:oso/9780199659982.001.0001</li>

<li>&ndash;&ndash;&ndash;, 2016, <em>Mental Files in Flux</em>,
Oxford: Oxford University Press.
doi:10.1093/acprof:oso/9780198790358.001.0001</li>

<li>Restall, Greg, 1996, &ldquo;Information Flow and Relevant
Logics&rdquo;, in Seligman and Westerst&aring;hl 1996:
463&ndash;477.</li>

<li>&ndash;&ndash;&ndash;, 2002, <em>An Introduction to Substructural
Logics</em>, London: Routledge. doi:10.4324/9780203016244</li>

<li>Routley, Richard, Robert K. Meyer, Valerie Plumwood, and Ross T.
Brady, 1982, <em>Relevant Logics and Their Rivals 1</em>, Atascadero,
CA: Ridgeview.</li>

<li>Sedl&aacute;r, Igor, 2016, &ldquo;Epistemic Extensions of Modal
Distributive Substructural Logics&rdquo;, <em>Journal of Logic and
Computation</em>, 26(6): 1787&ndash;1813.
doi:10.1093/logcom/exu034</li>

<li>&ndash;&ndash;&ndash;, 2020, &ldquo;Iterative Division in the
Distributive Full Non-Associative Lambek Calculus&rdquo;, in
<em>Dynamic Logic. New Trends and Applications: Second International
Workshop, DaL&iacute; 2019</em>, Lu&iacute;s Soares Barbosa and
Alexandru Baltag (eds.), (Lecture Notes in Computer Science 12005),
Cham: Springer International Publishing, 141&ndash;154.
doi:10.1007/978-3-030-38808-9_9</li>

<li>Sedl&aacute;r, Igor and V&iacute;t Pun&#269;och&aacute;&#345;,
2019, &ldquo;From Positive PDL to Its Non-Classical Extensions&rdquo;,
<em>Logic Journal of the IGPL</em>, 27(4): 522&ndash;542.
doi:10.1093/jigpal/jzz017</li>

<li>Sedl&aacute;r, Igor, V&iacute;t Pun&#269;och&aacute;&#345;, and
Andrew Tedder, 2019, &ldquo;First Degree Entailment with Group
Attitudes and Information Updates&rdquo;, in <em>Logic, Rationality,
and Interaction: 7th International Workshop, LORI 2019</em>, Patrick
Blackburn, Emiliano Lorini, and Meiyun Guo (eds.), (Lecture Notes in
Computer Science 11813), Berlin, Heidelberg: Springer Berlin
Heidelberg, 273&ndash;285. doi:10.1007/978-3-662-60292-8_20</li>

<li>Seligman, Jerry and Dag Westerst&aring;hl (eds.), 1996, <em>Logic,
Language and Computation, Volume 1</em>, (CSLI Lecture Notes 58),
Stanford, CA: CSLI Publications.</li>

<li>Sequoiah-Grayson, Sebastian, 2007, &ldquo;The Metaphilosophy of
Information&rdquo;, <em>Minds and Machines</em>, 17(3): 331&ndash;344.
doi:10.1007/s11023-007-9072-4</li>

<li>&ndash;&ndash;&ndash;, 2008, &ldquo;The Scandal of Deduction:
Hintikka on the Information Yield of Deductive Inferences&rdquo;,
<em>Journal of Philosophical Logic</em>, 37(1): 67&ndash;94.
doi:10.1007/s10992-007-9060-4</li>

<li>&ndash;&ndash;&ndash;, 2010, &ldquo;Lambek Calculi with 0 and
Test-Failure in DPL&rdquo;, in van Benthem and Moortgat 2010:
517&ndash;532.</li>

<li>&ndash;&ndash;&ndash;, 2021 &ldquo;A Logic of Affordances&rdquo;,
in <em>The Logica Yearbook 2020</em>, Martin Blicha and Igor
Sedl&aacute;r (eds), London: College Publications, 219&ndash;236.</li>

<li>Shannon, Claude E., 1950, &ldquo;General Treatment of the Problem
of Coding. The Lattice Theory of Information&rdquo;, presented at the
<em>Symposium on Information Theory</em>, London, September 1950.
Printed, 1953, as two papers, &ldquo;General Treatment of the Problem
of Coding&rdquo; and &ldquo;The Lattice Theory of Information&rdquo;
in <em>Institute of Radio Engineers (IRE), Transactions on Information
Theory</em>, 1(1): 102&ndash;104 and 105&ndash;107, respectively.
Reprinted, 1993, in <em>Claude E. Shannon: Collected Papers</em>, N.
J. A. Sloane and A. D. Wyner (eds), Los Alamos, CA: IEEE Press,
177&ndash;179 and 180&ndash;183. doi:10.1109/TIT.1953.1188572</li>

<li>Tedder, Andrew, 2017, &ldquo;Channel Composition and Ternary
Relation Semantics&rdquo;, <em>Proceedings of the Third Workshop</em>,
Katalin Bimb&oacute; and J. Michael Dunn (eds), special issue of
<em>The IfCoLog Journal of Logics and their Applications</em>, 4(3):
731&ndash;753.</li>

<li>&ndash;&ndash;&ndash;, 2021, &ldquo;Information Flow in Logics in
the Vicinity of BB&rdquo;, <em>The Australasian Journal of Logic</em>,
18(1): 1&ndash;24. doi:10.26686/ajl.v18i1.6288</li>

<li>Vakarelov, Orlin, 2010, &ldquo;Pre-Cognitive Semantic
Information&rdquo;, <em>Knowledge, Technology &amp; Policy</em>,
23(1&ndash;2): 193&ndash;226. doi:10.1007/s12130-010-9109-5</li>

<li>Westerh&aring;ll, Dag, Bj&ouml;rn Haglund, and Torbj&ouml;rn
Lager, 1993, &ldquo;A Situation-Theoretic Representation of Text
Meaning: Anaphora, Quantification, and Negation&rdquo;, in Aczel et
al. 1993: 375&ndash;408.</li>

<li>Williamson, Timothy (ed.), 2007, <em>The Philosophy of
Philosophy</em>, Oxford, UK: Blackwell Publishing Ltd.
doi:10.1002/9780470696675</li>

<li>Wittgenstein. Ludwig, 1921 [1922], &ldquo;Logisch-Philosophische
Abhandlung&rdquo;, <em>Annalen der Naturphilosophische</em>, 14(3/4):
185&ndash;262. Translated as <em>Tractatus Logico-Philosophicus</em>
(TLP), C. K. Ogden (trans.), London: Routledge &amp; Kegan Paul,
1922.</li>
</ul>
</div> 

<div id="academic-tools">

<h2 id="Aca">Academic Tools</h2>

<blockquote>
<table class="vert-top">
<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=information-semantic" target="other">How to cite this entry</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/sepman-icon.jpg" alt="sep man icon" /></td>
<td><a href="https://leibniz.stanford.edu/friends/preview/information-semantic/" target="other">Preview the PDF version of this entry</a> at the
 <a href="https://leibniz.stanford.edu/friends/" target="other">Friends of the SEP Society</a>.</td>
</tr>

<tr>
<td><img src="../../symbols/inpho.png" alt="inpho icon" /></td>
<td><a href="https://www.inphoproject.org/entity?sep=information-semantic&amp;redirect=True" target="other">Look up topics and thinkers related to this entry</a>
 at the Internet Philosophy Ontology Project (InPhO).</td>
</tr>

<tr>
<td><img src="../../symbols/pp.gif" alt="phil papers icon" /></td>
<td><a href="https://philpapers.org/sep/information-semantic/" target="other">Enhanced bibliography for this entry</a>
at <a href="https://philpapers.org/" target="other">PhilPapers</a>, with links to its database.</td>
</tr>

</table>
</blockquote>


</div>

<div id="other-internet-resources">

<h2 id="Oth">Other Internet Resources</h2>

<p>[Please contact the author with suggestions.]</p>
</div>

<div id="related-entries">

<h2 id="Rel">Related Entries</h2>

<p>

 <a href="../typelogical-grammar/index.html">grammar: typelogical</a> |
 <a href="../information/index.html">information</a> |
 <a href="../logic-information/index.html">logic: and information</a> |
 <a href="../logic-modal/index.html">logic: modal</a> |
 <a href="../logical-pluralism/index.html">logical pluralism</a> |
 <a href="../meaning/index.html">meaning, theories of</a> |
 <a href="../content-causal/index.html">mental content: causal theories of</a> |
 <a href="../situations-semantics/index.html">situations: in natural language semantics</a>

</p>

</div>

<script type="text/javascript" src="local.js"></script>
<script type="text/javascript" src="../../MathJax/MathJaxb198.js?config=TeX-MML-AM_CHTML"></script>


</div><!-- #aueditable --><!--DO NOT MODIFY THIS LINE AND BELOW-->

<!-- END ARTICLE HTML -->

</div> <!-- End article-content -->

  <div id="article-copyright">
    <p>
 <a href="../../info.html#c">Copyright &copy; 2022</a> by

<br />
Sebastian Sequoiah-Grayson
&lt;<a href="m&#97;ilto:sequoiah&#37;40gmail&#37;2ecom"><em>sequoiah<abbr title=" at ">&#64;</abbr>gmail<abbr title=" dot ">&#46;</abbr>com</em></a>&gt;<br />
<a href="http://www.philosophyofinformation.net/" target="other">Luciano Floridi</a>

    </p>
  </div>

</div> <!-- End article -->

<!-- NOTE: article banner is outside of the id="article" div. -->
<div id="article-banner" class="scroll-block">
  <div id="article-banner-content">
    <a href="../../fundraising/index.html">
    Open access to the SEP is made possible by a world-wide funding initiative.<br />
    The Encyclopedia Now Needs Your Support<br />
    Please Read How You Can Help Keep the Encyclopedia Free</a>
  </div>
</div> <!-- End article-banner -->

    </div> <!-- End content -->

    <div id="footer">

      <div id="footer-menu">
        <div class="menu-block">
          <h4><i class="icon-book"></i> Browse</h4>
          <ul role="menu">
            <li><a href="../../contents.html">Table of Contents</a></li>
            <li><a href="../../new.html">What's New</a></li>
            <li><a href="https://plato.stanford.edu/cgi-bin/encyclopedia/random">Random Entry</a></li>
            <li><a href="../../published.html">Chronological</a></li>
            <li><a href="../../archives/index.html">Archives</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-info-sign"></i> About</h4>
          <ul role="menu">
            <li><a href="../../info.html">Editorial Information</a></li>
            <li><a href="../../about.html">About the SEP</a></li>
            <li><a href="../../board.html">Editorial Board</a></li>
            <li><a href="../../cite.html">How to Cite the SEP</a></li>
            <li><a href="../../special-characters.html">Special Characters</a></li>
            <li><a href="../../tools/index.html">Advanced Tools</a></li>
            <li><a href="../../contact.html">Contact</a></li>
          </ul>
        </div>
        <div class="menu-block">
          <h4><i class="icon-leaf"></i> Support SEP</h4>
          <ul role="menu">
            <li><a href="../../support/index.html">Support the SEP</a></li>
            <li><a href="../../support/friends.html">PDFs for SEP Friends</a></li>
            <li><a href="../../support/donate.html">Make a Donation</a></li>
            <li><a href="../../support/sepia.html">SEPIA for Libraries</a></li>
          </ul>
        </div>
      </div> <!-- End footer menu -->

      <div id="mirrors">
        <div id="mirror-info">
          <h4><i class="icon-globe"></i> Mirror Sites</h4>
          <p>View this site from another server:</p>
        </div>
        <div class="btn-group open">
          <a class="btn dropdown-toggle" data-toggle="dropdown" href="https://plato.stanford.edu/">
            <span class="flag flag-usa"></span> USA (Main Site) <span class="caret"></span>
            <span class="mirror-source">Philosophy, Stanford University</span>
          </a>
          <ul class="dropdown-menu">
            <li><a href="../../mirrors.html">Info about mirror sites</a></li>
          </ul>
        </div>
      </div> <!-- End mirrors -->
      
      <div id="site-credits">
        <p>The Stanford Encyclopedia of Philosophy is <a href="../../info.html#c">copyright &copy; 2022</a> by <a href="http://mally.stanford.edu/">The Metaphysics Research Lab</a>, Department of Philosophy, Stanford University</p>
        <p>Library of Congress Catalog Data: ISSN 1095-5054</p>
      </div> <!-- End site credits -->

    </div> <!-- End footer -->

  </div> <!-- End container -->

   <!-- NOTE: Script required for drop-down button to work (mirrors). -->
  <script>
    $('.dropdown-toggle').dropdown();
  </script>

</body>

<!-- Mirrored from seop.illc.uva.nl/entries/information-semantic/ by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 22 Jun 2022 19:50:11 GMT -->
</html>
